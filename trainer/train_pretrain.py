"""
================================================================================
                    MiniMind 预训练脚本 (Pre-training)
================================================================================

【什么是预训练】
预训练是 LLM 训练的第一阶段，目标是让模型学习语言的基本规律:
1. 语法结构 - 什么样的句子是合法的
2. 语义关系 - 词语之间的含义关联  
3. 世界知识 - 从大量文本中学习事实和常识

【训练目标】
因果语言模型目标 (Causal Language Modeling):
给定文本 [t₁, t₂, ..., tₙ]，预测每个位置的下一个 token:
Loss = -Σ log P(tᵢ | t₁, ..., tᵢ₋₁)

【关键技术】
1. 混合精度训练 (AMP): 加速训练并节省显存
2. 梯度累积: 在显存有限时模拟大批量
3. 梯度裁剪: 防止梯度爆炸
4. 余弦学习率调度: 更好的收敛
5. 分布式训练 (DDP): 多 GPU 加速

【使用方法】
单卡训练:
    python train_pretrain.py --epochs 1 --batch_size 32

多卡训练:
    torchrun --nproc_per_node=4 train_pretrain.py --epochs 1 --batch_size 32
"""

import os
import sys

__package__ = "trainer"
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import argparse
import time
import warnings
import torch
import torch.distributed as dist
from contextlib import nullcontext
from torch import optim, nn
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data import DataLoader, DistributedSampler
from model.model_minimind import MiniMindConfig
from dataset.lm_dataset import PretrainDataset
from trainer.trainer_utils import get_lr, Logger, is_main_process, lm_checkpoint, init_distributed_mode, setup_seed, init_model, SkipBatchSampler

warnings.filterwarnings('ignore')


def train_epoch(epoch, loader, iters, start_step=0, wandb=None):
    """
    训练一个 epoch
    
    【核心训练循环】
    for batch in data:
        1. 前向传播: logits = model(X)
        2. 计算损失: loss = CrossEntropy(logits, Y) 
        3. 反向传播: loss.backward()
        4. 梯度累积: 累积 N 步后更新
        5. 梯度裁剪: 防止梯度爆炸
        6. 参数更新: optimizer.step()
    """
    # ════════════════════════════════════════════════════════════════════════════
    # 【什么是交叉熵 (Cross-Entropy)？】
    # ════════════════════════════════════════════════════════════════════════════
    #
    # 【信息论起源】
    # 交叉熵衡量两个概率分布之间的"距离":
    #   H(p, q) = -Σ p(x) × log(q(x))
    #   - p(x): 真实分布 (one-hot，正确词=1，其他=0)
    #   - q(x): 模型预测 (softmax后的概率)
    #
    # 【在语言模型中】
    # 例如预测"天气"的下一个词:
    #   真实: p = [0, 0, ..., 1(很好), ..., 0]
    #   预测: q = [0.1, 0.05, 0.3(很好), 0.15, ...]
    #   交叉熵 = -log(0.3) = 1.2
    #
    # 【直觉】
    #   预测概率 1.0 → loss = 0 (完美)
    #   预测概率 0.5 → loss = 0.69
    #   预测概率 0.01 → loss = 4.6 (很差)
    #
    # 【reduction='none'】
    # 返回每个位置的loss，不取平均
    # 这样可以用 loss_mask 手动过滤掉 padding 位置
    # ════════════════════════════════════════════════════════════════════════════
    loss_fct = nn.CrossEntropyLoss(reduction='none')
    start_time = time.time()
    
    for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):
        # X: 输入 [B, L-1], Y: 目标 [B, L-1] (X 右移一位)
        # loss_mask: 非 padding 位置为 1
        X = X.to(args.device)
        Y = Y.to(args.device)
        loss_mask = loss_mask.to(args.device)
        
        # ════════════════════════════════════════════════════════════════════
        # 【param_group 是什么？从哪来的？】
        # ════════════════════════════════════════════════════════════════════
        #
        # 【来源】optimizer 创建时自动生成
        #   optimizer = AdamW(model.parameters(), lr=0.001)
        #   → optimizer.param_groups = [
        #       {
        #           'params': [所有模型参数的引用],
        #           'lr': 0.001,           # 学习率
        #           'betas': (0.9, 0.999), # Adam动量参数
        #           'eps': 1e-8,           # 数值稳定性
        #           'weight_decay': 0.01,  # 权重衰减
        #       }
        #     ]
        #
        # 【为什么是列表？】可以给不同层设置不同学习率:
        #   optimizer = AdamW([
        #       {'params': model.encoder.parameters(), 'lr': 1e-4},
        #       {'params': model.decoder.parameters(), 'lr': 1e-3}
        #   ])  → param_groups 有 2 个元素
        #
        # 【这里干嘛？】动态调整学习率，无需重建 optimizer
        # ════════════════════════════════════════════════════════════════════
        lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        # 混合精度前向传播
        with autocast_ctx:
            res = model(X)
            # 计算每个位置的交叉熵损失
            loss = loss_fct(
                res.logits.view(-1, res.logits.size(-1)),  # [B*L, vocab_size]
                Y.view(-1)                                  # [B*L]
            ).view(Y.size())  # [B, L]

            # 应用 loss_mask，只在非 padding 位置计算损失
            logits_loss = (loss * loss_mask).sum() / loss_mask.sum()
            # 添加 MoE 辅助损失 (负载均衡)
            loss = logits_loss + res.aux_loss
            # 梯度累积: 损失除以累积步数
            loss = loss / args.accumulation_steps

        # ════════════════════════════════════════════════════════════════════
        # 【GradScaler 是干嘛的？】混合精度训练的梯度缩放器
        # ════════════════════════════════════════════════════════════════════
        #
        # 【背景问题】
        # float16 数值范围很小: 最小正数 ≈ 6×10⁻⁸
        # 梯度可能非常小(如 1×10⁻⁸)，在 float16 中会变成 0！
        # → 梯度消失，模型无法训练
        #
        # 【四步流程】
        # 1. scale(loss): 放大 loss (如 ×65536)
        #    → 反向传播时梯度也被放大，小梯度不会下溢
        # 2. unscale_(optimizer): 恢复原始梯度大小
        # 3. step(optimizer): 检查 inf/nan，无则更新参数
        # 4. update(): 动态调整缩放因子
        #
        # 【流程图】
        # loss=0.001 → scale() → 65.536 → backward()
        #           → scaled_grads → unscale_() → real_grads
        #           → clip → step() → update()
        # ════════════════════════════════════════════════════════════════════
        scaler.scale(loss).backward()

        # 反向传播 (使用 GradScaler 处理混合精度)
        if (step + 1) % args.accumulation_steps == 0:
            scaler.unscale_(optimizer)       # 恢复原始梯度
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)  # 梯度裁剪
            scaler.step(optimizer)           # 检查后更新参数
            scaler.update()                  # 调整缩放因子
            optimizer.zero_grad(set_to_none=True)

        if step % args.log_interval == 0 or step == iters - 1:
            spend_time = time.time() - start_time
            current_loss = loss.item() * args.accumulation_steps
            current_logits_loss = logits_loss.item()
            current_aux_loss = res.aux_loss.item()
            current_lr = optimizer.param_groups[-1]['lr']
            eta_min = spend_time / (step + 1) * iters // 60 - spend_time // 60
            
            Logger(f'Epoch:[{epoch + 1}/{args.epochs}]({step}/{iters}), loss: {current_loss:.4f}, logits_loss: {current_logits_loss:.4f}, aux_loss: {current_aux_loss:.4f}, learning_rate: {current_lr:.8f}, epoch_time: {eta_min:.3f}min')
            
            if wandb: wandb.log({"loss": current_loss, "logits_loss": current_logits_loss, "aux_loss": current_aux_loss, "learning_rate": current_lr, "epoch_time": eta_min})

        # ════════════════════════════════════════════════════════════════════
        # 【为什么保存前要 model.eval()？】
        # ════════════════════════════════════════════════════════════════════
        #
        # 【train() vs eval() 区别】
        # - Dropout: train()随机丢弃神经元, eval()不丢弃
        # - BatchNorm: train()用batch统计, eval()用全局统计
        #
        # 【eval() 其实不是必须的！】
        # state_dict() 只保存参数值，不保存模式状态
        # 加载后推理时还是要手动调 eval()
        #
        # 【这里 eval() 的真正原因】
        # 1. 防止 BatchNorm 的 running_mean/var 被意外更新
        # 2. 编程习惯: 保存时确保模型处于"干净"状态
        # 3. 如果保存后立即测试验证集，需要 eval 模式
        #
        # 【注意】保存完后要 model.train() 恢复训练模式！
        # ════════════════════════════════════════════════════════════════════
        if (step % args.save_interval == 0 or step == iters - 1) and is_main_process():
            model.eval()  # 切换到评估模式
            moe_suffix = '_moe' if lm_config.use_moe else ''
            ckp = f'{args.save_dir}/{args.save_weight}_{lm_config.hidden_size}{moe_suffix}.pth'
            if isinstance(model, torch.nn.parallel.DistributedDataParallel):
                state_dict = model.module.state_dict()
            else:
                state_dict = model.state_dict()
            state_dict = {k: v.half().cpu() for k, v in state_dict.items()}
            torch.save(state_dict, ckp)
            lm_checkpoint(lm_config, weight=args.save_weight, model=model, optimizer=optimizer, scaler=scaler, epoch=epoch, step=step, wandb=wandb, save_dir='../checkpoints')
            model.train()
            del state_dict

        del X, Y, loss_mask, res, loss


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MiniMind Pretraining")
    parser.add_argument("--save_dir", type=str, default="../out", help="模型保存目录")
    parser.add_argument('--save_weight', default='pretrain', type=str, help="保存权重的前缀名")
    parser.add_argument("--epochs", type=int, default=1, help="训练轮数（建议1轮zero或2-6轮充分训练）")
    parser.add_argument("--batch_size", type=int, default=32, help="batch size")
    parser.add_argument("--learning_rate", type=float, default=5e-4, help="初始学习率")
    parser.add_argument("--device", type=str, default="cuda:0" if torch.cuda.is_available() else "cpu", help="训练设备")
    parser.add_argument("--dtype", type=str, default="bfloat16", help="混合精度类型")
    parser.add_argument("--num_workers", type=int, default=8, help="数据加载线程数")
    parser.add_argument("--accumulation_steps", type=int, default=8, help="梯度累积步数")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="梯度裁剪阈值")
    parser.add_argument("--log_interval", type=int, default=100, help="日志打印间隔")
    parser.add_argument("--save_interval", type=int, default=1000, help="模型保存间隔")
    parser.add_argument('--hidden_size', default=512, type=int, help="隐藏层维度")
    parser.add_argument('--num_hidden_layers', default=8, type=int, help="隐藏层数量")
    parser.add_argument('--max_seq_len', default=340, type=int, help="训练的最大截断长度（中文1token≈1.5~1.7字符）")
    parser.add_argument('--use_moe', default=0, type=int, choices=[0, 1], help="是否使用MoE架构（0=否，1=是）")
    parser.add_argument("--data_path", type=str, default="../dataset/pretrain_hq.jsonl", help="预训练数据路径")
    parser.add_argument('--from_weight', default='none', type=str, help="基于哪个权重训练，为none则从头开始")
    parser.add_argument('--from_resume', default=0, type=int, choices=[0, 1], help="是否自动检测&续训（0=否，1=是）")
    parser.add_argument("--use_wandb", action="store_true", help="是否使用wandb")
    parser.add_argument("--wandb_project", type=str, default="MiniMind-Pretrain", help="wandb项目名")
    args = parser.parse_args()

    # ========== 1. 初始化环境和随机种子 ==========
    local_rank = init_distributed_mode()
    if dist.is_initialized(): args.device = f"cuda:{local_rank}"
    setup_seed(42 + (dist.get_rank() if dist.is_initialized() else 0))
    
    # ========== 2. 配置目录、模型参数、检查ckp ==========
    os.makedirs(args.save_dir, exist_ok=True)
    lm_config = MiniMindConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers, use_moe=bool(args.use_moe))
    ckp_data = lm_checkpoint(lm_config, weight=args.save_weight, save_dir='../checkpoints') if args.from_resume==1 else None
    
    # ========== 3. 设置混合精度 ==========
    device_type = "cuda" if "cuda" in args.device else "cpu"
    dtype = torch.bfloat16 if args.dtype == "bfloat16" else torch.float16
    autocast_ctx = nullcontext() if device_type == "cpu" else torch.cuda.amp.autocast(dtype=dtype)
    
    # ========== 4. 配wandb ==========
    wandb = None
    if args.use_wandb and is_main_process():
        import swanlab as wandb
        wandb_id = ckp_data.get('wandb_id') if ckp_data else None
        resume = 'must' if wandb_id else None
        wandb_run_name = f"MiniMind-Pretrain-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}"
        wandb.init(project=args.wandb_project, name=wandb_run_name, id=wandb_id, resume=resume)
    
    # ========== 5. 定义模型、数据、优化器 ==========
    model, tokenizer = init_model(lm_config, args.from_weight, device=args.device)
    train_ds = PretrainDataset(args.data_path, tokenizer, max_length=args.max_seq_len)
    train_sampler = DistributedSampler(train_ds) if dist.is_initialized() else None
    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == 'float16'))
    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)
    
    # ========== 6. 从ckp恢复状态 ==========
    start_epoch, start_step = 0, 0
    if ckp_data:
        model.load_state_dict(ckp_data['model'])
        optimizer.load_state_dict(ckp_data['optimizer'])
        scaler.load_state_dict(ckp_data['scaler'])
        start_epoch = ckp_data['epoch']
        start_step = ckp_data.get('step', 0)
    
    # ========== 7. DDP包模型 ==========
    if dist.is_initialized():
        model._ddp_params_and_buffers_to_ignore = {"freqs_cos", "freqs_sin"}
        model = DistributedDataParallel(model, device_ids=[local_rank])
    
    # ========== 8. 开始训练 ==========
    for epoch in range(start_epoch, args.epochs):
        train_sampler and train_sampler.set_epoch(epoch)
        if epoch == start_epoch and start_step > 0: # 第一个epoch且存在检查点
            batch_sampler = SkipBatchSampler(train_sampler or range(len(train_ds)), args.batch_size, start_step + 1)
            loader = DataLoader(train_ds, batch_sampler=batch_sampler, num_workers=args.num_workers, pin_memory=True)
            Logger(f'Epoch [{epoch + 1}/{args.epochs}]: 跳过前{start_step}个step，从step {start_step + 1}开始')
            train_epoch(epoch, loader, len(loader) + start_step + 1, start_step, wandb)
        else: # 默认从头开始
            loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=(train_sampler is None), sampler=train_sampler, num_workers=args.num_workers, pin_memory=True)
            train_epoch(epoch, loader, len(loader), 0, wandb)
    
    # ========== 9. 清理分布进程 ==========
    if dist.is_initialized(): dist.destroy_process_group()