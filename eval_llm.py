"""
================================================================================
                    MiniMind æ¨¡å‹è¯„ä¼°ä¸å¯¹è¯è„šæœ¬
================================================================================

ã€åŠŸèƒ½ã€‘
1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
2. æ”¯æŒè‡ªåŠ¨æµ‹è¯•å’Œäº¤äº’å¯¹è¯
3. æ”¯æŒ LoRA é€‚é…å™¨åŠ è½½
4. æ˜¾ç¤ºç”Ÿæˆé€Ÿåº¦

ã€æ”¯æŒçš„æƒé‡ç±»å‹ã€‘
- pretrain: é¢„è®­ç»ƒæ¨¡å‹ (åªå­¦äº†è¯­è¨€è§„å¾‹ï¼Œä¸ä¼šå¯¹è¯)
- full_sft: ç›‘ç£å¾®è°ƒæ¨¡å‹ (å­¦ä¼šäº†å¯¹è¯)
- dpo: DPO å¯¹é½åçš„æ¨¡å‹
- reason: æ¨ç†æ¨¡å‹ (ä¼šä½¿ç”¨ <think>...<answer> æ ¼å¼)
- ppo_actor/grpo/spo: å¼ºåŒ–å­¦ä¹ è®­ç»ƒåçš„æ¨¡å‹

ã€ä½¿ç”¨æ–¹æ³•ã€‘
python eval_llm.py --weight full_sft --hidden_size 512

ã€äº¤äº’æ¨¡å¼ã€‘
è¿è¡Œåé€‰æ‹©:
- 0: è‡ªåŠ¨æµ‹è¯•é¢„è®¾é—®é¢˜
- 1: æ‰‹åŠ¨è¾“å…¥é—®é¢˜å¯¹è¯
"""

import time
import argparse
import random
import warnings
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM
from model.model_lora import *
from trainer.trainer_utils import setup_seed, get_model_params
warnings.filterwarnings('ignore')

def init_model(args):
    """
    åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    
    ã€åŠ è½½æ–¹å¼ã€‘
    1. æœ¬åœ° PyTorch æƒé‡ (load_from='model')
       - ä» out/ ç›®å½•åŠ è½½ .pth æ–‡ä»¶
       - æ”¯æŒ LoRA é€‚é…å™¨
    
    2. HuggingFace æ¨¡å‹ (load_from='path/to/model')
       - ç›´æ¥åŠ è½½ transformers æ ¼å¼çš„æ¨¡å‹
    
    ã€LoRA åŠ è½½ã€‘
    å¦‚æœæŒ‡å®šäº† lora_weightï¼Œä¼š:
    1. å…ˆä¸ºæ¨¡å‹æ·»åŠ  LoRA å±‚
    2. ç„¶ååŠ è½½ LoRA æƒé‡
    """
    tokenizer = AutoTokenizer.from_pretrained(args.load_from)
    
    if 'model' in args.load_from:
        # æœ¬åœ°æƒé‡åŠ è½½
        model = MiniMindForCausalLM(MiniMindConfig(
            hidden_size=args.hidden_size,
            num_hidden_layers=args.num_hidden_layers,
            use_moe=bool(args.use_moe),
            inference_rope_scaling=args.inference_rope_scaling
        ))
        moe_suffix = '_moe' if args.use_moe else ''
        ckp = f'./{args.save_dir}/{args.weight}_{args.hidden_size}{moe_suffix}.pth'
        model.load_state_dict(torch.load(ckp, map_location=args.device), strict=True)
        
        # åŠ è½½ LoRA é€‚é…å™¨ (å¦‚æœæŒ‡å®š)
        if args.lora_weight != 'None':
            apply_lora(model)
            load_lora(model, f'./{args.save_dir}/lora/{args.lora_weight}_{args.hidden_size}.pth')
    else:
        # HuggingFace æ¨¡å‹åŠ è½½
        model = AutoModelForCausalLM.from_pretrained(args.load_from, trust_remote_code=True)
    
    get_model_params(model, model.config)
    return model.eval().to(args.device), tokenizer

def main():
    parser = argparse.ArgumentParser(description="MiniMindæ¨¡å‹æ¨ç†ä¸å¯¹è¯")
    parser.add_argument('--load_from', default='model', type=str, help="æ¨¡å‹åŠ è½½è·¯å¾„ï¼ˆmodel=åŸç”Ÿtorchæƒé‡ï¼Œå…¶ä»–è·¯å¾„=transformersæ ¼å¼ï¼‰")
    parser.add_argument('--save_dir', default='out', type=str, help="æ¨¡å‹æƒé‡ç›®å½•")
    parser.add_argument('--weight', default='full_sft', type=str, help="æƒé‡åç§°å‰ç¼€ï¼ˆpretrain, full_sft, rlhf, reason, ppo_actor, grpo, spoï¼‰")
    parser.add_argument('--lora_weight', default='None', type=str, help="LoRAæƒé‡åç§°ï¼ˆNoneè¡¨ç¤ºä¸ä½¿ç”¨ï¼Œå¯é€‰ï¼šlora_identity, lora_medicalï¼‰")
    parser.add_argument('--hidden_size', default=512, type=int, help="éšè—å±‚ç»´åº¦ï¼ˆ512=Small-26M, 640=MoE-145M, 768=Base-104Mï¼‰")
    parser.add_argument('--num_hidden_layers', default=8, type=int, help="éšè—å±‚æ•°é‡ï¼ˆSmall/MoE=8, Base=16ï¼‰")
    parser.add_argument('--use_moe', default=0, type=int, choices=[0, 1], help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    parser.add_argument('--inference_rope_scaling', default=False, action='store_true', help="å¯ç”¨RoPEä½ç½®ç¼–ç å¤–æ¨ï¼ˆ4å€ï¼Œä»…è§£å†³ä½ç½®ç¼–ç é—®é¢˜ï¼‰")
    parser.add_argument('--max_new_tokens', default=8192, type=int, help="æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆæ³¨æ„ï¼šå¹¶éæ¨¡å‹å®é™…é•¿æ–‡æœ¬èƒ½åŠ›ï¼‰")
    parser.add_argument('--temperature', default=0.85, type=float, help="ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šéšæœºï¼‰")
    parser.add_argument('--top_p', default=0.85, type=float, help="nucleusé‡‡æ ·é˜ˆå€¼ï¼ˆ0-1ï¼‰")
    parser.add_argument('--historys', default=0, type=int, help="æºå¸¦å†å²å¯¹è¯è½®æ•°ï¼ˆéœ€ä¸ºå¶æ•°ï¼Œ0è¡¨ç¤ºä¸æºå¸¦å†å²ï¼‰")
    parser.add_argument('--show_speed', default=1, type=int, help="æ˜¾ç¤ºdecodeé€Ÿåº¦ï¼ˆtokens/sï¼‰")
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str, help="è¿è¡Œè®¾å¤‡")
    args = parser.parse_args()
    
    prompts = [
        'ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ',
        'ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„',
        'è¯·ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°',
        'è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹',
        'å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨',
        'æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹',
        'è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ',
        'æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿ'
    ]
    
    conversation = []
    model, tokenizer = init_model(args)
    input_mode = int(input('[0] è‡ªåŠ¨æµ‹è¯•\n[1] æ‰‹åŠ¨è¾“å…¥\n'))
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    
    prompt_iter = prompts if input_mode == 0 else iter(lambda: input('ğŸ’¬: '), '')
    for prompt in prompt_iter:
        setup_seed(2026) # or setup_seed(random.randint(0, 2048))
        if input_mode == 0: print(f'ğŸ’¬: {prompt}')
        conversation = conversation[-args.historys:] if args.historys else []
        conversation.append({"role": "user", "content": prompt})

        templates = {"conversation": conversation, "tokenize": False, "add_generation_prompt": True}
        if args.weight == 'reason': templates["enable_thinking"] = True # ä»…Reasonæ¨¡å‹ä½¿ç”¨
        inputs = tokenizer.apply_chat_template(**templates) if args.weight != 'pretrain' else (tokenizer.bos_token + prompt)
        inputs = tokenizer(inputs, return_tensors="pt", truncation=True).to(args.device)

        print('ğŸ¤–: ', end='')
        st = time.time()
        generated_ids = model.generate(
            inputs=inputs["input_ids"], attention_mask=inputs["attention_mask"],
            max_new_tokens=args.max_new_tokens, do_sample=True, streamer=streamer,
            pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id,
            top_p=args.top_p, temperature=args.temperature, repetition_penalty=1.0
        )
        response = tokenizer.decode(generated_ids[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
        conversation.append({"role": "assistant", "content": response})
        gen_tokens = len(generated_ids[0]) - len(inputs["input_ids"][0])
        print(f'\n[Speed]: {gen_tokens / (time.time() - st):.2f} tokens/s\n\n') if args.show_speed else print('\n\n')

if __name__ == "__main__":
    main()