"""
================================================================================
                    LoRA (Low-Rank Adaptation) 实现
================================================================================

【什么是 LoRA】
LoRA 是一种高效的微调方法，核心思想是:
- 冻结预训练模型的权重
- 在需要微调的层旁边添加低秩分解的小矩阵
- 只训练这些小矩阵，大大减少可训练参数

【数学原理】
原始权重矩阵 W ∈ R^(d×k)
LoRA 添加: ΔW = B @ A，其中 A ∈ R^(r×k), B ∈ R^(d×r), r << min(d,k)

前向传播: y = W @ x + ΔW @ x = W @ x + B @ A @ x

【@ 是什么？矩阵乘法！】
Python 3.5+ 引入: A @ B 等价于 torch.matmul(A, B)

矩阵乘法规则: 结果[i,j] = A的第i行 · B的第j列（点积）

例: A=[2,3矩阵], B=[3,4矩阵] → A@B=[2,4矩阵]

【LoRA 中的计算流程】（假设 dim=512, rank=8）
1. A @ x: [8,512] @ [512] = [8]      压缩到8维
2. B @ (A@x): [512,8] @ [8] = [512]  投影回512维
3. W @ x: [512,512] @ [512] = [512]  原始结果
4. 相加得到最终输出 [512]

低秩的妙处: 用 8192 参数近似 262144 参数的效果！

【A/B 是怎么被训练的？】

关键: requires_grad 控制谁被训练！

原始权重 W: requires_grad = False  ← 冻结，不训练
LoRA 的 A: requires_grad = True   ← 可训练
LoRA 的 B: requires_grad = True   ← 可训练

前向: y = W @ x + B @ A @ x
反向: loss.backward() 只算 A 和 B 的梯度（因为 W 冻结了）
更新: optimizer.step() 只更新 A 和 B

A 和 B 被"同时"训练：链式法则自动算两个的梯度！

【参数如何减少？】

全量微调 W [512×512]: 262,144 参数
LoRA A[8,512] + B[512,8]: 8,192 参数  ← 减少32倍！

为什么能用这么少？
- 微调的权重变化 ΔW 通常是"低秩"的
- 预训练模型已经很好，微调改动很小
- 这些改动集中在几个"方向"上，rank=8 就够了

【为什么有效】
1. 预训练模型的权重已经很好，微调只需要小的调整
2. 这些调整通常是低秩的 (rank << d)
3. 只训练 r×(d+k) 个参数，而不是 d×k 个

【典型设置】
- rank=8 或 rank=16
- 只在 attention 的 Q/K/V/O 投影层使用
- 可以减少 90%+ 的可训练参数

【优势】
1. 显存占用少：只需存储小矩阵的梯度
2. 训练快：参数少，优化器状态小
3. 可插拔：可以轻松切换不同的 LoRA 适配器
4. 保持原模型：原模型权重不变，不会灾难性遗忘
"""

import torch
from torch import optim, nn


class LoRA(nn.Module):
    """
    LoRA 低秩适配器
    
    【结构】
    原始层: y = W @ x
    添加 LoRA 后: y = W @ x + B @ A @ x
    
    其中:
    - A: [in_features, rank] 下投影矩阵
    - B: [rank, out_features] 上投影矩阵
    - rank << in_features, out_features
    
    【初始化】
    - A: 高斯初始化 (std=0.02)
    - B: 零初始化
    - 这样初始时 ΔW = B @ A = 0，LoRA 不改变原模型行为
    """
    def __init__(self, in_features, out_features, rank):
        super().__init__()
        self.rank = rank  # LoRA 的秩，控制适配器容量
        
        # ================================================================
        # nn.Linear 是什么？（PyTorch 最基础的层之一）
        # ================================================================
        #
        # 【本质】一个带权重矩阵的线性变换，就是 y = x @ W^T + b
        #
        # 【数据结构】nn.Linear 包含两个成员：
        #   - weight: 权重矩阵，形状 [out_features, in_features]
        #   - bias:   偏置向量，形状 [out_features]（如果 bias=False 则为 None）
        #
        # 【计算过程】
        #   输入 x:  形状 [batch_size, in_features]
        #   输出 y:  形状 [batch_size, out_features]
        #   计算:    y = x @ weight.T + bias
        #
        # 【举例】nn.Linear(512, 8, bias=False)
        #   - weight 形状: [8, 512]
        #   - 输入 x: [batch, 512]
        #   - 计算: x @ weight.T = [batch, 512] @ [512, 8] = [batch, 8]
        #   - 效果: 把 512 维向量压缩成 8 维
        #
        # 【特点】
        #   1. 可训练: weight 和 bias 自动设置 requires_grad=True
        #   2. 自动注册: 会被 model.parameters() 收集到
        #   3. 纯线性: 没有激活函数，需要的话要额外加
        #
        # ================================================================
        
        # 低秩分解: ΔW = B @ A
        # A: 下投影 in_features → rank（压缩维度）
        self.A = nn.Linear(in_features, rank, bias=False)
        # B: 上投影 rank → out_features（恢复维度）
        self.B = nn.Linear(rank, out_features, bias=False)
        
        # ================================================================
        # 初始化策略详解（高中数学就能懂！）
        # ================================================================
        #
        # 【目标】训练刚开始时，LoRA 的输出必须是 0
        #
        # 为什么？因为 LoRA 的计算是：
        #   最终输出 = 原模型输出 + LoRA输出
        #            = W @ x      + B @ A @ x
        #
        # 如果一开始 LoRA输出 ≠ 0，就会"污染"原模型，
        # 导致预训练学到的知识被破坏，模型可能直接崩掉！
        #
        # 【如何让 LoRA 输出为 0？】
        #
        # LoRA输出 = B @ A @ x
        #
        # 想让结果为 0，最简单的办法：让 B 或 A 其中一个是零矩阵！
        # 因为：任何数 × 0 = 0
        #
        # 【为什么选择 B=0，而不是 A=0？】
        #
        # 其实两种都可以！但必须有一个非零，原因如下：
        #
        # 神经网络通过"梯度"来学习，梯度就像"指路牌"，
        # 告诉参数应该往哪个方向调整。
        #
        # 如果 A 和 B 都是 0：
        #   - 计算 A 的梯度时，需要用到 B 的值
        #   - 计算 B 的梯度时，需要用到 A 的值
        #   - 两个都是 0，梯度也都是 0
        #   - 梯度为 0 = 没有"指路牌" = 参数不知道往哪调 = 永远学不动！
        #
        # 这就像两个人互相等对方先动，结果谁都不动，卡死了。
        #
        # 【解决方案】
        #
        # 让其中一个非零（比如 A），另一个为零（比如 B）：
        #   - 初始输出：B @ A @ x = 0 @ A @ x = 0  ✓ 满足目标
        #   - B 的梯度 ∝ A @ x，因为 A ≠ 0，所以 B 有梯度，能学习
        #   - 第一步更新后 B ≠ 0 了，A 也能开始学习
        #   - 两个矩阵都能正常训练！
        #
        # 【为什么 A 用高斯分布，std=0.02？】
        #
        # 高斯分布（正态分布）就是那个钟形曲线，大部分值在 0 附近。
        # std=0.02 表示标准差很小，值大约在 -0.06 到 +0.06 之间。
        #
        # 为什么要这么小？
        #   - 太大：数值不稳定，训练可能"爆炸"
        #   - 太小：学习起步太慢
        #   - 0.02：经过大量实验验证的"甜点"值
        #
        # ================================================================
        
        # A: 高斯初始化，让梯度能够流动，LoRA 能学到东西
        self.A.weight.data.normal_(mean=0.0, std=0.02)
        # B: 零初始化，确保初始时 LoRA 输出 = B @ A @ x = 0
        self.B.weight.data.zero_()

    def forward(self, x):
        """LoRA 前向传播: x → A → B → output"""
        return self.B(self.A(x))


def apply_lora(model, rank=8):
    """
    为模型的线性层添加 LoRA 适配器
    
    【应用策略】
    只为方阵线性层添加 LoRA:
    - 这通常是 attention 层中的 Q/K/V/O 投影
    - 方阵特征: weight.shape[0] == weight.shape[1]
    
    【实现方式】
    1. 为每个目标层创建 LoRA 模块
    2. 修改层的 forward 方法: output = original(x) + lora(x)
    
    【参数】
    - model: 要添加 LoRA 的模型
    - rank: LoRA 的秩，越大容量越大，但参数也越多
    """
    # ================================================================
    #                    LLM 基础概念大白话解释
    #              （高中生也能看懂的 Transformer 原理）
    # ================================================================
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │                  零、LLM 的完整结构图                        │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 一个完整的 LLM (如 MiniMind) 包含以下层次结构:
    #
    # ┌─────────────────────────────────────────────────────────────────┐
    # │                    MiniMindForCausalLM                          │
    # │  (完整的语言模型，用于生成下一个词)                            │
    # ├─────────────────────────────────────────────────────────────────┤
    # │                                                                 │
    # │  ┌─────────────────────────────────────────────────────────┐   │
    # │  │ 1. embed_tokens (词嵌入层)                              │   │
    # │  │    nn.Embedding(vocab_size=6400, hidden_size=512)       │   │
    # │  │    作用: 把 token ID 变成 512 维向量                    │   │
    # │  │    参数量: 6400 × 512 = 3.3M                            │   │
    # │  └─────────────────────────────────────────────────────────┘   │
    # │                           ↓                                     │
    # │  ┌─────────────────────────────────────────────────────────┐   │
    # │  │ 2. Transformer Blocks × N层 (N = num_hidden_layers)    │   │
    # │  │                                                         │   │
    # │  │    【num_hidden_layers 是什么？】                       │   │
    # │  │                                                         │   │
    # │  │    它指的是 Transformer Block 堆叠的层数！              │   │
    # │  │    - num_hidden_layers = 8  → 8 个 Block 堆叠           │   │
    # │  │    - num_hidden_layers = 16 → 16 个 Block 堆叠          │   │
    # │  │                                                         │   │
    # │  │    【为什么叫"隐藏层"？】                                │   │
    # │  │                                                         │   │
    # │  │    神经网络的结构:                                      │   │
    # │  │    输入层 → 隐藏层1 → 隐藏层2 → ... → 输出层           │   │
    # │  │                                                         │   │
    # │  │    - 输入层: embed_tokens (能看到)                      │   │
    # │  │    - 隐藏层: Transformer Blocks (看不到，在中间处理)   │   │
    # │  │    - 输出层: lm_head (能看到)                           │   │
    # │  │                                                         │   │
    # │  │    "隐藏"是因为这些层的输出不直接可见，                │   │
    # │  │    只在网络内部流动。                                   │   │
    # │  │                                                         │   │
    # │  │    【层数越多 = 模型越"深"】                            │   │
    # │  │                                                         │   │
    # │  │    - 8层: 小模型，参数少，速度快                        │   │
    # │  │    - 32层: 中等模型 (如 LLaMA-7B)                       │   │
    # │  │    - 80层: 大模型 (如 LLaMA-65B)                        │   │
    # │  │                                                         │   │
    # │  │    层数多 → 能学到更复杂的模式 → 但也更慢更贵          │   │
    # │  │                                                         │   │
    # │  └─────────────────────────────────────────────────────────┘   │
    # │  ┌─────────────────────────────────────────────────────────┐   │
    # │  │    每个 Block 包含:                                     │   │
    # │  │    ┌─────────────────────────────────────────────────┐  │   │
    # │  │    │ 2.1 input_layernorm (RMSNorm)                   │  │   │
    # │  │    │     作用: 归一化，稳定训练                       │  │   │
    # │  │    │     参数量: 512                                  │  │   │
    # │  │    └─────────────────────────────────────────────────┘  │   │
    # │  │                         ↓                                │   │
    # │  │    ┌─────────────────────────────────────────────────┐  │   │
    # │  │    │ 2.2 self_attn (自注意力层)                      │  │   │
    # │  │    │     ├─ q_proj: Linear(512→512)  生成Query       │  │   │
    # │  │    │     ├─ k_proj: Linear(512→128)  生成Key (GQA)   │  │   │
    # │  │    │     ├─ v_proj: Linear(512→128)  生成Value (GQA) │  │   │
    # │  │    │     └─ o_proj: Linear(512→512)  输出投影        │  │   │
    # │  │    │     参数量: 512×512 + 512×128×2 + 512×512       │  │   │
    # │  │    │            = 262K + 131K + 262K = 655K          │  │   │
    # │  │    └─────────────────────────────────────────────────┘  │   │
    # │  │                         ↓ (+残差连接)                    │   │
    # │  │    ┌─────────────────────────────────────────────────┐  │   │
    # │  │    │ 2.3 post_attention_layernorm (RMSNorm)          │  │   │
    # │  │    │     参数量: 512                                  │  │   │
    # │  │    └─────────────────────────────────────────────────┘  │   │
    # │  │                         ↓                                │   │
    # │  │    ┌─────────────────────────────────────────────────┐  │   │
    # │  │    │ 2.4 mlp (前馈网络 FFN，或 MoE)                  │  │   │
    # │  │    │     ├─ gate_proj: Linear(512→1376)  门控        │  │   │
    # │  │    │     ├─ up_proj:   Linear(512→1376)  上投影      │  │   │
    # │  │    │     └─ down_proj: Linear(1376→512)  下投影      │  │   │
    # │  │    │     参数量: 512×1376×2 + 1376×512 = 2.1M       │  │   │
    # │  │    └─────────────────────────────────────────────────┘  │   │
    # │  │                         ↓ (+残差连接)                    │   │
    # │  │    每层参数量 ≈ 655K + 2.1M + 1K = 2.8M                 │   │
    # │  │    8层总计 ≈ 22M                                         │   │
    # │  └─────────────────────────────────────────────────────────┘   │
    # │                           ↓                                     │
    # │  ┌─────────────────────────────────────────────────────────┐   │
    # │  │ 3. norm (最终 RMSNorm)                                  │   │
    # │  │    作用: 输出前的最后归一化                              │   │
    # │  │    参数量: 512                                           │   │
    # │  └─────────────────────────────────────────────────────────┘   │
    # │                           ↓                                     │
    # │  ┌─────────────────────────────────────────────────────────┐   │
    # │  │ 4. lm_head (语言模型头)                                 │   │
    # │  │    nn.Linear(hidden_size=512, vocab_size=6400)          │   │
    # │  │    作用: 预测下一个 token 的概率分布                    │   │
    # │  │    参数量: 512 × 6400 = 3.3M                            │   │
    # │  └─────────────────────────────────────────────────────────┘   │
    # │                           ↓                                     │
    # │                    输出: [batch, seq, 6400]                     │
    # │                    每个位置预测 6400 个词的概率                │
    # │                                                                 │
    # ├─────────────────────────────────────────────────────────────────┤
    # │  【MiniMind 总参数量估算】                                     │
    # │                                                                 │
    # │  embed_tokens:           3.3M                                  │
    # │  8层 Transformer Block: 22.0M                                  │
    # │  final norm:             0.0005M                               │
    # │  lm_head:                3.3M (常与 embed_tokens 共享权重)     │
    # │  ─────────────────────────────                                 │
    # │  总计约: 26M ~ 30M 参数                                        │
    # │                                                                 │
    # └─────────────────────────────────────────────────────────────────┘
    #
    # 【数据流动过程】
    #
    # 输入: "今天天气"
    #   ↓
    # Token IDs: [156, 892, 892, 231]
    #   ↓
    # embed_tokens: 每个 ID → 512维向量 → [4, 512]
    #   ↓
    # Transformer Block 1: 自注意力 + FFN → [4, 512]
    #   ↓
    # Transformer Block 2: 自注意力 + FFN → [4, 512]
    #   ↓
    # ... (重复 N 层)
    #   ↓
    # final norm: 归一化 → [4, 512]
    #   ↓
    # lm_head: 512维 → 6400维 → [4, 6400]
    #   ↓
    # softmax: 变成概率 → 第4个位置概率最高的词是"很"
    #   ↓
    # 输出: "今天天气很"
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │                    一、什么是 Token 和 Vocab？               │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【Token = 语言的最小单位】
    # 电脑不认识文字，只认识数字。所以要把文字切成小块，每块对应一个数字。
    #
    # 例子: "我爱中国" → ["我", "爱", "中", "国"] → [156, 892, 45, 231]
    #
    # 【Vocab (词表) = 所有 token 的字典】
    # vocab_size = 6400 表示字典里有 6400 个"词"
    # 每个词对应一个 ID: "我"→156, "爱"→892, ...
    #
    # 【为什么 vocab 越大，模型越大？】
    #
    # 模型中有两个地方的大小和 vocab_size 直接相关:
    #
    # 1. 词嵌入层 (embed_tokens):
    #    - 每个词需要一个 hidden_size 维的向量
    #    - 参数量 = vocab_size × hidden_size
    #    - 例: 6400 × 512 = 3,276,800 个参数 (约 3.3M)
    #    - 如果 vocab=50000: 50000 × 512 = 25,600,000 (约 25.6M)
    #
    # 2. 输出层 (lm_head):
    #    - 把 hidden_size 映射到 vocab_size，预测下一个词
    #    - 参数量 = hidden_size × vocab_size
    #    - 和词嵌入一样大！
    #
    #    【疑问】512 维怎么能映射到 6400 维？这不是变多了吗？
    #
    #    这是个好问题！关键在于理解"映射"的含义:
    #
    #    矩阵乘法可以改变维度！
    #    输入: [1, 512] 的向量
    #    权重: [512, 6400] 的矩阵
    #    输出: [1, 512] @ [512, 6400] = [1, 6400]
    #
    #    具体计算:
    #    ┌─────────────────────────────────────────────────────────┐
    #    │  输出的第 i 个值 = 输入向量 · 权重矩阵的第 i 列         │
    #    │                                                         │
    #    │  output[0] = x[0]*W[0,0] + x[1]*W[1,0] + ... + x[511]*W[511,0]  │
    #    │  output[1] = x[0]*W[0,1] + x[1]*W[1,1] + ... + x[511]*W[511,1]  │
    #    │  ...                                                    │
    #    │  output[6399] = x[0]*W[0,6399] + ... + x[511]*W[511,6399]│
    #    └─────────────────────────────────────────────────────────┘
    #
    #    每个输出值是 512 个数的加权和，权重是学出来的！
    #
    #    【类比】就像投票:
    #    - 512 个评委 (隐藏维度)
    #    - 6400 个候选人 (词表)
    #    - 每个评委对每个候选人有个打分权重
    #    - 最终分数 = 所有评委的加权打分之和
    #    - 分数最高的候选人获胜 (预测的下一个词)
    #
    #    【为什么能工作？】
    #    - 512 维足够编码语义信息
    #    - lm_head 学会了"如果隐藏向量长这样，下一个词很可能是 X"
    #    - 通过 softmax 把 6400 个分数变成概率分布
    #
    # 【总计】vocab 相关参数 = 2 × vocab_size × hidden_size
    #
    #   vocab=6400:   2 × 6400 × 512   = 6.5M 参数
    #   vocab=50000:  2 × 50000 × 512  = 51.2M 参数
    #   vocab=150000: 2 × 150000 × 512 = 153.6M 参数
    #
    # 对于小模型，vocab 相关参数可能占总参数的 30%~50%！
    # 所以 MiniMind 用较小的 vocab (6400) 来控制模型大小。
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │              二、什么是 Hidden Size (隐藏维度)？             │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【每个 token 用一个向量表示】
    # hidden_size = 512 表示每个词用 512 个数字来描述。
    #
    # 为什么要这么多数字？因为一个词的含义很丰富:
    # - "苹果" 可能是水果，也可能是手机品牌
    # - 需要足够多的维度来区分不同语境下的含义
    #
    # 想象成: 每个词是 512 维空间中的一个点
    # - 相似的词（如"开心"、"快乐"）在空间中靠近
    # - 不同的词（如"开心"、"悲伤"）在空间中远离
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │           三、什么是注意力 (Attention)？Q/K/V 是什么？       │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【问题】理解一个词，需要看它周围的词
    #
    # 例: "苹果很好吃" vs "苹果发布新手机"
    #     同样是"苹果"，但含义完全不同！
    #     → 需要根据上下文来理解每个词
    #
    # 【注意力机制 = 让每个词"看"其他词，决定该关注谁】
    #
    # 用一个生活中的比喻来解释 Q/K/V:
    #
    # 想象你在图书馆找书:
    # ┌─────────────────────────────────────────────────────────────┐
    # │                                                             │
    # │  你 (Query): "我想找关于 Python 编程的书"                   │
    # │       ↓                                                     │
    # │  书架上每本书的标签 (Key):                                  │
    # │     - 书1: "Python 入门"      ← 很匹配！                    │
    # │     - 书2: "Java 编程"        ← 有点相关                    │
    # │     - 书3: "法国历史"         ← 不相关                      │
    # │       ↓                                                     │
    # │  取出书的内容 (Value):                                      │
    # │     - 主要看书1的内容 (权重高)                              │
    # │     - 稍微看看书2 (权重低)                                  │
    # │     - 忽略书3 (权重≈0)                                      │
    # │                                                             │
    # └─────────────────────────────────────────────────────────────┘
    #
    # 【Q/K/V 的数学定义】
    #
    # 对于句子中的每个词:
    #   Q (Query)  = "我在找什么？" → 这个词想要的信息
    #   K (Key)    = "我有什么标签？" → 这个词能提供什么
    #   V (Value)  = "我的具体内容" → 这个词的实际信息
    #
    # 【注意力计算步骤】
    #
    # 1. 计算相关度: score = Q · K^T  (点积，越大越相关)
    # 2. 归一化: weights = softmax(score)  (变成概率，和为1)
    # 3. 加权求和: output = weights × V  (按相关度混合信息)
    #
    # 【具体例子】"苹果 很 好吃"
    #
    # 当处理"苹果"时:
    #   Q_苹果 与 K_很、K_好吃 计算相关度
    #   → 发现和"好吃"很相关 (权重0.7)
    #   → 和"很"有点相关 (权重0.3)
    #   → 输出 = 0.7 × V_好吃 + 0.3 × V_很
    #   → "苹果"的表示融合了"好吃"的信息，模型知道这是水果！
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │             四、什么是注意力头 (Attention Head)？            │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【问题】一个词可能从多个角度需要不同的信息
    #
    # 例: "小明 昨天 在 北京 吃了 苹果"
    #
    # 对于"吃了"这个词，需要知道:
    # - 谁吃的？→ 关注"小明" (语法关系)
    # - 什么时候？→ 关注"昨天" (时间关系)
    # - 在哪吃？→ 关注"北京" (地点关系)
    # - 吃了什么？→ 关注"苹果" (动宾关系)
    #
    # 【多头注意力 = 多个独立的"视角"】
    #
    # num_attention_heads = 8 表示有 8 个注意力头
    #
    # 每个头学习关注不同类型的关系:
    # - 头1: 可能学会关注主谓关系
    # - 头2: 可能学会关注动宾关系
    # - 头3: 可能学会关注时间词
    # - ...
    #
    # 【head_dim = hidden_size / num_heads】
    #
    # hidden_size=512, num_heads=8 → head_dim=64
    #
    # 每个头处理 64 维的子空间，8 个头合起来是 512 维
    # 这样既有多个视角，又保持总维度不变
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │               五、什么是 KV 头 (GQA 技术)？                  │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【问题】注意力计算中，K 和 V 需要大量显存
    #
    # 推理时需要缓存之前所有 token 的 K 和 V (叫 KV Cache)
    # 如果对话很长，KV Cache 会占用几个 GB 显存！
    #
    # 【GQA (分组查询注意力) = 让多个 Q 头共享 K/V 头】
    #
    # num_attention_heads = 8 (Q 头数)
    # num_key_value_heads = 2 (KV 头数)
    #
    # 意思是: 8 个 Q 头，只有 2 个 KV 头
    # 每 4 个 Q 头共享 1 个 KV 头
    #
    # ┌─────────────────────────────────────────────┐
    # │  Q头: [Q1, Q2, Q3, Q4] [Q5, Q6, Q7, Q8]     │
    # │         ↓     ↓     ↓     ↓      ↓     ↓    │
    # │  KV头:      [KV1]            [KV2]          │
    # └─────────────────────────────────────────────┘
    #
    # 【效果】
    # - 显存减少 4 倍 (KV Cache 从 8 份变 2 份)
    # - 推理速度更快
    # - 效果几乎不下降 (研究证明 KV 头可以共享)
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │           六、Q/K/V/O 投影层是什么？为什么需要？             │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【原始输入】每个 token 是 512 维向量 x
    #
    # 【问题】同一个向量 x，如何变出 Q、K、V 三个不同的东西？
    #
    # 【答案】用三个不同的矩阵去"投影"！
    #
    #   Q = x @ W_q  (W_q 是 q_proj 的权重)
    #   K = x @ W_k  (W_k 是 k_proj 的权重)
    #   V = x @ W_v  (W_v 是 v_proj 的权重)
    #
    # 不同的投影矩阵，让同一个输入变成不同角色:
    # - W_q 学习: "这个词想找什么信息"
    # - W_k 学习: "这个词能提供什么信息"
    # - W_v 学习: "这个词的实际内容是什么"
    #
    # 【O 投影 (输出投影)】
    #
    # 多头注意力的输出需要合并回 512 维:
    #   output = concat(head1, head2, ..., head8) @ W_o
    #
    # W_o 学习如何融合多个头的信息
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │           七、什么是 intermediate_size (FFN 中间层)？        │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【Transformer 的两大组件】
    # 1. Attention: 让词与词之间交流信息
    # 2. FFN (前馈网络): 对每个词独立做非线性变换
    #
    # 【FFN 结构】
    #   输入 512 维 → 扩展到 1376 维 → 压缩回 512 维
    #
    # intermediate_size = 1376 就是这个中间层的维度
    #
    # 【为什么要先扩展再压缩？】
    #
    # 就像画家画画:
    # - 先在大画布上画草稿 (扩展维度，有更多空间表达)
    # - 然后精选最好的部分 (压缩维度，提取精华)
    #
    # 中间层越大，模型的"表达空间"越大，能学到更复杂的模式
    #
    # ================================================================
    # LLM 中所有 nn.Linear 层的完整清单（以 MiniMind 为例）
    # ================================================================
    #
    # 假设配置: hidden_size=512, num_attention_heads=8, num_key_value_heads=2
    #          intermediate_size=1376, vocab_size=6400
    #
    # 【1. Attention 层的投影矩阵】
    # ┌─────────────┬────────────────────────┬──────────────┬──────────┐
    # │ 名称        │ 作用                   │ 形状         │ 是方阵？ │
    # ├─────────────┼────────────────────────┼──────────────┼──────────┤
    # │ q_proj      │ 生成 Query             │ [512, 512]   │ ✓ 是     │
    # │ k_proj      │ 生成 Key (GQA 共享)    │ [128, 512]   │ ✗ 否     │
    # │ v_proj      │ 生成 Value (GQA 共享)  │ [128, 512]   │ ✗ 否     │
    # │ o_proj      │ 输出投影               │ [512, 512]   │ ✓ 是     │
    # └─────────────┴────────────────────────┴──────────────┴──────────┘
    #
    # 【2. FFN/MLP 层的投影矩阵】
    # ┌─────────────┬────────────────────────┬──────────────┬──────────┐
    # │ 名称        │ 作用                   │ 形状         │ 是方阵？ │
    # ├─────────────┼────────────────────────┼──────────────┼──────────┤
    # │ gate_proj   │ 门控信号 (SwiGLU)      │ [1376, 512]  │ ✗ 否     │
    # │ up_proj     │ 上投影 (扩展维度)      │ [1376, 512]  │ ✗ 否     │
    # │ down_proj   │ 下投影 (压缩回来)      │ [512, 1376]  │ ✗ 否     │
    # └─────────────┴────────────────────────┴──────────────┴──────────┘
    #
    # 【3. 其他层】
    # ┌─────────────┬────────────────────────┬──────────────┬──────────┐
    # │ 名称        │ 作用                   │ 形状         │ 是方阵？ │
    # ├─────────────┼────────────────────────┼──────────────┼──────────┤
    # │ embed_tokens│ 词嵌入 (Embedding)     │ [6400, 512]  │ ✗ 否     │
    # │ lm_head     │ 输出词表概率           │ [6400, 512]  │ ✗ 否     │
    # └─────────────┴────────────────────────┴──────────────┴──────────┘
    #
    # ================================================================
    # 为什么用 shape[0] == shape[1] 判断？
    # ================================================================
    #
    # 【核心洞察】在 LLM 中，只有 Attention 的 Q 和 O 投影是方阵！
    #
    # 原因：
    # - Q 投影: 输入 hidden_size → 输出 num_heads × head_dim
    #          = 512 → 8 × 64 = 512  （方阵！）
    #
    # - O 投影: 输入 num_heads × head_dim → 输出 hidden_size
    #          = 512 → 512  （方阵！）
    #
    # - K/V 投影: 因为 GQA (分组查询注意力) 让 KV 头数 < Q 头数
    #          = 512 → 2 × 64 = 128  （不是方阵）
    #
    # - FFN 层: 中间维度是 hidden_size 的 2.67 倍
    #          = 512 → 1376 或 1376 → 512  （不是方阵）
    #
    # 【这个判断的效果】
    # - 自动选中: q_proj, o_proj（每层 2 个）
    # - 自动跳过: k_proj, v_proj, gate_proj, up_proj, down_proj, embed, lm_head
    #
    # 【为什么只对 Q 和 O 加 LoRA？】
    # 1. 参数效率: 这两层对模型行为影响最大
    # 2. 研究表明: 微调 Q 和 O 就能达到很好的效果
    # 3. 内存节省: 只用 2 层而不是 7 层
    #
    # 【注意】这是一种简化策略！
    # - 原版 LoRA 论文建议对 Q/K/V/O 都加
    # - 有些实现还会对 FFN 加 LoRA
    # - 这里用方阵判断只是一种简便的实现方式
    #
    # ================================================================
    
    for name, module in model.named_modules():
        # 只为方阵线性层添加 LoRA
        # 在本模型中，方阵 = Attention 的 q_proj 和 o_proj
        if isinstance(module, nn.Linear) and module.weight.shape[0] == module.weight.shape[1]:
            
            # ============================================================
            # .to(model.device) 是什么？为什么需要它？
            # ============================================================
            #
            # 【问题背景】
            # 神经网络的张量必须在同一个"设备"上才能计算:
            # - CPU: 普通内存，计算慢
            # - GPU (cuda:0, cuda:1...): 显卡显存，计算快
            #
            # 如果 model 在 GPU 上，而 LoRA 在 CPU 上:
            #   model(x) + lora(x)  →  报错！不同设备的张量不能相加
            #
            # 【.to(device) 做了什么？】
            # 把模块的所有参数（weight, bias 等）移动到指定设备:
            #   lora.to("cuda:0")  →  lora.A.weight 和 lora.B.weight 都移到 GPU
            #
            # 【model.device 是什么？】
            # 获取 model 所在的设备，通常通过第一个参数的设备来判断:
            #   model.device = next(model.parameters()).device
            #
            # 这样 LoRA 就和原模型在同一个设备上了，可以正常计算！
            #
            # ============================================================
            
            # 创建 LoRA 适配器，并移动到和模型相同的设备
            lora = LoRA(module.weight.shape[0], module.weight.shape[1], rank=rank).to(model.device)
            
            # ============================================================
            # setattr 是什么？
            # ============================================================
            #
            # setattr(obj, "name", value) 等价于 obj.name = value
            #
            # 这里的作用:
            #   module.lora = lora
            #
            # 把 LoRA 适配器"挂"到原始层上，方便后续访问和保存
            #
            # ============================================================
            
            setattr(module, "lora", lora)
            
            # ============================================================
            # 如何"劫持" forward 函数？这是 Python 闭包的魔法！
            # ============================================================
            #
            # 【第一步】保存原始 forward 函数的引用
            #
            # original_forward 现在指向 module 原来的 forward 方法
            # 即使后面 module.forward 被覆盖，original_forward 仍然有效！
            #
            # 这就像:
            #   原来: module.forward → 函数A
            #   保存: original_forward → 函数A  (直接指向函数A)
            #   覆盖: module.forward → 函数B
            #   调用 original_forward() 仍然执行函数A！
            #
            # ============================================================
            
            original_forward = module.forward

            # ============================================================
            # 闭包 (Closure) 和默认参数的技巧
            # ============================================================
            #
            # 【问题】为什么要写 layer1=original_forward, layer2=lora？
            #
            # 如果直接写:
            #   def forward_with_lora(x):
            #       return original_forward(x) + lora(x)  # ❌ 有 bug!
            #
            # 这会出问题！因为 Python 闭包的"延迟绑定"特性:
            # - 函数定义时不会立即读取 original_forward 和 lora 的值
            # - 而是在函数调用时才去查找这两个变量
            # - 循环结束后，所有函数都会指向最后一次循环的值！
            #
            # 【解决方案】用默认参数"立即绑定"
            #   def forward_with_lora(x, layer1=original_forward, layer2=lora):
            #
            # 默认参数在函数定义时就会计算并保存:
            # - layer1 立即绑定到当前的 original_forward
            # - layer2 立即绑定到当前的 lora
            # - 每次循环创建的函数都有自己独立的 layer1 和 layer2！
            #
            # 【layer1(x) 为什么能调用原来的 forward？】
            #
            # layer1 就是 original_forward，它是一个可调用对象:
            #   layer1(x)
            #   = original_forward(x)
            #   = module 原来的 forward 方法(x)
            #   = nn.Linear 的标准前向传播
            #   = x @ weight.T + bias
            #
            # Python 中，函数/方法都是"一等公民"，可以:
            # - 赋值给变量
            # - 作为参数传递
            # - 直接调用
            #
            # ============================================================
            
            # 修改 forward: 原始输出 + LoRA 输出
            def forward_with_lora(x, layer1=original_forward, layer2=lora):
                # layer1(x) = 原始 nn.Linear 的输出 = W @ x
                # layer2(x) = LoRA 的输出 = B @ A @ x
                # 最终输出 = W @ x + B @ A @ x
                return layer1(x) + layer2(x)

            # ============================================================
            # 覆盖原始 forward
            # ============================================================
            #
            # 现在 module.forward 指向新函数 forward_with_lora
            # 当其他代码调用 module(x) 时:
            #   1. Python 自动调用 module.forward(x)
            #   2. 执行 forward_with_lora(x)
            #   3. 返回 原始输出 + LoRA 输出
            #
            # 这就是"猴子补丁"(Monkey Patching)技术！
            # 在运行时动态修改对象的行为，无需改动原始代码。
            #
            # ============================================================
            
            module.forward = forward_with_lora


def load_lora(model, path):
    """
    加载 LoRA 权重
    
    【流程】
    1. 从文件加载状态字典
    2. 处理 DDP 前缀 (module.)
    3. 找到每个 LoRA 模块对应的权重
    4. 加载到模型中
    """
    state_dict = torch.load(path, map_location=model.device)
    # 移除可能的 DDP 前缀
    state_dict = {(k[7:] if k.startswith('module.') else k): v for k, v in state_dict.items()}

    for name, module in model.named_modules():
        if hasattr(module, 'lora'):
            # 提取该层的 LoRA 权重
            lora_state = {k.replace(f'{name}.lora.', ''): v for k, v in state_dict.items() if f'{name}.lora.' in k}
            module.lora.load_state_dict(lora_state)


def save_lora(model, path):
    """
    保存 LoRA 权重
    
    【注意】
    只保存 LoRA 参数，不保存原模型参数
    - 文件非常小 (通常几 MB)
    - 需要配合原模型使用
    """
    state_dict = {}
    for name, module in model.named_modules():
        if hasattr(module, 'lora'):
            # 处理 DDP 前缀
            clean_name = name[7:] if name.startswith("module.") else name
            # 收集该层的 LoRA 权重
            lora_state = {f'{clean_name}.lora.{k}': v for k, v in module.lora.state_dict().items()}
            state_dict.update(lora_state)
    torch.save(state_dict, path)
