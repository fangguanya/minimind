"""
================================================================================
                    LoRA (Low-Rank Adaptation) 实现
================================================================================

【什么是 LoRA】
LoRA 是一种高效的微调方法，核心思想是:
- 冻结预训练模型的权重
- 在需要微调的层旁边添加低秩分解的小矩阵
- 只训练这些小矩阵，大大减少可训练参数

【数学原理】
原始权重矩阵 W ∈ R^(d×k)
LoRA 添加: ΔW = B @ A，其中 A ∈ R^(r×k), B ∈ R^(d×r), r << min(d,k)

前向传播: y = W @ x + ΔW @ x = W @ x + B @ A @ x

【@ 是什么？矩阵乘法！】
Python 3.5+ 引入: A @ B 等价于 torch.matmul(A, B)

矩阵乘法规则: 结果[i,j] = A的第i行 · B的第j列（点积）

例: A=[2,3矩阵], B=[3,4矩阵] → A@B=[2,4矩阵]

【LoRA 中的计算流程】（假设 dim=512, rank=8）
1. A @ x: [8,512] @ [512] = [8]      压缩到8维
2. B @ (A@x): [512,8] @ [8] = [512]  投影回512维
3. W @ x: [512,512] @ [512] = [512]  原始结果
4. 相加得到最终输出 [512]

低秩的妙处: 用 8192 参数近似 262144 参数的效果！

【A/B 是怎么被训练的？】

关键: requires_grad 控制谁被训练！

原始权重 W: requires_grad = False  ← 冻结，不训练
LoRA 的 A: requires_grad = True   ← 可训练
LoRA 的 B: requires_grad = True   ← 可训练

前向: y = W @ x + B @ A @ x
反向: loss.backward() 只算 A 和 B 的梯度（因为 W 冻结了）
更新: optimizer.step() 只更新 A 和 B

A 和 B 被"同时"训练：链式法则自动算两个的梯度！

【参数如何减少？】

全量微调 W [512×512]: 262,144 参数
LoRA A[8,512] + B[512,8]: 8,192 参数  ← 减少32倍！

为什么能用这么少？
- 微调的权重变化 ΔW 通常是"低秩"的
- 预训练模型已经很好，微调改动很小
- 这些改动集中在几个"方向"上，rank=8 就够了

【为什么有效】
1. 预训练模型的权重已经很好，微调只需要小的调整
2. 这些调整通常是低秩的 (rank << d)
3. 只训练 r×(d+k) 个参数，而不是 d×k 个

【典型设置】
- rank=8 或 rank=16
- 只在 attention 的 Q/K/V/O 投影层使用
- 可以减少 90%+ 的可训练参数

【优势】
1. 显存占用少：只需存储小矩阵的梯度
2. 训练快：参数少，优化器状态小
3. 可插拔：可以轻松切换不同的 LoRA 适配器
4. 保持原模型：原模型权重不变，不会灾难性遗忘
"""

import torch
from torch import optim, nn


class LoRA(nn.Module):
    """
    LoRA 低秩适配器
    
    【结构】
    原始层: y = W @ x
    添加 LoRA 后: y = W @ x + B @ A @ x
    
    其中:
    - A: [in_features, rank] 下投影矩阵
    - B: [rank, out_features] 上投影矩阵
    - rank << in_features, out_features
    
    【初始化】
    - A: 高斯初始化 (std=0.02)
    - B: 零初始化
    - 这样初始时 ΔW = B @ A = 0，LoRA 不改变原模型行为
    """
    def __init__(self, in_features, out_features, rank):
        super().__init__()
        self.rank = rank  # LoRA 的秩，控制适配器容量
        
        # ================================================================
        # nn.Linear 是什么？（PyTorch 最基础的层之一）
        # ================================================================
        #
        # 【本质】一个带权重矩阵的线性变换，就是 y = x @ W^T + b
        #
        # 【数据结构】nn.Linear 包含两个成员：
        #   - weight: 权重矩阵，形状 [out_features, in_features]
        #   - bias:   偏置向量，形状 [out_features]（如果 bias=False 则为 None）
        #
        # 【计算过程】
        #   输入 x:  形状 [batch_size, in_features]
        #   输出 y:  形状 [batch_size, out_features]
        #   计算:    y = x @ weight.T + bias
        #
        # 【举例】nn.Linear(512, 8, bias=False)
        #   - weight 形状: [8, 512]
        #   - 输入 x: [batch, 512]
        #   - 计算: x @ weight.T = [batch, 512] @ [512, 8] = [batch, 8]
        #   - 效果: 把 512 维向量压缩成 8 维
        #
        # 【特点】
        #   1. 可训练: weight 和 bias 自动设置 requires_grad=True
        #   2. 自动注册: 会被 model.parameters() 收集到
        #   3. 纯线性: 没有激活函数，需要的话要额外加
        #
        # ================================================================
        
        # ================================================================
        # 低秩分解详解：为什么叫"低秩"？为什么 A 是下投影、B 是上投影？
        # ================================================================
        #
        # 【什么是"秩 (Rank)"？】
        #
        # 秩是线性代数中矩阵的属性，表示矩阵中线性无关的行/列的数量。
        # 简单理解：秩 = 矩阵包含的"有效信息维度"
        #
        # 【为什么叫"低秩分解"？】
        #
        # 原始权重矩阵 W 形状 [512, 512]，理论最大秩是 512。
        # 但 LoRA 用两个小矩阵来近似权重变化：
        #
        #   ΔW = B @ A
        #   其中: A ∈ [rank, 512], B ∈ [512, rank], rank=8
        #
        # 关键数学性质：矩阵乘法的秩
        #   rank(B @ A) ≤ min(rank(B), rank(A)) ≤ 8
        #
        # 所以 B @ A 的结果虽然是 [512, 512] 的矩阵，
        # 但它的秩最多只有 8！远小于 512，所以叫"低秩"。
        #
        # 【为什么低秩能工作？】
        #
        # 研究发现：微调时的权重变化 ΔW 通常是低秩的！
        # - 预训练模型已经很好，微调只是小调整
        # - 这些调整集中在几个"主要方向"上
        # - 用 rank=8 就能捕捉大部分有用的变化
        #
        # 【为什么 A 叫"下投影"，B 叫"上投影"？】
        #
        # 看数据流动时的维度变化：
        #
        #   输入 x:    [batch, 512]
        #       ↓ A:   512 → 8     ← 维度"下降"（下投影/压缩）
        #   中间:      [batch, 8]   ← 瓶颈层，信息被压缩
        #       ↓ B:   8 → 512     ← 维度"上升"（上投影/恢复）
        #   输出:      [batch, 512]
        #
        # 这和 AutoEncoder 的 encoder/decoder 概念类似：
        # - A = encoder，把高维压缩到低维
        # - B = decoder，把低维展开回高维
        #
        # ================================================================
        
        # 低秩分解: ΔW = B @ A
        # A: 下投影 in_features → rank（压缩维度）
        self.A = nn.Linear(in_features, rank, bias=False)
        # B: 上投影 rank → out_features（恢复维度）
        self.B = nn.Linear(rank, out_features, bias=False)
        
        # ================================================================
        # 初始化策略详解（高中数学就能懂！）
        # ================================================================
        #
        # 【目标】训练刚开始时，LoRA 的输出必须是 0
        #
        # 为什么？因为 LoRA 的计算是：
        #   最终输出 = 原模型输出 + LoRA输出
        #            = W @ x      + B @ A @ x
        #
        # 如果一开始 LoRA输出 ≠ 0，就会"污染"原模型，
        # 导致预训练学到的知识被破坏，模型可能直接崩掉！
        #
        # 【如何让 LoRA 输出为 0？】
        #
        # LoRA输出 = B @ A @ x
        #
        # 想让结果为 0，最简单的办法：让 B 或 A 其中一个是零矩阵！
        # 因为：任何数 × 0 = 0
        #
        # 【为什么选择 B=0，而不是 A=0？】
        #
        # 其实两种都可以！但必须有一个非零，原因如下：
        #
        # 神经网络通过"梯度"来学习，梯度就像"指路牌"，
        # 告诉参数应该往哪个方向调整。
        #
        # 如果 A 和 B 都是 0：
        #   - 计算 A 的梯度时，需要用到 B 的值
        #   - 计算 B 的梯度时，需要用到 A 的值
        #   - 两个都是 0，梯度也都是 0
        #   - 梯度为 0 = 没有"指路牌" = 参数不知道往哪调 = 永远学不动！
        #
        # 这就像两个人互相等对方先动，结果谁都不动，卡死了。
        #
        # 【解决方案】
        #
        # 让其中一个非零（比如 A），另一个为零（比如 B）：
        #   - 初始输出：B @ A @ x = 0 @ A @ x = 0  ✓ 满足目标
        #   - B 的梯度 ∝ A @ x，因为 A ≠ 0，所以 B 有梯度，能学习
        #   - 第一步更新后 B ≠ 0 了，A 也能开始学习
        #   - 两个矩阵都能正常训练！
        #
        # 【为什么 A 用高斯分布，std=0.02？】
        #
        # 高斯分布（正态分布）就是那个钟形曲线，大部分值在 0 附近。
        # std=0.02 表示标准差很小，值大约在 -0.06 到 +0.06 之间。
        #
        # 为什么要这么小？
        #   - 太大：数值不稳定，训练可能"爆炸"
        #   - 太小：学习起步太慢
        #   - 0.02：经过大量实验验证的"甜点"值
        #
        # ================================================================
        
        # A: 高斯初始化，让梯度能够流动，LoRA 能学到东西
        #
        # 【高斯初始化后的矩阵长什么样？】
        #
        # 假设 in_features=512, rank=8，则 A.weight 形状是 [8, 512]
        # mean=0.0, std=0.02 的高斯分布，大部分值在 -0.06 ~ +0.06 之间
        #
        # 实际例子（8行 × 512列，这里只展示前5列）：
        # ┌─────────────────────────────────────────────────────────┐
        # │  A.weight = tensor([                                    │
        # │    [ 0.0134, -0.0087,  0.0251, -0.0012,  0.0198, ...],  │ ← 第0行
        # │    [-0.0156,  0.0089, -0.0234,  0.0167, -0.0045, ...],  │ ← 第1行
        # │    [ 0.0078, -0.0201,  0.0123, -0.0089,  0.0234, ...],  │ ← 第2行
        # │    [-0.0112,  0.0156, -0.0078,  0.0201, -0.0134, ...],  │ ← 第3行
        # │    [ 0.0189, -0.0145,  0.0067, -0.0223,  0.0112, ...],  │ ← 第4行
        # │    [-0.0034,  0.0178, -0.0189,  0.0056, -0.0167, ...],  │ ← 第5行
        # │    [ 0.0223, -0.0056,  0.0145, -0.0178,  0.0089, ...],  │ ← 第6行
        # │    [-0.0067,  0.0112, -0.0034,  0.0145, -0.0201, ...],  │ ← 第7行
        # │  ])                                                     │
        # │                                                         │
        # │  形状: [8, 512] = [rank, in_features]                   │
        # │  总参数量: 8 × 512 = 4096 个小数                        │
        # └─────────────────────────────────────────────────────────┘
        #
        # 【特点】
        # - 每个值都是独立随机生成的
        # - 大部分值很小，在 ±0.06 范围内（3σ 原则）
        # - 正负值大致各占一半
        # - 没有明显的模式或结构（纯随机）
        #
        # 【为什么要随机？】
        # - 打破对称性：如果所有值都一样，所有神经元学到的东西也一样
        # - 提供多样性：不同的初始值让不同参数探索不同的方向
        #
        # 【为什么不用"求和=1"的概率分布初始化？】
        #
        # 问：既然是 2D 矩阵，为什么不用钟形采样投影，让每行求和=1？
        #
        # 答：权重矩阵和概率分布的用途不同！
        #
        # ┌────────────────────────────────────────────────────────────┐
        # │  场景              │ 需要求和=1？ │ 原因                   │
        # ├────────────────────┼──────────────┼────────────────────────┤
        # │  Softmax 输出      │ ✅ 需要      │ 表示概率分布，必须归一 │
        # │  Attention 权重    │ ✅ 需要      │ 表示"关注比例"        │
        # │  线性层权重 W      │ ❌ 不需要    │ 是变换矩阵，不是概率   │
        # └────────────────────────────────────────────────────────────┘
        #
        # 【线性层的数学本质】
        #
        #   y = x @ W   # W 是变换矩阵，把 x 投影到新空间
        #
        # W 的作用是旋转、缩放、投影，不是加权平均。
        # 如果强制 W 的每行求和=1：
        #   - 会严重限制变换能力（只能做特定类型的变换）
        #   - 无法实现缩放（放大/缩小向量）
        #   - 表达能力大大降低
        #
        # 【更高级的初始化方法】
        #
        # 确实存在更精心设计的初始化：
        #   - Xavier:    std = √(2/(fan_in+fan_out))  保持方差稳定
        #   - Kaiming:   std = √(2/fan_in)            适合 ReLU
        #   - Orthogonal: 正交矩阵                    保持向量长度
        #
        # 【但 LoRA 用简单高斯的原因】
        #   1. B 初始化为 0，初始输出必须是 0
        #   2. A 只需要"非零"就行，具体值会被训练覆盖
        #   3. std=0.02 足够小，不会破坏数值稳定性
        #   4. 简单有效，过度设计初始化意义不大
        #        
        self.A.weight.data.normal_(mean=0.0, std=0.02)
        # B: 零初始化，确保初始时 LoRA 输出 = B @ A @ x = 0
        self.B.weight.data.zero_()
        
        # ================================================================
        # A 和 B 一起运算，梯度下降时如何区分它们？
        # ================================================================
        #
        # 【核心答案】PyTorch 的自动微分 (Autograd) 会自动处理！
        #
        # 【计算图 (Computational Graph)】
        #
        # 前向传播时，PyTorch 自动记录计算路径：
        #
        #   x → [A.weight] → z(中间结果) → [B.weight] → y → loss
        #
        # 每个 nn.Linear 的 weight 是独立的 nn.Parameter，
        # 它们在内存中有不同的地址，PyTorch 能区分它们。
        #
        # 【反向传播：链式法则自动分配梯度】
        #
        # 调用 loss.backward() 时，PyTorch 沿计算图反向走，
        # 用链式法则给每个参数计算各自的梯度：
        #
        #   ∂loss/∂B = ∂loss/∂y × ∂y/∂B = ∂loss/∂y × z^T
        #   ∂loss/∂A = ∂loss/∂y × ∂y/∂z × ∂z/∂A = (B^T × ∂loss/∂y) × x^T
        #
        # ════════════════════════════════════════════════════════════
        # 【详解这两个公式】先回顾前向传播
        # ════════════════════════════════════════════════════════════
        #
        # 前向传播（假设 batch=1 简化讨论）：
        #
        #   x: [1, 512]    输入
        #   ↓ A
        #   z = x @ A^T    z: [1, 8]     中间结果（A的输出）
        #   ↓ B
        #   y = z @ B^T    y: [1, 512]   最终输出
        #   ↓
        #   loss = f(y)    标量         损失函数
        #
        # ════════════════════════════════════════════════════════════
        # 【公式1】∂loss/∂B 的推导
        # ════════════════════════════════════════════════════════════
        #
        # 目标：求 loss 对 B 的梯度
        #
        # 链式法则：
        #   ∂loss/∂B = ∂loss/∂y × ∂y/∂B
        #              ↑           ↑
        #              |           └── y 对 B 怎么变化？
        #              └── loss 对 y 怎么变化？(上游梯度)
        #
        # 【第一项】∂loss/∂y
        #   - 这是从 loss 反向传回来的"上游梯度"
        #   - 形状和 y 一样: [1, 512]
        #   - PyTorch 自动计算好传给我们
        #
        # 【第二项】∂y/∂B
        #   - y = z @ B^T，对 B 求导
        #   - 矩阵求导规则：∂(z @ B^T)/∂B = z^T
        #   - 直觉：y 的每个元素是 z 和 B 某行的点积
        #
        # 【合并】
        #   ∂loss/∂B = ∂loss/∂y × z^T
        #            = [1, 512]^T × [1, 8]^T（外积形式）
        #            = [512, 1] × [1, 8]
        #            = [512, 8]  ← B 的形状！
        #
        # 【具体数值例子】
        #   假设 ∂loss/∂y = [0.1, -0.2, 0.3, ...]  (512个数)
        #   假设 z = [0.5, -0.3, 0.2, ...]         (8个数)
        #
        #   ∂loss/∂B[i,j] = ∂loss/∂y[i] × z[j]
        #
        #   例如: ∂loss/∂B[0,0] = 0.1 × 0.5 = 0.05
        #         ∂loss/∂B[0,1] = 0.1 × (-0.3) = -0.03
        #         ∂loss/∂B[1,0] = (-0.2) × 0.5 = -0.10
        #         ...
        #
        # ════════════════════════════════════════════════════════════
        # 【公式2】∂loss/∂A 的推导
        # ════════════════════════════════════════════════════════════
        #
        # 目标：求 loss 对 A 的梯度
        #
        # 链式法则（要穿过 B！）：
        #   ∂loss/∂A = ∂loss/∂y × ∂y/∂z × ∂z/∂A
        #              ↑           ↑        ↑
        #              |           |        └── z 对 A 怎么变化？
        #              |           └── y 对 z 怎么变化？
        #              └── loss 对 y 怎么变化？
        #
        # 【第一项】∂loss/∂y: [1, 512]，同上
        #
        # 【第二项】∂y/∂z
        #   - y = z @ B^T，对 z 求导
        #   - ∂y/∂z = B^T 的转置 = B
        #   - 但在链式法则中: ∂loss/∂z = ∂loss/∂y @ B
        #   - 形状: [1, 512] @ [512, 8] = [1, 8]
        #
        # 【第三项】∂z/∂A
        #   - z = x @ A^T，对 A 求导
        #   - 类似公式1: ∂z/∂A = x^T
        #
        # 【合并】
        #   先算 ∂loss/∂z = ∂loss/∂y @ B = [1, 512] @ [512, 8] = [1, 8]
        #   再算 ∂loss/∂A = ∂loss/∂z^T × x^T
        #                 = [8, 1] × [1, 512]
        #                 = [8, 512]  ← A 的形状！
        #
        # 【关键洞察】
        #   计算 A 的梯度时，需要用到 B 的值！
        #   ∂loss/∂A = (∂loss/∂y @ B)^T @ x
        #                         ↑
        #                    B 在这里！
        #
        #   如果 B = 0，那么 ∂loss/∂A = 0，A 学不动！
        #   如果 A = 0，那么 z = 0，∂loss/∂B = ∂loss/∂y × 0^T = 0，B 也学不动！
        #
        #   这就是为什么 A 和 B 不能都初始化为 0！
        #
        # ════════════════════════════════════════════════════════════
        # 【总结：梯度流动图】
        # ════════════════════════════════════════════════════════════
        #
        #   前向: x ──→ A ──→ z ──→ B ──→ y ──→ loss
        #
        #   反向: x ←── A ←── z ←── B ←── y ←── loss
        #              ↑        ↑        ↑
        #           ∂loss/∂A  需要B   ∂loss/∂B
        #           用到B的值  传梯度  用到z的值
        #
        # ════════════════════════════════════════════════════════════
        #
        # 【梯度存储位置】
        #
        # 梯度会存储在各自参数的 .grad 属性中：
        #   self.A.weight.grad  # A 的梯度，形状 [rank, in_features]
        #   self.B.weight.grad  # B 的梯度，形状 [out_features, rank]
        #
        # optimizer.step() 会分别用各自的梯度更新 A 和 B。
        #
        # 【类比】
        # 就像两个人合作搬东西，虽然一起用力，
        # 但每个人出了多少力是可以分别测量的。
        # PyTorch 的自动微分就是这个"测量工具"。
        #
        # ================================================================

    def forward(self, x):
        """LoRA 前向传播: x → A → B → output"""
        return self.B(self.A(x))


def apply_lora(model, rank=8):
    """
    为模型的线性层添加 LoRA 适配器
    
    【应用策略】
    只为方阵线性层添加 LoRA:
    - 这通常是 attention 层中的 Q/K/V/O 投影
    - 方阵特征: weight.shape[0] == weight.shape[1]
    
    【实现方式】
    1. 为每个目标层创建 LoRA 模块
    2. 修改层的 forward 方法: output = original(x) + lora(x)
    
    【参数】
    - model: 要添加 LoRA 的模型
    - rank: LoRA 的秩，越大容量越大，但参数也越多
    """
    # ================================================================
    #                    LLM 基础概念大白话解释
    #              （高中生也能看懂的 Transformer 原理）
    # ================================================================
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │                  零、LLM 的完整结构图                        │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 一个完整的 LLM (如 MiniMind) 包含以下层次结构:
    #
    # ┌─────────────────────────────────────────────────────────────────┐
    # │                    MiniMindForCausalLM                          │
    # │  (完整的语言模型，用于生成下一个词)                            │
    # ├─────────────────────────────────────────────────────────────────┤
    # │                                                                 │
    # │  ┌─────────────────────────────────────────────────────────┐   │
    # │  │ 1. embed_tokens (词嵌入层)                              │   │
    # │  │    nn.Embedding(vocab_size=6400, hidden_size=512)       │   │
    # │  │    作用: 把 token ID 变成 512 维向量                    │   │
    # │  │    参数量: 6400 × 512 = 3.3M                            │   │
    # │  └─────────────────────────────────────────────────────────┘   │
    # │                           ↓                                     │
    # │  ┌─────────────────────────────────────────────────────────┐   │
    # │  │ 2. Transformer Blocks × N层 (N = num_hidden_layers)    │   │
    # │  │                                                         │   │
    # │  │    【num_hidden_layers 是什么？】                       │   │
    # │  │                                                         │   │
    # │  │    它指的是 Transformer Block 堆叠的层数！              │   │
    # │  │    - num_hidden_layers = 8  → 8 个 Block 堆叠           │   │
    # │  │    - num_hidden_layers = 16 → 16 个 Block 堆叠          │   │
    # │  │                                                         │   │
    # │  │    【为什么叫"隐藏层"？】                                │   │
    # │  │                                                         │   │
    # │  │    神经网络的结构:                                      │   │
    # │  │    输入层 → 隐藏层1 → 隐藏层2 → ... → 输出层           │   │
    # │  │                                                         │   │
    # │  │    - 输入层: embed_tokens (能看到)                      │   │
    # │  │    - 隐藏层: Transformer Blocks (看不到，在中间处理)   │   │
    # │  │    - 输出层: lm_head (能看到)                           │   │
    # │  │                                                         │   │
    # │  │    "隐藏"是因为这些层的输出不直接可见，                │   │
    # │  │    只在网络内部流动。                                   │   │
    # │  │                                                         │   │
    # │  │    【层数越多 = 模型越"深"】                            │   │
    # │  │                                                         │   │
    # │  │    - 8层: 小模型，参数少，速度快                        │   │
    # │  │    - 32层: 中等模型 (如 LLaMA-7B)                       │   │
    # │  │    - 80层: 大模型 (如 LLaMA-65B)                        │   │
    # │  │                                                         │   │
    # │  │    层数多 → 能学到更复杂的模式 → 但也更慢更贵          │   │
    # │  │                                                         │   │
    # │  └─────────────────────────────────────────────────────────┘   │
    # │  ┌─────────────────────────────────────────────────────────┐   │
    # │  │    每个 Block 包含:                                     │   │
    # │  │    ┌─────────────────────────────────────────────────┐  │   │
    # │  │    │ 2.1 input_layernorm (RMSNorm)                   │  │   │
    # │  │    │     作用: 归一化，稳定训练                       │  │   │
    # │  │    │     参数量: 512                                  │  │   │
    # │  │    └─────────────────────────────────────────────────┘  │   │
    # │  │                         ↓                                │   │
    # │  │    ┌─────────────────────────────────────────────────┐  │   │
    # │  │    │ 2.2 self_attn (自注意力层)                      │  │   │
    # │  │    │     ├─ q_proj: Linear(512→512)  生成Query       │  │   │
    # │  │    │     ├─ k_proj: Linear(512→128)  生成Key (GQA)   │  │   │
    # │  │    │     ├─ v_proj: Linear(512→128)  生成Value (GQA) │  │   │
    # │  │    │     └─ o_proj: Linear(512→512)  输出投影        │  │   │
    # │  │    │     参数量: 512×512 + 512×128×2 + 512×512       │  │   │
    # │  │    │            = 262K + 131K + 262K = 655K          │  │   │
    # │  │    └─────────────────────────────────────────────────┘  │   │
    # │  │                         ↓ (+残差连接)                    │   │
    # │  │    ┌─────────────────────────────────────────────────┐  │   │
    # │  │    │ 2.3 post_attention_layernorm (RMSNorm)          │  │   │
    # │  │    │     参数量: 512                                  │  │   │
    # │  │    └─────────────────────────────────────────────────┘  │   │
    # │  │                         ↓                                │   │
    # │  │    ┌─────────────────────────────────────────────────┐  │   │
    # │  │    │ 2.4 mlp (前馈网络 FFN，或 MoE)                  │  │   │
    # │  │    │     ├─ gate_proj: Linear(512→1376)  门控        │  │   │
    # │  │    │     ├─ up_proj:   Linear(512→1376)  上投影      │  │   │
    # │  │    │     └─ down_proj: Linear(1376→512)  下投影      │  │   │
    # │  │    │     参数量: 512×1376×2 + 1376×512 = 2.1M       │  │   │
    # │  │    └─────────────────────────────────────────────────┘  │   │
    # │  │                         ↓ (+残差连接)                    │   │
    # │  │    每层参数量 ≈ 655K + 2.1M + 1K = 2.8M                 │   │
    # │  │    8层总计 ≈ 22M                                         │   │
    # │  └─────────────────────────────────────────────────────────┘   │
    # │                           ↓                                     │
    # │  ┌─────────────────────────────────────────────────────────┐   │
    # │  │ 3. norm (最终 RMSNorm)                                  │   │
    # │  │    作用: 输出前的最后归一化                              │   │
    # │  │    参数量: 512                                           │   │
    # │  └─────────────────────────────────────────────────────────┘   │
    # │                           ↓                                     │
    # │  ┌─────────────────────────────────────────────────────────┐   │
    # │  │ 4. lm_head (语言模型头)                                 │   │
    # │  │    nn.Linear(hidden_size=512, vocab_size=6400)          │   │
    # │  │    作用: 预测下一个 token 的概率分布                    │   │
    # │  │    参数量: 512 × 6400 = 3.3M                            │   │
    # │  └─────────────────────────────────────────────────────────┘   │
    # │                           ↓                                     │
    # │                    输出: [batch, seq, 6400]                     │
    # │                    每个位置预测 6400 个词的概率                │
    # │                                                                 │
    # ├─────────────────────────────────────────────────────────────────┤
    # │  【MiniMind 总参数量估算】                                     │
    # │                                                                 │
    # │  embed_tokens:           3.3M                                  │
    # │  8层 Transformer Block: 22.0M                                  │
    # │  final norm:             0.0005M                               │
    # │  lm_head:                3.3M (常与 embed_tokens 共享权重)     │
    # │  ─────────────────────────────                                 │
    # │  总计约: 26M ~ 30M 参数                                        │
    # │                                                                 │
    # └─────────────────────────────────────────────────────────────────┘
    #
    # 【数据流动过程】
    #
    # 输入: "今天天气"
    #   ↓
    # Token IDs: [156, 892, 892, 231]
    #   ↓
    # embed_tokens: 每个 ID → 512维向量 → [4, 512]
    #   ↓
    # Transformer Block 1: 自注意力 + FFN → [4, 512]
    #   ↓
    # Transformer Block 2: 自注意力 + FFN → [4, 512]
    #   ↓
    # ... (重复 N 层)
    #   ↓
    # final norm: 归一化 → [4, 512]
    #   ↓
    # lm_head: 512维 → 6400维 → [4, 6400]
    #   ↓
    # softmax: 变成概率 → 第4个位置概率最高的词是"很"
    #   ↓
    # 输出: "今天天气很"
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │        零.5、RMSNorm 详解：每个字符都是什么意思？           │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【名字拆解】RMSNorm = Root Mean Square Normalization
    #
    #   R   = Root    = 根号 √
    #   M   = Mean    = 平均值
    #   S   = Square  = 平方
    #   Norm = Normalization = 归一化
    #
    # 合起来：均方根归一化
    #
    # 【为什么需要归一化？】
    #
    # 神经网络的问题：数值会越算越大或越算越小！
    #
    # 例如：经过多层计算后
    #   - 某些值变成 1000000（太大，梯度爆炸）
    #   - 某些值变成 0.000001（太小，梯度消失）
    #
    # 归一化的作用：把数值"拉回"到合理范围，稳定训练
    #
    # 【RMSNorm 的数学公式】
    #
    # 输入: x = [x₁, x₂, x₃, ..., xₙ]  （n=512 维向量）
    #
    # 步骤1: 计算均方根 (RMS)
    #   ┌────────────────────────────────────────────┐
    #   │                    _______________________  │
    #   │                   /  x₁² + x₂² + ... + xₙ² │
    #   │   RMS(x) =      / ─────────────────────── │
    #   │               \/           n               │
    #   │                                            │
    #   │   用代码表示: rms = sqrt(mean(x²))         │
    #   └────────────────────────────────────────────┘
    #
    # 步骤2: 归一化
    #   ┌────────────────────────────────────────────┐
    #   │              xᵢ                            │
    #   │   x̂ᵢ = ──────────                          │
    #   │          RMS(x)                            │
    #   │                                            │
    #   │   每个元素除以 RMS，把向量"缩放"到标准大小 │
    #   └────────────────────────────────────────────┘
    #
    # 步骤3: 可学习的缩放（这就是那 512 个参数！）
    #   ┌────────────────────────────────────────────┐
    #   │   output = γ × x̂                           │
    #   │                                            │
    #   │   γ (gamma) 是可学习参数，形状 [512]       │
    #   │   每个维度有自己的缩放系数                 │
    #   │   初始化为全 1，训练时学习最佳值           │
    #   └────────────────────────────────────────────┘
    #
    # 【具体数值例子】
    #
    # 假设 x = [3, 4, 0, 0, ..., 0]（简化为4维）
    #
    # 步骤1: RMS = √((3² + 4² + 0² + 0²) / 4)
    #            = √((9 + 16) / 4)
    #            = √(6.25)
    #            = 2.5
    #
    # 步骤2: x̂ = [3/2.5, 4/2.5, 0/2.5, 0/2.5]
    #          = [1.2, 1.6, 0, 0]
    #
    # 步骤3: 假设 γ = [1.0, 1.0, 1.0, 1.0]
    #        output = [1.2, 1.6, 0, 0]
    #
    # 【为什么参数量是 512？】
    #
    # 因为 γ 向量的长度 = hidden_size = 512
    # 每个维度一个可学习的缩放系数
    #
    # 【RMSNorm vs LayerNorm】
    #
    # LayerNorm: 先减均值，再除标准差，有 γ 和 β 两组参数
    # RMSNorm:   只除 RMS，只有 γ 一组参数
    #
    # RMSNorm 更简单，计算更快，效果差不多，所以 LLaMA 系列都用它
    #
    # ════════════════════════════════════════════════════════════
    # 【LayerNorm 详解 - 什么是 LayerNorm？】
    # ════════════════════════════════════════════════════════════
    #
    # LayerNorm = Layer Normalization (层归一化)
    #
    # 【LayerNorm 的数学公式】
    #
    #   LayerNorm(x) = γ * (x - μ) / √(σ² + ε) + β
    #
    #   其中:
    #   - x: 输入向量 [512]
    #   - μ: x 的均值 (mean)
    #   - σ²: x 的方差 (variance)
    #   - ε: 小常数，防止除零 (1e-6)
    #   - γ: 可学习的缩放参数 [512]
    #   - β: 可学习的偏移参数 [512]
    #
    # 【计算步骤】
    #
    #   输入: x = [2.0, 4.0, 6.0, 8.0]  (简化为 4 维)
    #
    #   Step 1: 计算均值
    #           μ = (2 + 4 + 6 + 8) / 4 = 5.0
    #
    #   Step 2: 计算方差
    #           σ² = [(2-5)² + (4-5)² + (6-5)² + (8-5)²] / 4
    #              = [9 + 1 + 1 + 9] / 4 = 5.0
    #
    #   Step 3: 归一化 (减均值，除标准差)
    #           x_norm = (x - μ) / √(σ² + ε)
    #                  = (x - 5.0) / √5.0
    #                  = [-1.34, -0.45, 0.45, 1.34]
    #
    #   Step 4: 仿射变换 (缩放 + 偏移)
    #           output = γ * x_norm + β
    #           (γ 和 β 是可学习的参数)
    #
    # 【为什么要减均值 + 除标准差？】
    #
    #   减均值: 把数据中心移到 0 附近 (去中心化)
    #   除标准差: 把数据缩放到标准范围 (单位方差)
    #
    #   效果: 输出的均值≈0，方差≈1
    #
    # ════════════════════════════════════════════════════════════
    # 【RMSNorm 详解 - 为什么更简单更快？】
    # ════════════════════════════════════════════════════════════
    #
    # 【RMSNorm 的数学公式】
    #
    #   RMSNorm(x) = γ * x / RMS(x)
    #
    #   其中:
    #   - RMS(x) = √(mean(x²)) = √(Σx_i² / n)
    #   - γ: 可学习的缩放参数 [512]
    #   - 注意: 没有 β (偏移参数)！
    #
    # 【计算步骤】
    #
    #   输入: x = [2.0, 4.0, 6.0, 8.0]
    #
    #   Step 1: 计算平方和的均值
    #           mean(x²) = (4 + 16 + 36 + 64) / 4 = 30
    #
    #   Step 2: 开根号得到 RMS
    #           RMS = √30 ≈ 5.48
    #
    #   Step 3: 除以 RMS
    #           x_norm = x / RMS = [0.36, 0.73, 1.09, 1.46]
    #
    #   Step 4: 缩放
    #           output = γ * x_norm
    #
    # 【LayerNorm vs RMSNorm 对比】
    #
    # ┌─────────────┬────────────────────────┬────────────────────────┐
    # │             │ LayerNorm              │ RMSNorm                │
    # ├─────────────┼────────────────────────┼────────────────────────┤
    # │ 公式         │ γ*(x-μ)/σ + β          │ γ*x/RMS               │
    # │ 参数数       │ 2d (γ 和 β)            │ d (只有 γ)             │
    # │ 计算量       │ 需计算均值+方差         │ 只需计算平方和           │
    # │ 去中心化     │ ✔ 有 (减均值)          │ ✘ 无                   │
    # │ 效果         │ 略好                   │ 几乎相同               │
    # │ 速度         │ 较慢                   │ 较快 (~10%)           │
    # │ 使用者       │ BERT, GPT-2           │ LLaMA, MiniMind       │
    # └─────────────┴────────────────────────┴────────────────────────┘
    #
    # 【为什么 RMSNorm 没有去中心化也能工作？】
    #
    # 研究发现: 归一化的主要作用是"控制尺度"，而不是"去中心化"
    # RMSNorm 保留了尺度控制，去掉了去中心化，效果几乎不变
    # 但计算量减少了 (不用算均值)，参数也减少了 (不需要 β)
    #
    # 【PyTorch 代码实现】
    #
    #   class RMSNorm(nn.Module):
    #       def __init__(self, dim):
    #           self.weight = nn.Parameter(torch.ones(dim))  # γ，512个参数
    #
    #       def forward(self, x):
    #           rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True))
    #           x_norm = x / (rms + 1e-6)  # 加小数防止除零
    #           return self.weight * x_norm
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │      零.6、Transformer Block 每个组件详解                   │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 一个 Transformer Block 的完整流程：
    #
    #   输入 x
    #     │
    #     ▼
    #   ┌─────────────────┐
    #   │ input_layernorm │ ← RMSNorm，稳定数值
    #   └────────┬────────┘
    #            │
    #            ▼
    #   ┌─────────────────┐
    #   │   self_attn     │ ← 自注意力，让 token 互相交流
    #   └────────┬────────┘
    #            │
    #     ┌──────┴──────┐
    #     │  + 残差连接  │ ← 把原始 x 加回来
    #     └──────┬──────┘
    #            │
    #            ▼
    #   ┌─────────────────┐
    #   │post_attn_norm   │ ← 又一个 RMSNorm
    #   └────────┬────────┘
    #            │
    #            ▼
    #   ┌─────────────────┐
    #   │      MLP        │ ← 前馈网络，独立处理每个 token
    #   └────────┬────────┘
    #            │
    #     ┌──────┴──────┐
    #     │  + 残差连接  │ ← 再把输入加回来
    #     └──────┬──────┘
    #            │
    #            ▼
    #         输出
    #
    # ════════════════════════════════════════════════════════════════
    # 【2.1 input_layernorm - 输入归一化】
    # ════════════════════════════════════════════════════════════════
    #
    # 作用：在进入注意力之前，先把数值稳定住
    #
    # 代码：x_norm = self.input_layernorm(x)
    #
    # 输入: [batch, seq_len, 512]
    # 输出: [batch, seq_len, 512]（形状不变，只是数值被归一化）
    #
    # ════════════════════════════════════════════════════════════════
    # 【2.2 self_attn - 自注意力层（最核心！）】
    # ════════════════════════════════════════════════════════════════
    #
    # 【什么是"自"注意力？】
    #
    # "自" = self = 句子关注自己
    # 让句子中的每个词都能"看到"其他所有词，决定该关注谁
    #
    # 【Q/K/V 投影层详解】
    #
    # 问：为什么需要 q_proj, k_proj, v_proj 三个投影？
    #
    # 答：同一个词要扮演三种角色！
    #
    #   原始输入 x: "我 爱 中国" → 每个词是 512 维向量
    #
    #   q_proj (Query 投影):
    #     - 把 x 变成 "我想找什么信息"
    #     - Q = x @ W_q，W_q 形状 [512, 512]
    #     - 输出 Q: [seq_len, 512] ← 这是 8 个 Q 头合在一起的总维度！
    #
    #   k_proj (Key 投影):
    #     - 把 x 变成 "我能提供什么信息"
    #     - K = x @ W_k，W_k 形状 [512, 128]（GQA 压缩）
    #     - 输出 K: [seq_len, 128] ← 这是 2 个 KV 头合在一起的总维度！
    #
    #   v_proj (Value 投影):
    #     - 把 x 变成 "我的实际内容"
    #     - V = x @ W_v，W_v 形状 [512, 128]（GQA 压缩）
    #     - 输出 V: [seq_len, 128] ← 这是 2 个 KV 头合在一起的总维度！
    #
    # ════════════════════════════════════════════════════════════════════
    # 【重要！维度的两种说法】
    # ════════════════════════════════════════════════════════════════════
    #
    #   上面说 Q=512, K=128, V=128，下面公式里说 Q=64, K=64, V=64
    #   为什么数字不一样？？
    #
    #   答：两种说法指的是不同的东西！
    #
    #   1. 投影后的"总维度" (所有头合在一起):
    #      - Q 总维度 = 512 = 8个Q头 × 64维/头
    #      - K 总维度 = 128 = 2个KV头 × 64维/头
    #      - V 总维度 = 128 = 2个KV头 × 64维/头
    #
    #   2. 注意力计算时的"每头维度" (拆分成单个头后):
    #      - 每个 Q 头 = 64 维 (head_dim)
    #      - 每个 K 头 = 64 维 (head_dim)
    #      - 每个 V 头 = 64 维 (head_dim)
    #
    #   【关键】注意力公式是在"每个头内部"单独计算的！
    #
    #   完整流程:
    #   ┌─────────────────────────────────────────────────────────────┐
    #   │  投影          拆分多头              注意力计算              │
    #   │  ────          ────────              ────────                │
    #   │  Q [seq,512] → 拆成 8 个 [seq,64] → 每个头单独算 Attention  │
    #   │  K [seq,128] → 拆成 2 个 [seq,64] → (GQA: 复制给4个Q头共享) │
    #   │  V [seq,128] → 拆成 2 个 [seq,64] → (GQA: 复制给4个Q头共享) │
    #   └─────────────────────────────────────────────────────────────┘
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问1: 1个512维向量，要计算几个 Q/K/V 头？】
    # ════════════════════════════════════════════════════════════════════
    #
    #   输入: 1 个 token 的 512 维向量
    #
    #   Q 头数 = 8 个 (每个 token 产生 8 个 Q 头)
    #   K 头数 = 2 个 (每个 token 产生 2 个 K 头)
    #   V 头数 = 2 个 (每个 token 产生 2 个 V 头)
    #
    #   计算量:
    #   - Q: 512维 → W_q[512,512] → 512维 → 拆成8个64维
    #   - K: 512维 → W_k[512,128] → 128维 → 拆成2个64维
    #   - V: 512维 → W_v[512,128] → 128维 → 拆成2个64维
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问2: 哪个 Q 对应哪个 KV？什么逻辑原理？】
    # ════════════════════════════════════════════════════════════════════
    #
    #   GQA (Grouped Query Attention) 的分组逻辑:
    #
    #   8个 Q 头分成 2 组，每组 4 个 Q 头共享 1 个 KV 头:
    #
    #   组号    Q 头           KV 头
    #   ────   ──────────     ──────
    #   组 0   Q0, Q1, Q2, Q3  → K0, V0
    #   组 1   Q4, Q5, Q6, Q7  → K1, V1
    #
    #   【分组公式】
    #   n_rep = num_q_heads / num_kv_heads = 8 / 2 = 4
    #   第 i 个 Q 头使用第 (i // n_rep) 个 KV 头
    #
    #   例子:
    #   - Q0: 0 // 4 = 0 → 用 K0, V0
    #   - Q1: 1 // 4 = 0 → 用 K0, V0
    #   - Q2: 2 // 4 = 0 → 用 K0, V0
    #   - Q3: 3 // 4 = 0 → 用 K0, V0
    #   - Q4: 4 // 4 = 1 → 用 K1, V1
    #   - Q5: 5 // 4 = 1 → 用 K1, V1
    #   - Q6: 6 // 4 = 1 → 用 K1, V1
    #   - Q7: 7 // 4 = 1 → 用 K1, V1
    #
    #   【代码实现】 在 repeat_kv() 函数中:
    #   K: [batch, 2, seq, 64] → repeat → [batch, 8, seq, 64]
    #   即把 2 个 KV 头复制 4 次，变成 8 个
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问3: K/V 为什么是 128 维？也用 512 不行吗？】
    # ════════════════════════════════════════════════════════════════════
    #
    #   128 = num_kv_heads × head_dim = 2 × 64
    #
    #   【如果 K/V 也用 512 维？】
    #
    #   完全可以！那就是标准的 MHA (Multi-Head Attention):
    #   - Q: 512 = 8头 × 64维
    #   - K: 512 = 8头 × 64维  (每个 Q 头有自己的 K)
    #   - V: 512 = 8头 × 64维  (每个 Q 头有自己的 V)
    #
    #   【会怎样？】
    #
    #   MHA (K/V = 512):              GQA (K/V = 128):
    #   - KV Cache: 32 MB             - KV Cache: 8 MB  (省 75%)
    #   - 参数量: 更多                 - 参数量: 更少
    #   - 速度: 更慢                  - 速度: 更快
    #   - 效果: 略好                  - 效果: 几乎相同
    #
    #   【为什么 GQA 效果几乎相同？】
    #   研究发现: 多个 Q 头学到的 "KV模式" 往往很相似
    #   共享 KV 头 = 强制合并相似模式，减少冗余，不损失性能
    #
    # ════════════════════════════════════════════════════════════════════
    # 【哪里配置的？】
    # ════════════════════════════════════════════════════════════════════
    #
    #   在 LMConfig 类中配置:
    #
    #   class LMConfig:
    #       hidden_size = 512           # 隐藏层维度
    #       num_attention_heads = 8     # Q 头数
    #       num_key_value_heads = 2     # KV 头数 (这里决定是 MHA 还是 GQA)
    #
    #   【不同配置的含义】
    #
    #   num_kv_heads = 8  → MHA (每个 Q 头有独立的 KV)
    #   num_kv_heads = 2  → GQA (每 4 个 Q 头共享 1 个 KV)  ← MiniMind 用这个
    #   num_kv_heads = 1  → MQA (所有 Q 头共享同 1 个 KV)
    #
    #   【K/V 维度计算】
    #   kv_dim = num_kv_heads × head_dim
    #         = 2 × 64 = 128
    #
    #   如果想要 K/V = 512，设置 num_kv_heads = 8 即可
    #
    # ════════════════════════════════════════════════════════════════════
    # 【注意力计算公式】(每个头内部的计算)
    # ════════════════════════════════════════════════════════════════════
    #
    #                           Q × K^T
    #   Attention(Q,K,V) = softmax(───────) × V
    #                           sqrt(d_k)
    #
    #   其中 d_k = 64 (每头维度)
    #
    #   分解这个公式 (以单个头为例):
    #
    #   1. Q × K^T: 计算每对词之间的"相关度分数"
    #      - Q: [seq, 64]  ← 单个 Q 头，64 维
    #      - K: [seq, 64]  ← 单个 K 头，64 维
    #      - K^T: [64, seq]
    #      - Q × K^T: [seq, seq]，每个位置表示"词i对词j的关注度"
    #
    #   2. / sqrt(d_k): 除以 sqrt(64) = 8，防止分数太大导致 softmax 饱和
    #
    #   3. softmax: 把分数变成概率（每行加起来=1）
    #      - 高分 → 高概率 → 多关注
    #      - 低分 → 低概率 → 少关注
    #
    #   4. × V: 按概率对 V 加权求和
    #      - V: [seq, 64]  ← 单个 V 头，64 维
    #      - softmax结果: [seq, seq]
    #      - 输出: [seq, 64] ← 还是 64 维，维度不变
    #
    # 【具体例子】"我 爱 中国"
    #
    #   Q × K^T 得到注意力矩阵:
    #
    #              我    爱    中国
    #         ┌─────────────────────┐
    #      我 │  0.1   0.3   0.6   │ ← "我"主要关注"中国"
    #      爱 │  0.2   0.1   0.7   │ ← "爱"也主要关注"中国"
    #    中国 │  0.4   0.4   0.2   │ ← "中国"关注"我"和"爱"
    #         └─────────────────────┘
    #         （每行 softmax 后加起来=1）
    #
    #   最终 "我" 的输出 = 0.1×V_我 + 0.3×V_爱 + 0.6×V_中国
    #
    # 【o_proj - 输出投影】
    #
    # 多头注意力的输出需要合并，再投影回 512 维
    # O = concat(head1, head2, ..., head8) @ W_o
    # W_o 形状: [512, 512]
    #
    # ════════════════════════════════════════════════════════════════
    # 【什么是多头注意力？为什么需要？怎么实现？】
    # ════════════════════════════════════════════════════════════════
    #
    # 【问题】一个注意力只能学一种"关注模式"
    #
    # 但语言很复杂！同一个词需要从多个角度理解：
    #
    #   "小明 昨天 在 北京 吃了 苹果"
    #
    #   对于"吃了"这个词，需要知道：
    #     - 谁吃的？→ 关注"小明"
    #     - 什么时候？→ 关注"昨天"
    #     - 在哪？→ 关注"北京"
    #     - 吃什么？→ 关注"苹果"
    #
    #   一个注意力头只能学一种关系，无法同时关注所有！
    #
    # 【解决方案】多头注意力 = 多个独立的注意力"视角"
    #
    # ┌────────────────────────────────────────────────────────────┐
    # │                                                            │
    # │  num_attention_heads = 8 表示有 8 个注意力头               │
    # │                                                            │
    # │  每个头学习关注不同类型的关系：                            │
    # │    - 头1: 学会关注主谓关系（谁做的？）                     │
    # │    - 头2: 学会关注动宾关系（做什么？）                     │
    # │    - 头3: 学会关注时间词（什么时候？）                     │
    # │    - 头4: 学会关注地点词（在哪里？）                       │
    # │    - ...                                                   │
    # │                                                            │
    # │  这些不是人为设定的，是模型自己学出来的！                  │
    # │                                                            │
    # └────────────────────────────────────────────────────────────┘
    #
    # 【多头的数学实现】
    #
    # 关键公式: head_dim = hidden_size / num_heads = 512 / 8 = 64
    #
    # 不是真的算 8 次独立的注意力，而是：
    #
    #   1. 把 Q/K/V 切成 8 份
    #   2. 每份在自己的 64 维子空间里做注意力
    #   3. 最后把 8 份结果拼起来
    #
    # 【图解多头注意力】
    #
    #   输入 x: [batch, seq, 512]
    #       │
    #       ▼
    #   ┌───────────────────────────────────────────────────────┐
    #   │  Q = x @ W_q  →  [batch, seq, 512]                    │
    #   │  K = x @ W_k  →  [batch, seq, 128] (GQA)              │
    #   │  V = x @ W_v  →  [batch, seq, 128] (GQA)              │
    #   └───────────────────────────────────────────────────────┘
    #       │
    #       ▼ reshape（重塑形状，切分成多头）
    #   ┌───────────────────────────────────────────────────────┐
    #   │  Q: [batch, seq, 512] → [batch, 8, seq, 64]           │
    #   │                          ↑                            │
    #   │                     8个头，每头64维                    │
    #   │                                                       │
    #   │  K: [batch, seq, 128] → [batch, 2, seq, 64] (GQA)     │
    #   │  V: [batch, seq, 128] → [batch, 2, seq, 64] (GQA)     │
    #   │                          ↑                            │
    #   │                     2个KV头，被8个Q头共享              │
    #   └───────────────────────────────────────────────────────┘
    #       │
    #       ▼ 每个头独立计算注意力
    #   ┌───────────────────────────────────────────────────────┐
    #   │  头1: Q[:,0,:,:] × K[:,0,:,:]^T → softmax → × V[:,0]  │
    #   │  头2: Q[:,1,:,:] × K[:,0,:,:]^T → softmax → × V[:,0]  │
    #   │  头3: Q[:,2,:,:] × K[:,0,:,:]^T → softmax → × V[:,0]  │
    #   │  头4: Q[:,3,:,:] × K[:,0,:,:]^T → softmax → × V[:,0]  │
    #   │  头5: Q[:,4,:,:] × K[:,1,:,:]^T → softmax → × V[:,1]  │
    #   │  头6: Q[:,5,:,:] × K[:,1,:,:]^T → softmax → × V[:,1]  │
    #   │  头7: Q[:,6,:,:] × K[:,1,:,:]^T → softmax → × V[:,1]  │
    #   │  头8: Q[:,7,:,:] × K[:,1,:,:]^T → softmax → × V[:,1]  │
    #   │                                                       │
    #   │  注意：头1-4共享KV头0，头5-8共享KV头1（这就是GQA！）   │
    #   └───────────────────────────────────────────────────────┘
    #       │
    #       ▼ concat（拼接8个头的输出）
    #   ┌───────────────────────────────────────────────────────┐
    #   │  [batch, 8, seq, 64] → [batch, seq, 512]              │
    #   │                                                       │
    #   │  8个头 × 64维 = 512维（恢复原始维度）                 │
    #   └───────────────────────────────────────────────────────┘
    #       │
    #       ▼ 输出投影
    #   ┌───────────────────────────────────────────────────────┐
    #   │  output = concat_result @ W_o                         │
    #   │  W_o: [512, 512]                                      │
    #   │  输出: [batch, seq, 512]                              │
    #   └───────────────────────────────────────────────────────┘
    #
    # 【多头注意力的代码核心】
    #
    #   # 1. 投影
    #   Q = self.q_proj(x)  # [batch, seq, 512]
    #   K = self.k_proj(x)  # [batch, seq, 128]
    #   V = self.v_proj(x)  # [batch, seq, 128]
    #
    #   # 2. 重塑成多头形式
    #   Q = Q.view(batch, seq, 8, 64).transpose(1, 2)  # [batch, 8, seq, 64]
    #   K = K.view(batch, seq, 2, 64).transpose(1, 2)  # [batch, 2, seq, 64]
    #   V = V.view(batch, seq, 2, 64).transpose(1, 2)  # [batch, 2, seq, 64]
    #
    #   # 3. GQA: 扩展 K/V 让每个 Q 头都能用
    #   K = K.repeat_interleave(4, dim=1)  # [batch, 8, seq, 64]
    #   V = V.repeat_interleave(4, dim=1)  # [batch, 8, seq, 64]
    #
    #   # 4. 计算注意力（所有头并行！）
    #   scores = Q @ K.transpose(-2, -1) / sqrt(64)  # [batch, 8, seq, seq]
    #   attn = softmax(scores, dim=-1)
    #   output = attn @ V  # [batch, 8, seq, 64]
    #
    #   # 5. 合并多头
    #   output = output.transpose(1, 2).reshape(batch, seq, 512)
    #
    #   # 6. 输出投影
    #   output = self.o_proj(output)
    #
    # 【为什么多头能并行计算？】
    #
    # 关键洞察：维度操作！
    #
    # 把 heads 放到 batch 维度旁边 [batch, heads, seq, head_dim]
    # 矩阵乘法会自动对 batch 和 heads 两个维度并行
    # GPU 一次算完所有头，不是串行算 8 次！
    #
    # ════════════════════════════════════════════════════════════════
    # 【残差连接 - 为什么要 "+" ？】
    # ════════════════════════════════════════════════════════════════
    #
    # 代码: x = x + self.self_attn(self.input_layernorm(x))
    #           ↑
    #        这个 + 就是残差连接！
    #
    # 【问题】深层网络的梯度消失
    #
    # 假设有 80 层，反向传播时梯度要穿过 80 层
    # 每层梯度都会衰减一点，到第一层时梯度可能 ≈ 0
    # 结果：前面的层学不动！
    #
    # 【残差连接的解决方案】
    #
    # 不是 y = f(x)，而是 y = x + f(x)
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: 残差不应该是矩阵的某个属性吗？为什么就是加个 x？】
    # ════════════════════════════════════════════════════════════════════
    #
    # "残差" 这个词确实容易混淆！让我澄清：
    #
    # - 矩阵的残差 (matrix residual): 线性代数里的概念，跟这里无关
    # - 残差连接 (residual connection): 深度学习的技巧，就是加个 x
    #
    # 【实际代码实现 - 就是这么简单！】
    #
    #   # 没有残差连接:
    #   y = attention(x)          # y = f(x)
    #
    #   # 有残差连接:
    #   y = x + attention(x)      # y = x + f(x)，就是多加了个 x！
    #
    #   # MiniMind 的实际代码 (MiniMindBlock.forward):
    #   h = x + self.attention(self.attention_norm(x))  # 残差1
    #   out = h + self.feed_forward(self.ffn_norm(h))   # 残差2
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: 梯度不是 torch 内部记录的吗？残差跟梯度什么关系？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 没错，torch 确实会自动计算梯度。但关键是：
    # 不同的计算图，梯度的数值不一样！
    #
    # 【数值例子: 为什么“加1”很重要】
    #
    # 假设 f(x) 的梯度 = 0.5 (每层衰减一半)
    #
    # 没有残差连接 (y = f(x)):
    #   第1层梯度: 0.5
    #   第2层梯度: 0.5 × 0.5 = 0.25
    #   第3层梯度: 0.5 × 0.5 × 0.5 = 0.125
    #   ...
    #   第80层梯度: 0.5^80 ≈ 0.00000000000000000000000083  # 基本=0
    #
    # 有残差连接 (y = x + f(x)):
    #   每层梯度 = 1 + 0.5 = 1.5
    #   第1层梯度: 1.5
    #   第2层梯度: 1.5 (x路径) + 0.75 (f路径) ≈ 保持在 1 附近
    #   ...
    #   第80层梯度: 依然有意义！
    #
    # 【“抄近路”的真正含义】
    #
    # 没有残差连接时，梯度必须穿过每一层:
    #
    #   输入 → [层f1] → [层f2] → ... → [层f80] → 输出
    #          (梯度要一层层往回传，每层都乘个<1的数)
    #
    # 有残差连接时，梯度有“高速公路”:
    #
    #   输入 ───────────────────────────────→ (+) ─→ (+) ─→ 输出
    #     │                                  ↑       ↑
    #     └→[层f1]─→[层f2]─→...─→[层f80]─┘       │
    #                                               │
    #   梯度可以走上面那条“直通路”，不用穿过 f 层！
    #
    # 【为什么跟 1 比较不会丢失？】
    #
    # 你的疑问: “没有 x 跟 0 比较，加了 x 跟 1 比较，不还是会丢失？”
    #
    # 关键点: 不是“跟什么比较”，是“乘以什么”！
    #
    #   没有残差: 梯度 = 0.5 × 0.5 × 0.5 × ...  (连乘，越乘越小)
    #   有残差:   梯度 = (1 + 0.5) 的加权组合    (不是纯连乘！)
    #
    # 更简单的理解:
    #   没有残差: 0.5^80 ≈ 0       # 连乘小数 = 越来越小
    #   有残差:   至少有一条路径的梯度 = 1^80 = 1  # 连乘 1 = 不变
    #
    # 【直觉理解】
    #
    # 残差 = "改变量"
    # x + f(x) 的意思是：输出 = 原始输入 + 学到的改变
    #
    # 如果 f(x) 学到的东西没用，那就保持原样（f(x)≈0）
    # 这比从头学"正确答案"容易得多！
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: 每层都加 x，值不会爆炸吗？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 误解: 每层都加最初的 x0，越加越多
    # 实际: 每层加的是"当前层的输入"，是链式结构！
    #
    # 【链式结构 - 实际代码】
    #
    #   x0 = embedding(tokens)       # 原始输入
    #
    #   # 第1层
    #   x1 = x0 + f1(x0)             # x1 = x0 + 变化1
    #
    #   # 第2层 (注意: 输入是 x1，不是 x0！)
    #   x2 = x1 + f2(x1)             # x2 = x1 + 变化2
    #
    #   # 第3层
    #   x3 = x2 + f3(x2)             # x3 = x2 + 变化3
    #
    #   # ...以此类推
    #
    # 关键点: 每层加的是"当前层的输入"，而当前层的输入已经是上一层的输出！
    #
    # 【为什么不会爆炸？3个原因】
    #
    # 1. f(x) 学到的是"小改动"
    #    x1 = x0 + f1(x0)    # f1(x0) 是对 x0 的微调，不是全新的值
    #    x2 = x1 + f2(x1)    # f2(x1) 是对 x1 的微调
    #
    # 2. 每层有 RMSNorm 控制尺度
    #    # MiniMind 实际代码:
    #    h = x + self.attention(self.attention_norm(x))
    #                           ^^^^^^^^^^^^^^^^^^
    #                           RMSNorm 把值控制在合理范围！
    #
    # 3. 数值例子
    #    假设每层的 f(x) 平均贡献 = 0.1 × x
    #
    #    理论上 (无 Norm):
    #      x1 = 1.1 × x0
    #      x2 = 1.1 × x1 = 1.21 × x0
    #      x80 ≈ 1.1^80 × x0 ≈ 2000 × x0  # 会增长
    #
    #    实际上 (每层有 RMSNorm):
    #      x1 = norm(x0 + f(x0)) ≈ 保持标准范围
    #      x2 = norm(x1 + f(x1)) ≈ 保持标准范围
    #      ...
    #      x80 ≈ 依然在标准范围！
    #
    #    RMSNorm 在每层都会把数值"拉回"正常范围！
    #
    # 【总结】
    #
    # - 误解: 每层都加最初的 x0 → 累加爆炸
    # - 实际: 每层加当前层的输入 → 链式结构
    # - 不爆炸: RMSNorm 在每层控制尺度
    #
    # ════════════════════════════════════════════════════════════════
    # 【2.4 MLP/FFN - 前馈网络详解】
    # ════════════════════════════════════════════════════════════════
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: MLP 和 FFN 到底有什么区别？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 答: 没有区别！就是同一个东西的两个名字！
    #
    # - MLP = Multi-Layer Perceptron (多层感知机)
    # - FFN = Feed-Forward Network (前馈网络)
    #
    # 两个词完全可以互换，论文中随便用。
    # 本质就是: 几个线性层 + 激活函数。
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: 增加 2 个矩阵乘法就是 MLP？这不是纯粹造概念吗？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 不是造概念！关键是 “激活函数”！
    #
    # 如果只有矩阵乘法 (线性变换):
    #   y = x @ W1 @ W2 @ W3 ... = x @ W_组合
    #   无论多少层，最终可以合并成一个矩阵，等于只有一层！
    #
    # 加入激活函数 (非线性):
    #   y = W2 @ σ(W1 @ x)
    #   中间有个非线性操作，无法合并，真正实现了“多层”！
    #
    # 【最简单的 MLP: 2 个矩阵 + 1 个激活函数】
    #
    #   y = W2 @ ReLU(W1 @ x)
    #       │       │    │
    #       │       │    └─ 第1层: 512 → 1376
    #       │       └───── 非线性激活
    #       └─────────── 第2层: 1376 → 512
    #
    # 【FFN 是每层都有吗？】
    #
    # 是的！每个 Transformer 层都有：
    #
    #   Transformer层 = Attention + FFN
    #                 = "看别人"  + "自己想"
    #
    # MiniMind 有 8 层，就有 8 个 Attention + 8 个 FFN
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: SiLU/ReLU 到底是啥？激活函数是什么意思？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 【激活函数 = 简单的数学公式，把数值"扭曲"一下】
    #
    # ReLU(x) = max(0, x)  ← 就这么简单！负数变0，正数不变
    #   x = 5   → 5
    #   x = -3  → 0   ← 负数直接变0
    #   x = -100 → 0
    #
    # SiLU(x) = x × sigmoid(x) = x × (1/(1+e^(-x)))
    #   x = 5   → 4.97  (基本不变)
    #   x = -3  → -0.14 (被压小了，但没完全变0)
    #
    # 【为什么需要激活函数？】
    #
    # 没有激活函数：
    #   y = W2 @ W1 @ x = W_combined @ x  # 可以合并成一层！
    #   无论堆多少层矩阵，都等于一层，没意义！
    #
    # 有激活函数：
    #   y = W2 @ ReLU(W1 @ x)  # 中间有个非线性，不能合并！
    #   这才是真正的"多层"，能学到更复杂的模式！
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: RMSNorm 不也是非线性吗？为什么还要激活函数？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 不一样！RMSNorm 和激活函数做的事情完全不同！
    #
    # 【对比】
    #
    #   ┌─────────────┬────────────────────┬─────────────────────┐
    #   │             │ RMSNorm            │ 激活函数 (ReLU/SiLU) │
    #   ├─────────────┼────────────────────┼─────────────────────┤
    #   │ 目的        │ 控制数值大小       │ 引入非线性           │
    #   │ 做什么      │ 把向量"缩放"       │ 扭曲数值的"形状"     │
    #   │ 数学        │ x / sqrt(mean(x²)) │ max(0, x)           │
    #   │ 可否绕过    │ 可以（线性）       │ 不能（非线性）       │
    #   └─────────────┴────────────────────┴─────────────────────┘
    #
    # 【关键区别】
    #
    # RMSNorm 本质是"除以一个数"，还是线性操作：
    #   y = x / scale
    #   如果 x 翻倍，y 也翻倍，比例关系不变
    #
    # ReLU 是真正的非线性：
    #   x = -5  → 0
    #   x = -100 → 0
    #   不管输入多大的负数，输出都是 0，比例关系被破坏了！
    #
    # 【RMSNorm 不能替代激活函数！】
    #
    # RMSNorm 作用：防止数值爆炸/消失
    # 激活函数作用：让多层矩阵不能合并，真正实现"深度"
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: 为什么要 3 个矩阵，不是 2 个？】
    # ════════════════════════════════════════════════════════════════════
    #
    # MiniMind 用的是 SwiGLU，是一种“门控 MLP”，比普通 MLP 效果更好。
    #
    # 【普通 MLP (2个矩阵)】
    #
    #   y = down_proj( ReLU( up_proj(x) ) )
    #   y = W_down @ ReLU( W_up @ x )
    #
    # 【SwiGLU MLP (3个矩阵)】
    #
    #   y = down_proj( SiLU(gate_proj(x)) * up_proj(x) )
    #   y = W_down @ ( SiLU(W_gate @ x) * (W_up @ x) )
    #
    # 【为什么多一个 gate_proj？】
    #
    # gate_proj 输出经过 SiLU 后，得到 0~1 的“门控信号”
    # up_proj 输出是“候选内容”
    # 两者相乘 = 门决定哪些信息通过
    #
    # 这比“全部通过或全部截断”更精细，效果更好
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: token 之间怎么就有关联了？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 注意！MLP/FFN 本身不让 token 之间有关联！
    #
    # - Attention: 让 token 之间交流信息 (“看”其他词)
    # - MLP/FFN:   对每个 token 独立处理 (“思考”当前词)
    #
    # MLP 是“各算各的”，没有跨 token 交互！
    #
    # 【具体例子】 "我 爱 中国" (3个 token)
    #
    #   Attention 阶段 (交流):
    #   "我" 可以看到 "爱" 和 "中国"
    #   "爱" 可以看到 "我" 和 "中国"
    #   "中国" 可以看到 "我" 和 "爱"
    #   输出: 每个 token 的向量融合了其他 token 的信息
    #
    #   MLP 阶段 (独立思考):
    #   "我" 的向量 → MLP → "我" 的新向量 (只处理“我”)
    #   "爱" 的向量 → MLP → "爱" 的新向量 (只处理“爱”)
    #   "中国" 的向量 → MLP → "中国" 的新向量 (只处理“中国”)
    #
    # MLP 的作用: 对 Attention 融合后的信息做"深度加工"
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: Attention 怎么"看"其他词？具体怎么实现的？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 关键是 Q @ K.T 产生了"注意力分数矩阵"！
    #
    # 【数值例子】 "我 爱 中国" (3个 token)
    #
    # Step 1: 每个 token 生成自己的 Q, K, V 向量
    #
    #   Q = [q_我, q_爱, q_中国]  # 形状 [3, 64]
    #   K = [k_我, k_爱, k_中国]  # 形状 [3, 64]
    #   V = [v_我, v_爱, v_中国]  # 形状 [3, 64]
    #
    # Step 2: Q @ K.T 计算"谁看谁"的分数
    #
    #   scores = Q @ K.T  # [3, 64] @ [64, 3] = [3, 3]
    #
    #   这个 [3, 3] 矩阵就是注意力分数：
    #
    #              k_我   k_爱   k_中国
    #   q_我  [   0.8    0.1    0.1   ]  <- "我"对每个词的注意力
    #   q_爱  [   0.3    0.4    0.3   ]  <- "爱"对每个词的注意力
    #   q_中国[   0.2    0.3    0.5   ]  <- "中国"对每个词的注意力
    #
    # Step 3: 用分数加权求和 V
    #
    #   output_我 = 0.8 * v_我 + 0.1 * v_爱 + 0.1 * v_中国
    #
    #   "我"的新向量 = 80%自己 + 10%"爱" + 10%"中国" 的信息混合！
    #
    # 【这就是"看"的含义】
    #
    # "看"其他词 = 用注意力分数把其他词的向量"混进来"
    # 不是真的"看"，是数学上的加权求和
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: MLP 怎么就"独立处理"了？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 【对比矩阵乘法的形状】
    #
    # Attention 有跨 token 的乘法：
    #   scores = Q @ K.T   # [seq, 64] @ [64, seq] = [seq, seq]
    #                                                 ^^^
    #                                    这是 token x token 的矩阵！
    #
    # MLP 没有跨 token 的乘法：
    #   output = x @ W    # [seq, 512] @ [512, 1376] = [seq, 1376]
    #                                                   ^^^
    #                                    seq 维度完全独立！
    #
    # 【代码证明】
    #
    #   def forward(self, x):  # x: [batch, seq, 512]
    #       gate = self.gate_proj(x)   # [batch, seq, 1376]
    #       up = self.up_proj(x)       # [batch, seq, 1376]
    #       # 没有任何操作让不同 seq 位置的 token 交互！
    #       return self.down_proj(SiLU(gate) * up)
    #
    # 【总结】
    # Attention: Q @ K.T 产生 [seq, seq] -> token 交互
    # MLP:       x @ W 产生 [seq, dim]   -> token 独立
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: FFN 的 gate * up 不也是矩阵吗？为什么没有交互？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 关键区别: @ 是矩阵乘法，* 是逐元素乘法！
    #
    # 【Attention: Q @ K.T (矩阵乘法)】
    #
    #      K.T (转置后每个token是一列)
    #      ↓
    #    [k0 k1 k2]     <- 3个token的K向量(列)
    # Q→ [q0]  [q0·k0  q0·k1  q0·k2]   <- token0 和所有token算点积
    #    [q1]  [q1·k0  q1·k1  q1·k2]   <- token1 和所有token算点积
    #    [q2]  [q2·k0  q2·k1  q2·k2]   <- token2 和所有token算点积
    #          ↑
    #          每个格子都是两个不同token的交互！
    #
    # 【FFN: gate * up (逐元素乘法)】
    #
    # gate:  [g0_0, g0_1, ...]   <- token0 的 gate
    #        [g1_0, g1_1, ...]   <- token1 的 gate
    #        [g2_0, g2_1, ...]   <- token2 的 gate
    #
    # up:    [u0_0, u0_1, ...]   <- token0 的 up
    #        [u1_0, u1_1, ...]   <- token1 的 up
    #        [u2_0, u2_1, ...]   <- token2 的 up
    #
    # gate * up (逐元素):
    #        [g0_0*u0_0, g0_1*u0_1, ...]  <- token0 自己和自己乘
    #        [g1_0*u1_0, g1_1*u1_1, ...]  <- token1 自己和自己乘
    #        [g2_0*u2_0, g2_1*u2_1, ...]  <- token2 自己和自己乘
    #        ↑
    #        每行只用自己的数据，没有跨行！
    #
    # 【关键区别】
    #
    #   ┌────────────────┬─────────────────┬─────────────────────┐
    #   │                │ Attention Q@K.T │ FFN gate*up         │
    #   ├────────────────┼─────────────────┼─────────────────────┤
    #   │ 运算符         │ @ 矩阵乘法      │ * 逐元素乘法        │
    #   │ 形状变化       │ [seq,64]@[64,seq]=[seq,seq] │ [seq,1376]*[seq,1376]=[seq,1376] │
    #   │ token 交互     │ 有! 第i行j列=token_i和j交互 │ 无! 每行独立        │
    #   └────────────────┴─────────────────┴─────────────────────┘
    #
    # 一句话: @ 跨行列交互，* 只在对应位置相乘
    #
    # ════════════════════════════════════════════════════════════════════
    # 【3个矩阵的作用 - 数值例子】
    # ════════════════════════════════════════════════════════════════════
    #
    # 输入 x = [1.0, -0.5, 2.0, 0.3] (简化为4维)
    #
    # Step 1: gate_proj(x) - 计算"门"
    #   gate = W_gate @ x = [0.8, -1.2, 0.5, 2.1, -0.3, 0.7] (6维)
    #
    # Step 2: SiLU(gate) - 激活门
    #   gate_activated = [0.57, -0.26, 0.31, 1.94, -0.11, 0.47]
    #   (SiLU 把负数压到接近 0，正数保留)
    #
    # Step 3: up_proj(x) - 计算"候选内容"
    #   up = W_up @ x = [1.5, 0.3, -0.8, 1.2, 0.9, -0.4] (6维)
    #
    # Step 4: gate * up - 门控
    #   hidden = [0.57×1.5, -0.26×0.3, 0.31×-0.8, ...]
    #          = [0.86, -0.08, -0.25, 2.33, -0.10, -0.19]
    #
    #   注意: gate 接近 0 的位置，输出也接近 0 (被“关闭”了)
    #         gate 接近 1 的位置，输出保留 up 的值 (被“放行”了)
    #
    # Step 5: down_proj(hidden) - 压缩回原始维度
    #   output = W_down @ hidden = [0.42, 0.15, -0.33, 0.78] (4维)
    #
    # 【总结: 3个矩阵的分工】
    #
    # - gate_proj: “可以让哪些信息通过？” (学习筛选规则)
    # - up_proj:   “有哪些候选信息？”       (学习特征提取)
    # - down_proj: “如何整合回原始维度？”   (学习维度压缩)
    #
    # 【MLP 的作用】
    #
    # 【结构】
    #
    #   输入 x: [batch, seq, 512]
    #       │
    #       ├──────────────────────┐
    #       ▼                      ▼
    #   gate_proj               up_proj
    #   [512→1376]             [512→1376]
    #       │                      │
    #       ▼                      ▼
    #     SiLU                   直通
    #       │                      │
    #       └────────┬─────────────┘
    #                │
    #                ▼
    #             逐元素相乘 (门控)
    #                │
    #                ▼
    #            down_proj
    #            [1376→512]
    #                │
    #                ▼
    #            输出: [batch, seq, 512]
    #
    # 【门控机制 (Gated)】
    #
    # 代码: output = down_proj(SiLU(gate_proj(x)) * up_proj(x))
    #
    # gate_proj 输出经过 SiLU 激活，变成 0~1 之间的"门"
    # up_proj 输出是"候选值"
    # 两者相乘 = 门控制哪些信息通过
    #
    # 【为什么中间维度是 1376？】
    #
    # 这叫 intermediate_size（中间层大小）
    # 通常是 hidden_size 的 2.7~4 倍
    # 512 × 2.7 ≈ 1376
    #
    # 先升维（512→1376）再降维（1376→512）
    # 给模型更大的"思考空间"
    #
    # 【SiLU 激活函数】
    #
    # SiLU(x) = x × sigmoid(x) = x × (1 / (1 + e^(-x)))
    #
    # 特点：
    #   - 平滑的非线性
    #   - 允许负值通过（不像 ReLU 直接截断）
    #   - 效果比 ReLU 更好
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │                    一、什么是 Token 和 Vocab？               │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【Token = 语言的最小单位】
    # 电脑不认识文字，只认识数字。所以要把文字切成小块，每块对应一个数字。
    #
    # 例子: "我爱中国" → ["我", "爱", "中", "国"] → [156, 892, 45, 231]
    #
    # 【Vocab (词表) = 所有 token 的字典】
    # vocab_size = 6400 表示字典里有 6400 个"词"
    # 每个词对应一个 ID: "我"→156, "爱"→892, ...
    #
    # ════════════════════════════════════════════════════════════════
    # 【重要澄清】Token ID vs 512维向量 vs 6400 vs 隐藏层数，它们是什么关系？
    # ════════════════════════════════════════════════════════════════
    #
    # 这是初学者最容易混淆的地方！让我彻底讲清楚：
    #
    # 【第一步】文字 → Token ID（一个整数）
    #
    #   "我爱中国" → [156, 892, 45, 231]
    #
    #   这一步只是"查字典"，每个词变成一个整数（ID）
    #   这个 ID 范围是 0 ~ 6399（因为词表大小 vocab_size = 6400）
    #
    # 【第二步】Token ID → 512维向量（Embedding！）
    #
    #   这是关键的一步！ID 只是索引，不能直接计算
    #   需要把 ID 转换成"有意义"的向量
    #
    #   ┌────────────────────────────────────────────────────────────┐
    #   │  Embedding 层本质上是一个大查找表:                         │
    #   │                                                            │
    #   │  embed_tokens.weight 形状: [6400, 512]                     │
    #   │                             ↑     ↑                        │
    #   │                          词表大小  隐藏维度                 │
    #   │                                                            │
    #   │  每一行是一个词的 512 维向量表示                           │
    #   │                                                            │
    #   │  第 0 行: 词 ID=0 的向量 [0.12, -0.34, 0.56, ..., 0.78]   │
    #   │  第 1 行: 词 ID=1 的向量 [0.23, 0.45, -0.67, ..., 0.89]   │
    #   │  ...                                                       │
    #   │  第 156 行: "我" 的向量 [0.11, 0.22, 0.33, ..., 0.99]     │
    #   │  ...                                                       │
    #   │  第 6399 行: 最后一个词的向量                              │
    #   └────────────────────────────────────────────────────────────┘
    #
    #   代码: embedding_vector = embed_tokens.weight[token_id]
    #
    # 【具体例子】"我爱中国" 的 Embedding 过程
    #
    #   输入 Token IDs: [156, 892, 45, 231]  （4个整数）
    #
    #   经过 Embedding 后:
    #   ┌────────────────────────────────────────────────────────────┐
    #   │  "我" (ID=156) → [0.11, 0.22, 0.33, ..., 0.99]  (512个数) │
    #   │  "爱" (ID=892) → [0.12, 0.34, 0.56, ..., 0.78]  (512个数) │
    #   │  "中" (ID=45)  → [0.21, 0.43, 0.65, ..., 0.87]  (512个数) │
    #   │  "国" (ID=231) → [0.13, 0.35, 0.57, ..., 0.79]  (512个数) │
    #   └────────────────────────────────────────────────────────────┘
    #
    #   输出形状: [4, 512] = [句子长度, 隐藏维度]
    #              ↑   ↑
    #           4个词  每个词512维
    #
    # 【512 和 6400 的关系】
    #
    #   6400 = vocab_size = 词表大小 = 模型认识多少个不同的词
    #   512  = hidden_size = 隐藏维度 = 用多少个数字来描述一个词
    #
    #   它们是完全独立的两个概念！
    #
    #   - 6400 决定"能认识多少词"
    #   - 512 决定"每个词的表达有多丰富"
    #
    #   Embedding 矩阵形状 [6400, 512] 就是把这两个概念连接起来
    #
    # ════════════════════════════════════════════════════════════════
    # 【隐藏层数 vs 隐藏维度：完全不同的概念！】
    # ════════════════════════════════════════════════════════════════
    #
    # 很多人把这两个搞混了！
    #
    # 【隐藏维度 hidden_size = 512】
    #
    #   = 每个 token 用多少个数字表示
    #   = 向量的长度
    #   = "宽度"
    #
    # 【隐藏层数 num_hidden_layers = 8】
    #
    #   = 有多少个 Transformer Block 堆叠
    #   = 数据要经过多少层处理
    #   = "深度"
    #
    # 【图示】
    #
    #   输入: "我" → [512维向量]
    #            │
    #            ▼
    #      ┌─────────────┐
    #      │ Block 1     │ ← 第1层，输入512维，输出512维
    #      └─────────────┘
    #            │
    #            ▼
    #      ┌─────────────┐
    #      │ Block 2     │ ← 第2层，输入512维，输出512维
    #      └─────────────┘
    #            │
    #            ▼
    #          ...
    #            │
    #            ▼
    #      ┌─────────────┐
    #      │ Block 8     │ ← 第8层，输入512维，输出512维
    #      └─────────────┘
    #            │
    #            ▼
    #   输出: [512维向量]
    #
    #   注意：每一层的输入输出都是 512 维！维度不变！
    #   8层只是说数据要"过"8次处理，不是变成8维！
    #
    # 【类比】
    #
    #   隐藏维度 512 = 一条河的"宽度"（水流有多宽）
    #   隐藏层数 8   = 河流的"长度"（水要流多远）
    #
    #   水流宽度不会因为河流长度增加而改变！
    #
    # 【为什么维度始终是 512？】
    #
    #   因为每一层的 Linear 都是 [512, 512] 的方阵！
    #   输入 512 维 → 输出 512 维，维度不变
    #
    #   唯一改变维度的地方：
    #   - Embedding: ID → 512 维（开始）
    #   - lm_head: 512 维 → 6400 维（结束，预测下一个词）
    #
    # ════════════════════════════════════════════════════════════════
    # 【总结：一句话的完整处理流程】
    # ════════════════════════════════════════════════════════════════
    #
    #   "我爱中国"
    #       │
    #       ▼ Tokenizer（分词 + 查字典）
    #   [156, 892, 45, 231]        ← 4个整数（Token IDs）
    #       │
    #       ▼ Embedding（查表，ID→向量）
    #   [[0.1, 0.2, ..., 0.9],    ← "我" 的512维向量
    #    [0.2, 0.3, ..., 0.8],    ← "爱" 的512维向量
    #    [0.3, 0.4, ..., 0.7],    ← "中" 的512维向量
    #    [0.4, 0.5, ..., 0.6]]    ← "国" 的512维向量
    #   形状: [4, 512]
    #       │
    #       ▼ 8层 Transformer Block（每层输入输出都是512维）
    #   [[...512个数...],
    #    [...512个数...],
    #    [...512个数...],
    #    [...512个数...]]
    #   形状: [4, 512]（维度没变！只是值变了）
    #       │
    #       ▼ lm_head（512→6400，预测下一个词）
    #   [[...6400个数...],        ← 第1个位置预测的6400个词的概率
    #    [...6400个数...],        ← 第2个位置预测的6400个词的概率
    #    [...6400个数...],        ← 第3个位置预测的6400个词的概率
    #    [...6400个数...]]        ← 第4个位置预测的6400个词的概率
    #   形状: [4, 6400]
    #       │
    #       ▼ 取最后一个位置，argmax 选概率最大的词
    #   预测结果: "很"（下一个词）
    #
    # ════════════════════════════════════════════════════════════════
    #
    # 【为什么 vocab 越大，模型越大？】
    #
    # 模型中有两个地方的大小和 vocab_size 直接相关:
    #
    # 1. 词嵌入层 (embed_tokens):
    #    - 每个词需要一个 hidden_size 维的向量
    #    - 参数量 = vocab_size × hidden_size
    #    - 例: 6400 × 512 = 3,276,800 个参数 (约 3.3M)
    #    - 如果 vocab=50000: 50000 × 512 = 25,600,000 (约 25.6M)
    #
    # 2. 输出层 (lm_head):
    #    - 把 hidden_size 映射到 vocab_size，预测下一个词
    #    - 参数量 = hidden_size × vocab_size
    #    - 和词嵌入一样大！
    #
    #    【疑问】512 维怎么能映射到 6400 维？这不是变多了吗？
    #
    #    这是个好问题！关键在于理解"映射"的含义:
    #
    #    矩阵乘法可以改变维度！
    #    输入: [1, 512] 的向量
    #    权重: [512, 6400] 的矩阵
    #    输出: [1, 512] @ [512, 6400] = [1, 6400]
    #
    #    具体计算:
    #    ┌─────────────────────────────────────────────────────────┐
    #    │  输出的第 i 个值 = 输入向量 · 权重矩阵的第 i 列         │
    #    │                                                         │
    #    │  output[0] = x[0]*W[0,0] + x[1]*W[1,0] + ... + x[511]*W[511,0]  │
    #    │  output[1] = x[0]*W[0,1] + x[1]*W[1,1] + ... + x[511]*W[511,1]  │
    #    │  ...                                                    │
    #    │  output[6399] = x[0]*W[0,6399] + ... + x[511]*W[511,6399]│
    #    └─────────────────────────────────────────────────────────┘
    #
    #    每个输出值是 512 个数的加权和，权重是学出来的！
    #
    #    【类比】就像投票:
    #    - 512 个评委 (隐藏维度)
    #    - 6400 个候选人 (词表)
    #    - 每个评委对每个候选人有个打分权重
    #    - 最终分数 = 所有评委的加权打分之和
    #    - 分数最高的候选人获胜 (预测的下一个词)
    #
    #    【为什么能工作？】
    #    - 512 维足够编码语义信息
    #    - lm_head 学会了"如果隐藏向量长这样，下一个词很可能是 X"
    #    - 通过 softmax 把 6400 个分数变成概率分布
    #
    # 【总计】vocab 相关参数 = 2 × vocab_size × hidden_size
    #
    #   vocab=6400:   2 × 6400 × 512   = 6.5M 参数
    #   vocab=50000:  2 × 50000 × 512  = 51.2M 参数
    #   vocab=150000: 2 × 150000 × 512 = 153.6M 参数
    #
    # 对于小模型，vocab 相关参数可能占总参数的 30%~50%！
    # 所以 MiniMind 用较小的 vocab (6400) 来控制模型大小。
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │              二、什么是 Hidden Size (隐藏维度)？             │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【每个 token 用一个向量表示】
    # hidden_size = 512 表示每个词用 512 个数字来描述。
    #
    # 为什么要这么多数字？因为一个词的含义很丰富:
    # - "苹果" 可能是水果，也可能是手机品牌
    # - 需要足够多的维度来区分不同语境下的含义
    #
    # 想象成: 每个词是 512 维空间中的一个点
    # - 相似的词（如"开心"、"快乐"）在空间中靠近
    # - 不同的词（如"开心"、"悲伤"）在空间中远离
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │           三、什么是注意力 (Attention)？Q/K/V 是什么？       │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【问题】理解一个词，需要看它周围的词
    #
    # 例: "苹果很好吃" vs "苹果发布新手机"
    #     同样是"苹果"，但含义完全不同！
    #     → 需要根据上下文来理解每个词
    #
    # 【注意力机制 = 让每个词"看"其他词，决定该关注谁】
    #
    # 用一个生活中的比喻来解释 Q/K/V:
    #
    # 想象你在图书馆找书:
    # ┌─────────────────────────────────────────────────────────────┐
    # │                                                             │
    # │  你 (Query): "我想找关于 Python 编程的书"                   │
    # │       ↓                                                     │
    # │  书架上每本书的标签 (Key):                                  │
    # │     - 书1: "Python 入门"      ← 很匹配！                    │
    # │     - 书2: "Java 编程"        ← 有点相关                    │
    # │     - 书3: "法国历史"         ← 不相关                      │
    # │       ↓                                                     │
    # │  取出书的内容 (Value):                                      │
    # │     - 主要看书1的内容 (权重高)                              │
    # │     - 稍微看看书2 (权重低)                                  │
    # │     - 忽略书3 (权重≈0)                                      │
    # │                                                             │
    # └─────────────────────────────────────────────────────────────┘
    #
    # 【Q/K/V 的数学定义】
    #
    # 对于句子中的每个词:
    #   Q (Query)  = "我在找什么？" → 这个词想要的信息
    #   K (Key)    = "我有什么标签？" → 这个词能提供什么
    #   V (Value)  = "我的具体内容" → 这个词的实际信息
    #
    # 【注意力计算步骤】
    #
    # 1. 计算相关度: score = Q · K^T  (点积，越大越相关)
    # 2. 归一化: weights = softmax(score)  (变成概率，和为1)
    # 3. 加权求和: output = weights × V  (按相关度混合信息)
    #
    # 【具体例子】"苹果 很 好吃"
    #
    # 当处理"苹果"时:
    #   Q_苹果 与 K_很、K_好吃 计算相关度
    #   → 发现和"好吃"很相关 (权重0.7)
    #   → 和"很"有点相关 (权重0.3)
    #   → 输出 = 0.7 × V_好吃 + 0.3 × V_很
    #   → "苹果"的表示融合了"好吃"的信息，模型知道这是水果！
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │             四、什么是注意力头 (Attention Head)？            │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【问题】一个词可能从多个角度需要不同的信息
    #
    # 例: "小明 昨天 在 北京 吃了 苹果"
    #
    # 对于"吃了"这个词，需要知道:
    # - 谁吃的？→ 关注"小明" (语法关系)
    # - 什么时候？→ 关注"昨天" (时间关系)
    # - 在哪吃？→ 关注"北京" (地点关系)
    # - 吃了什么？→ 关注"苹果" (动宾关系)
    #
    # 【多头注意力 = 多个独立的"视角"】
    #
    # num_attention_heads = 8 表示有 8 个注意力头
    #
    # 每个头学习关注不同类型的关系:
    # - 头1: 可能学会关注主谓关系
    # - 头2: 可能学会关注动宾关系
    # - 头3: 可能学会关注时间词
    # - ...
    #
    # 【head_dim = hidden_size / num_heads】
    #
    # hidden_size=512, num_heads=8 → head_dim=64
    #
    # 每个头处理 64 维的子空间，8 个头合起来是 512 维
    # 这样既有多个视角，又保持总维度不变
    #
    # ═══════════════════════════════════════════════════════════════
    # 【多头注意力的完整数学算法】
    # ═══════════════════════════════════════════════════════════════
    #
    # 设: batch_size=B, seq_len=S, hidden_size=512, num_heads=8, head_dim=64
    #
    # 【Step 1: 线性投影生成 Q、K、V】
    #
    #   输入 X: [B, S, 512]
    #
    #   Q = X @ W_q    # [B, S, 512] @ [512, 512] → [B, S, 512]
    #   K = X @ W_k    # [B, S, 512] @ [512, 512] → [B, S, 512]
    #   V = X @ W_v    # [B, S, 512] @ [512, 512] → [B, S, 512]
    #
    # 【Step 2: 拆分成多个头 (reshape)】
    #
    #   单头形式: Q/K/V = [B, S, 512]
    #
    #   多头形式: Q/K/V = [B, S, 8, 64]  # 8个头，每头64维
    #                   → [B, 8, S, 64]  # 转置，让 head 维度在前
    #
    #   代码实现:
    #     Q = Q.view(B, S, 8, 64).transpose(1, 2)  # [B, 8, S, 64]
    #
    #   这步相当于把 512 维向量"切"成 8 段，每段 64 维给一个头
    #
    # 【Step 3: 每个头独立计算注意力】
    #
    #   对于每个头 h (h = 1, 2, ..., 8):
    #
    #   Q_h: [B, S, 64]   (第 h 个头的 Query)
    #   K_h: [B, S, 64]   (第 h 个头的 Key)
    #   V_h: [B, S, 64]   (第 h 个头的 Value)
    #
    #   (3a) 计算注意力分数:
    #        Scores_h = Q_h @ K_h^T / sqrt(64)
    #                 = [B, S, 64] @ [B, 64, S] / 8
    #                 = [B, S, S]
    #
    #        除以 sqrt(head_dim) 是为了防止点积值过大导致 softmax 梯度消失
    #
    #   (3b) 转换为概率分布:
    #        Weights_h = softmax(Scores_h, dim=-1)
    #                  = [B, S, S]
    #
    #        每行是一个概率分布，表示该 token 对所有其他 token 的关注度
    #
    #   (3c) 加权聚合 Value:
    #        Output_h = Weights_h @ V_h
    #                 = [B, S, S] @ [B, S, 64]
    #                 = [B, S, 64]
    #
    # 【Step 4: 合并所有头的输出】
    #
    #   8 个头的输出: [B, 8, S, 64]
    #
    #   转置并 reshape:
    #     → [B, S, 8, 64]
    #     → [B, S, 512]   # 8×64=512，恢复原始维度
    #
    #   代码实现:
    #     output = output.transpose(1, 2).contiguous().view(B, S, 512)
    #
    # 【Step 5: 输出投影】
    #
    #   Final = output @ W_o
    #         = [B, S, 512] @ [512, 512]
    #         = [B, S, 512]
    #
    # ═══════════════════════════════════════════════════════════════
    # 【具体数值示例】
    # ═══════════════════════════════════════════════════════════════
    #
    # 输入句子: "我 爱 中国" (3 个 token, seq_len=3)
    # 假设 batch_size=1, hidden_size=512, num_heads=8, head_dim=64
    #
    # Step 1: X = [1, 3, 512]  (3 个 token 的嵌入)
    #
    # Step 2: 拆分成 8 个头
    #         Q = [1, 8, 3, 64]  (每个头看 64 维)
    #
    # Step 3: 头1 独立计算 (假设关注语法关系)
    #
    #         Q_1 @ K_1^T = [1, 3, 64] @ [1, 64, 3] = [1, 3, 3]
    #
    #         得到注意力矩阵 (假设):
    #         ┌──────────────────────────────┐
    #         │           我     爱    中国  │
    #         │  我      0.8    0.1    0.1   │  ← "我"主要关注自己
    #         │  爱      0.3    0.5    0.2   │  ← "爱"关注主语和自己
    #         │ 中国     0.1    0.7    0.2   │  ← "中国"关注谓语"爱"
    #         └──────────────────────────────┘
    #
    #         头2 可能学到不同的模式 (假设关注位置关系):
    #         ┌──────────────────────────────┐
    #         │           我     爱    中国  │
    #         │  我      0.9    0.05   0.05  │  ← 关注前一个词
    #         │  爱      0.7    0.2    0.1   │
    #         │ 中国     0.1    0.6    0.3   │
    #         └──────────────────────────────┘
    #
    # Step 4: 8 个头输出 concat → [1, 3, 512]
    #
    # Step 5: 输出投影 → [1, 3, 512]
    #
    # ═══════════════════════════════════════════════════════════════
    # 【为什么多头比单头好？】
    # ═══════════════════════════════════════════════════════════════
    #
    # 单头 (head_dim=512):
    # - 只能学习一种关系模式
    # - 所有维度共享同一套注意力权重
    #
    # 多头 (8×head_dim=8×64=512):
    # - 每个头在独立的 64 维子空间学习
    # - 不同头自动分工，关注不同语义关系
    # - 更丰富的表达能力，同样的参数量
    #
    # 实验证明: 8 个头效果 >> 1 个头，但并非越多越好
    #          头太多 → head_dim 太小 → 每个头表达能力不足
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │               五、什么是 KV 头 (GQA 技术)？                  │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【问题】注意力计算中，K 和 V 需要大量显存
    #
    # 推理时需要缓存之前所有 token 的 K 和 V (叫 KV Cache)
    # 如果对话很长，KV Cache 会占用几个 GB 显存！
    #
    # 【GQA (分组查询注意力) = 让多个 Q 头共享 K/V 头】
    #
    # num_attention_heads = 8 (Q 头数)
    # num_key_value_heads = 2 (KV 头数)
    #
    # 意思是: 8 个 Q 头，只有 2 个 KV 头
    # 每 4 个 Q 头共享 1 个 KV 头
    #
    # ┌─────────────────────────────────────────────┐
    # │  Q头: [Q1, Q2, Q3, Q4] [Q5, Q6, Q7, Q8]     │
    # │         ↓     ↓     ↓     ↓      ↓     ↓    │
    # │  KV头:      [KV1]            [KV2]          │
    # └─────────────────────────────────────────────┘
    #
    # 【效果】
    # - 显存减少 4 倍 (KV Cache 从 8 份变 2 份)
    # - 推理速度更快
    # - 效果几乎不下降 (研究证明 KV 头可以共享)
    #
    # ═══════════════════════════════════════════════════════════════
    # 【KV Cache 到底存储了什么？】
    # ═══════════════════════════════════════════════════════════════
    #
    # 【核心问题】KV Cache 存的是 512 维还是 64 维？
    #
    # 答案: 存的是每个 KV 头的 head_dim 维度，不是完整的 512 维！
    #
    # 【详细拆解】
    #
    # 配置: hidden_size=512, num_heads=8, num_kv_heads=2, head_dim=64
    #
    # 标准 MHA (Multi-Head Attention, 8 个 KV 头):
    #   K 投影后: [B, S, 512] → 拆成 8 个头 → [B, 8, S, 64]
    #   V 投影后: [B, S, 512] → 拆成 8 个头 → [B, 8, S, 64]
    #   KV Cache 每个层存储: K=[B, 8, S, 64] + V=[B, 8, S, 64]
    #
    # GQA (Grouped-Query Attention, 2 个 KV 头):
    #   K 投影后: [B, S, 128] → 拆成 2 个头 → [B, 2, S, 64]
    #   V 投影后: [B, S, 128] → 拆成 2 个头 → [B, 2, S, 64]
    #   KV Cache 每个层存储: K=[B, 2, S, 64] + V=[B, 2, S, 64]
    #
    # 【显存计算示例】
    #
    # 假设: 8层, seq_len=2048, batch=1, float16 (2 bytes)
    #
    # MHA (8 KV头):
    #   每层 KV Cache = 2 × 8 × 2048 × 64 × 2 bytes = 4 MB
    #   总共 = 8层 × 4 MB = 32 MB
    #
    # GQA (2 KV头):
    #   每层 KV Cache = 2 × 2 × 2048 × 64 × 2 bytes = 1 MB
    #   总共 = 8层 × 1 MB = 8 MB
    #
    # 显存节省: 32MB → 8MB，减少 75%！
    #
    # ═══════════════════════════════════════════════════════════════
    # 【Q头/KV头的关系 - 完整算法流程】
    # ═══════════════════════════════════════════════════════════════
    #
    # 【Step 1: 投影生成 Q/K/V】
    #
    #   输入 X: [B, S, 512]
    #
    #   Q = X @ W_q    # [B, S, 512] @ [512, 512] → [B, S, 512]
    #   K = X @ W_k    # [B, S, 512] @ [512, 128] → [B, S, 128]  ← 注意: 只有128维!
    #   V = X @ W_v    # [B, S, 512] @ [512, 128] → [B, S, 128]  ← 注意: 只有128维!
    #
    #   Q 有 8×64=512 维 (8 个 Q 头)
    #   K 有 2×64=128 维 (2 个 KV 头)
    #   V 有 2×64=128 维 (2 个 KV 头)
    #
    # 【Step 2: 拆分成多头】
    #
    #   Q: [B, S, 512] → [B, S, 8, 64] → [B, 8, S, 64]  (8 个 Q 头)
    #   K: [B, S, 128] → [B, S, 2, 64] → [B, 2, S, 64]  (2 个 KV 头)
    #   V: [B, S, 128] → [B, S, 2, 64] → [B, 2, S, 64]  (2 个 KV 头)
    #
    # 【Step 3: KV 头复制/广播】 ← 这是关键！
    #
    #   问题: 8 个 Q 头要和 2 个 KV 头做注意力，维度不匹配！
    #
    #   解决: 把 2 个 KV 头"复制"成 8 个，每个复制 4 次
    #
    #   K: [B, 2, S, 64] → [B, 8, S, 64]  (复制扩展)
    #   V: [B, 2, S, 64] → [B, 8, S, 64]  (复制扩展)
    #
    #   具体实现 (repeat_interleave):
    #     n_rep = 8 // 2 = 4  (每个 KV 头复制 4 次)
    #     K = K.repeat_interleave(n_rep, dim=1)  # [B, 2, S, 64] → [B, 8, S, 64]
    #
    #   结果:
    #     KV头1 复制给 Q头 1,2,3,4 用
    #     KV头2 复制给 Q头 5,6,7,8 用
    #
    # 【Step 4: 标准注意力计算】
    #
    #   现在 Q/K/V 都是 [B, 8, S, 64]，可以正常计算:
    #
    #   Scores = Q @ K^T / sqrt(64)  # [B, 8, S, S]
    #   Weights = softmax(Scores)     # [B, 8, S, S]
    #   Output = Weights @ V          # [B, 8, S, 64]
    #
    # 【Step 5: 合并输出】
    #
    #   Output: [B, 8, S, 64] → [B, S, 512]
    #   Final = Output @ W_o  # [B, S, 512]
    #
    # ═══════════════════════════════════════════════════════════════
    # 【KV Cache 的工作原理】
    # ═══════════════════════════════════════════════════════════════
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: 512拆成8个64，跟token有什么关系？不是直接拆矩阵吗？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 你混淆了两个完全不同的维度！
    #
    # K 的完整形状: [batch, num_heads, seq_len, head_dim]
    #                                   ↑        ↑
    #                              token数量   每头64维
    #                              (会增长!)   (固定)
    #
    # "512拆成8个64" = head_dim 维度，是固定的，跟token无关
    # "KV Cache存的" = seq_len 维度，是每个token的K/V向量
    #
    # 【每个 token 都有自己的 K/V 向量！】
    #
    # 输入: "我 爱 中国" (3个token)
    #
    # 每个 token 都会生成自己的 K 和 V:
    #
    #   token "我"   → K_我:   [8头, 64维]  V_我:   [8头, 64维]
    #   token "爱"   → K_爱:   [8头, 64维]  V_爱:   [8头, 64维]
    #   token "中国" → K_中国: [8头, 64维]  V_中国: [8头, 64维]
    #
    # 所以 K 矩阵的完整形状是:
    #
    #   K = [K_我, K_爱, K_中国]
    #     = [3个token, 8头, 64维]
    #        ↑
    #        这就是 seq_len 维度！每个 token 一行！
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: 多头注意力不是只看部分K吗？不是全面的自注意力？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 错！每个头都看所有 token！
    #
    # 头0: Q_我[头0] 和 K_我[头0], K_爱[头0], K_中国[头0] 都算点积
    # 头1: Q_我[头1] 和 K_我[头1], K_爱[头1], K_中国[头1] 都算点积
    # ...
    # 头7: Q_我[头7] 和 K_我[头7], K_爱[头7], K_中国[头7] 都算点积
    #
    # 不是"头0只看部分token"！
    # 而是: 每个头都看全部token，只是用不同的64维子空间计算
    #
    # 【图解】
    #
    #   头0的注意力矩阵 [3,3]:     头1的注意力矩阵 [3,3]:
    #      我  爱  中国               我  爱  中国
    #   我 0.8 0.1 0.1             我 0.5 0.3 0.2
    #   爱 0.2 0.6 0.2             爱 0.4 0.4 0.2
    #   中国 0.1 0.2 0.7           中国 0.3 0.2 0.5
    #
    #   每个头都是 [3,3]，都覆盖所有token！
    #   只是学到的注意力模式不同
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: 为什么新token需要之前的K？KV Cache存来干嘛？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 因为注意力公式需要和所有之前的token算点积！
    #
    #   scores = Q_新 @ [K_所有之前的token].T
    #
    # 【具体例子】
    #
    #   第1步: 输入"你好"
    #     计算 K_你好, V_你好 → 存入cache
    #     Q_你好 @ [K_你好].T → 生成 "，"
    #
    #   第2步: 输入","（新token）
    #     计算 K_逗号, V_逗号 → cache = [K_你好, K_逗号]
    #     Q_逗号 @ [K_你好, K_逗号].T → 生成 "世"
    #                  ↑
    #           需要之前token的K才能算注意力！
    #
    #   第3步: 输入"世"
    #     计算 K_世, V_世 → cache = [K_你好, K_逗号, K_世]
    #     Q_世 @ [K_你好, K_逗号, K_世].T → 生成 "界"
    #
    # 【为什么要存cache？】
    #
    # 不存cache: 每次都要重算之前所有token的K/V (浪费计算)
    # 存cache:   只算新token的K/V，之前的直接从cache读
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: KV Cache 最多存多少？一直生成一直存？】
    # ════════════════════════════════════════════════════════════════════
    #
    # 有上限！由 max_seq_len 配置决定
    #
    # MiniMind 默认: max_seq_len = 512
    # GPT-4:        max_seq_len = 8192 或更大
    #
    # 存满了怎么办？
    #   1. 滑动窗口: 丢弃最早的token (常用)
    #   2. 报错停止: 达到最大长度就停止生成
    #   3. 压缩: 把旧的信息压缩合并 (研究中)
    #
    # 【内存占用计算】
    #
    # 每层 KV Cache 大小:
    #   = 2(K和V) × num_kv_heads × seq_len × head_dim × 2(float16)
    #   = 2 × 2 × 512 × 64 × 2 = 256KB (每层)
    #
    # 8层总共: 256KB × 8 = 2MB
    #
    # 这就是为什么长上下文模型需要更多显存！
    #
    # ════════════════════════════════════════════════════════════════════
    # 【完整流程梳理】用 "明天天气是晴天真好" (8个token) 举例
    # ════════════════════════════════════════════════════════════════════
    #
    # 输入: "明天天气是晴天真好"
    # 假设: batch=1, seq_len=8, hidden_size=512, num_heads=8, head_dim=64
    #
    # ┌─────────────────────────────────────────────────────────────────┐
    # │ Step 1: Tokenizer 分词                                          │
    # └─────────────────────────────────────────────────────────────────┘
    #
    # "明天天气是晴天真好" → [token_ids]
    #
    # 假设分词结果:
    #   "明" → 101
    #   "天" → 102
    #   "天" → 102  (同一个字，同一个ID)
    #   "气" → 103
    #   "是" → 104
    #   "晴" → 105
    #   "天" → 102
    #   "真" → 106
    #   "好" → 107
    #
    # token_ids = [101, 102, 102, 103, 104, 105, 102, 106, 107]
    # 形状: [1, 9]  (batch=1, seq=9个token)
    #
    # ┌─────────────────────────────────────────────────────────────────┐
    # │ Step 2: Embedding 嵌入                                          │
    # └─────────────────────────────────────────────────────────────────┘
    #
    # embed_tokens.weight: [6400, 512]  ← 词表大小 × 隐藏维度
    #
    # 每个 token_id 查表得到 512 维向量:
    #   token_id=101 → embed_tokens.weight[101] → [0.1, 0.2, ..., 0.5] (512维)
    #   token_id=102 → embed_tokens.weight[102] → [0.3, 0.1, ..., 0.8] (512维)
    #   ...
    #
    # 输出 x: [1, 9, 512]
    #         batch=1, 9个token, 每个512维
    #
    # ┌─────────────────────────────────────────────────────────────────┐
    # │ Step 3: 进入 Transformer 层 (共8层，这里展示1层)                 │
    # └─────────────────────────────────────────────────────────────────┘
    #
    # ═══════════════════════════════════════════════════════════════════
    # Step 3.1: RMSNorm (归一化)
    # ═══════════════════════════════════════════════════════════════════
    #
    # x_norm = RMSNorm(x)  # [1, 9, 512] → [1, 9, 512]
    # 每个 token 的 512 维向量被归一化到标准长度
    #
    # ═══════════════════════════════════════════════════════════════════
    # Step 3.2: Q/K/V 投影
    # ═══════════════════════════════════════════════════════════════════
    #
    # W_q: [512, 512]   (8头 × 64维)
    # W_k: [512, 128]   (2头 × 64维, GQA)
    # W_v: [512, 128]   (2头 × 64维, GQA)
    #
    # Q = x_norm @ W_q  # [1, 9, 512] @ [512, 512] = [1, 9, 512]
    # K = x_norm @ W_k  # [1, 9, 512] @ [512, 128] = [1, 9, 128]
    # V = x_norm @ W_v  # [1, 9, 512] @ [512, 128] = [1, 9, 128]
    #
    # 【每个token都有自己的Q/K/V！】
    #
    #   token "明" → Q_明:[512], K_明:[128], V_明:[128]
    #   token "天" → Q_天:[512], K_天:[128], V_天:[128]
    #   ...
    #
    # ═══════════════════════════════════════════════════════════════════
    # Step 3.3: Reshape 成多头形式
    # ═══════════════════════════════════════════════════════════════════
    #
    # Q: [1, 9, 512] → [1, 8, 9, 64]   # 8个头，每头64维
    #                   batch,heads,seq,head_dim
    #
    # K: [1, 9, 128] → [1, 2, 9, 64]   # 2个KV头
    # V: [1, 9, 128] → [1, 2, 9, 64]
    #
    # 【GQA扩展】每个KV头被4个Q头共享:
    # K: [1, 2, 9, 64] → repeat → [1, 8, 9, 64]
    # V: [1, 2, 9, 64] → repeat → [1, 8, 9, 64]
    #
    # ═══════════════════════════════════════════════════════════════════
    # Step 3.4: 计算注意力分数 (每个头独立)
    # ═══════════════════════════════════════════════════════════════════
    #
    # 【疑问澄清: 注意力矩阵的维度】
    #
    # 不是 [seq×64, seq×64]！是 [seq, seq]！
    #
    # Q[:,head_i,:,:]: [1, 9, 64]   # 第i个头的Q
    # K[:,head_i,:,:]: [1, 9, 64]   # 第i个头的K
    # K.T:             [1, 64, 9]   # 转置
    #
    # scores = Q @ K.T = [1, 9, 64] @ [1, 64, 9] = [1, 9, 9]
    #                                               ↑
    #                                    64维在点积中被"消耗"！
    #                                    结果是 token×token 的相似度
    #
    # 【具体数值例子 - 头0的注意力矩阵 [9, 9]】
    #
    #        明   天   天   气   是   晴   天   真   好
    #   明  0.3  0.1  0.1  0.1  0.1  0.1  0.1  0.05 0.05
    #   天  0.2  0.2  0.15 0.1  0.1  0.1  0.1  0.03 0.02
    #   天  0.15 0.2  0.2  0.1  0.1  0.1  0.1  0.03 0.02
    #   气  0.1  0.15 0.15 0.2  0.1  0.1  0.1  0.05 0.05
    #   是  0.1  0.1  0.1  0.1  0.2  0.15 0.1  0.1  0.05
    #   晴  0.1  0.1  0.1  0.1  0.1  0.2  0.15 0.1  0.05
    #   天  0.1  0.15 0.15 0.1  0.1  0.15 0.15 0.05 0.05
    #   真  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.2  0.1
    #   好  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.2
    #
    # 每一行加起来=1 (softmax后)
    # scores[0][1]=0.1 表示 "明" 对 "天" 的注意力是 10%
    #
    # 【8个头各自有自己的 [9,9] 矩阵！】
    # 合起来: scores: [1, 8, 9, 9]
    #
    # ═══════════════════════════════════════════════════════════════════
    # Step 3.5: 加权求和 V
    # ═══════════════════════════════════════════════════════════════════
    #
    # attn_output = softmax(scores) @ V
    #             = [1, 8, 9, 9] @ [1, 8, 9, 64]
    #             = [1, 8, 9, 64]
    #
    # 【每个token的新向量 = 所有token的V的加权和】
    #
    # output_明 = 0.3×V_明 + 0.1×V_天 + 0.1×V_天 + 0.1×V_气 + ...
    #             ↑
    #         "明"的新向量融合了所有token的信息！
    #
    # ═══════════════════════════════════════════════════════════════════
    # Step 3.6: 合并多头 + 输出投影
    # ═══════════════════════════════════════════════════════════════════
    #
    # concat: [1, 8, 9, 64] → [1, 9, 512]  # 8个头拼起来
    # output = concat @ W_o  # [1, 9, 512] @ [512, 512] = [1, 9, 512]
    #
    # ═══════════════════════════════════════════════════════════════════
    # Step 3.7: 残差连接
    # ═══════════════════════════════════════════════════════════════════
    #
    # x = x + output  # [1, 9, 512] + [1, 9, 512] = [1, 9, 512]
    #
    # ═══════════════════════════════════════════════════════════════════
    # Step 3.8: FFN (前馈网络)
    # ═══════════════════════════════════════════════════════════════════
    #
    # x_norm = RMSNorm(x)  # [1, 9, 512]
    #
    # gate = x_norm @ W_gate  # [1, 9, 512] @ [512, 1376] = [1, 9, 1376]
    # up = x_norm @ W_up      # [1, 9, 512] @ [512, 1376] = [1, 9, 1376]
    # hidden = SiLU(gate) * up  # [1, 9, 1376] 逐元素相乘
    # ffn_out = hidden @ W_down # [1, 9, 1376] @ [1376, 512] = [1, 9, 512]
    #
    # 【FFN 对每个 token 独立处理！】
    # "明"只用自己的向量，不看其他token
    #
    # ═══════════════════════════════════════════════════════════════════
    # Step 3.9: 残差连接
    # ═══════════════════════════════════════════════════════════════════
    #
    # x = x + ffn_out  # [1, 9, 512]
    #
    # 【重复 Step 3 共 8 层】
    #
    # ┌─────────────────────────────────────────────────────────────────┐
    # │ Step 4: 最终输出                                                │
    # └─────────────────────────────────────────────────────────────────┘
    #
    # x_final = RMSNorm(x)  # [1, 9, 512]
    # logits = x_final @ lm_head.weight.T  # [1, 9, 512] @ [512, 6400] = [1, 9, 6400]
    #
    # logits[0, 8, :] = 第9个位置（"好"之后）对所有6400个词的概率分数
    # next_token = argmax(logits[0, 8, :])  # 取概率最高的词作为下一个token
    #
    # ┌─────────────────────────────────────────────────────────────────┐
    # │ 总结: 每一步的矩阵形状变化                                       │
    # └─────────────────────────────────────────────────────────────────┘
    #
    # token_ids:     [1, 9]           ← 9个整数
    # Embedding:     [1, 9, 512]      ← 每个token变成512维向量
    # Q投影:         [1, 9, 512]      ← 保持不变
    # K/V投影:       [1, 9, 128]      ← GQA压缩到128维
    # 多头reshape:   [1, 8, 9, 64]    ← 8个头，每头64维
    # 注意力分数:    [1, 8, 9, 9]     ← 8个头各自的 token×token 矩阵
    # 注意力输出:    [1, 8, 9, 64]    ← 加权求和后
    # 合并多头:      [1, 9, 512]      ← 拼接回512维
    # FFN中间:       [1, 9, 1376]     ← 升维
    # FFN输出:       [1, 9, 512]      ← 降维回来
    # 最终logits:    [1, 9, 6400]     ← 对应词表每个词的分数
    #
    # ════════════════════════════════════════════════════════════════════
    #
    # 【为什么需要 KV Cache?】
    #
    # 自回归生成时，每次只生成 1 个新 token:
    #
    #   第1步: "你好" → 计算 K1,V1 → 生成 "，"
    #   第2步: "你好，" → 重新计算 K1,K2,V1,V2 → 生成 "世"
    #   第3步: "你好，世" → 重新计算 K1,K2,K3,V1,V2,V3 → 生成 "界"
    #   ...
    #
    # 问题: 每次都重新计算之前所有 token 的 K/V，太浪费！
    #
    # 【KV Cache 解决方案】
    #
    #   第1步: 计算 K1,V1 → 存入 cache → 生成 "，"
    #   第2步: 只计算 K2,V2 → cache += [K2,V2] → 生成 "世"
    #   第3步: 只计算 K3,V3 → cache += [K3,V3] → 生成 "界"
    #   ...
    #
    #   每次只计算新 token 的 K/V，然后拼接到 cache 后面
    #
    # 【KV Cache 的数据结构】
    #
    #   每层存储一个 tuple: (K_cache, V_cache)
    #
    #   K_cache: [B, num_kv_heads, current_seq_len, head_dim]
    #          = [1, 2, 100, 64]  (已生成 100 个 token)
    #
    #   V_cache: [B, num_kv_heads, current_seq_len, head_dim]
    #          = [1, 2, 100, 64]
    #
    #   生成第 101 个 token 时:
    #     新 K: [1, 2, 1, 64]  (只有 1 个新 token)
    #     K_cache = concat(K_cache, 新K, dim=2)  # [1, 2, 101, 64]
    #
    # ═══════════════════════════════════════════════════════════════
    # 【三种注意力变体对比】
    # ═══════════════════════════════════════════════════════════════
    #
    # ┌──────────┬─────────┬─────────┬────────────────────┐
    # │ 类型       │ Q头数   │ KV头数  │ KV Cache 大小       │
    # ├──────────┼─────────┼─────────┼────────────────────┤
    # │ MHA       │ 8       │ 8       │ 100% (baseline)    │
    # │ GQA       │ 8       │ 2       │ 25%  (4倍节省)       │
    # │ MQA       │ 8       │ 1       │ 12.5% (8倍节省)      │
    # └──────────┴─────────┴─────────┴────────────────────┘
    #
    # MHA: Multi-Head Attention (每个 Q 头有自己的 KV)
    # GQA: Grouped-Query Attention (多个 Q 头共享一组 KV)
    # MQA: Multi-Query Attention (所有 Q 头共享同一个 KV)
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │           六、Q/K/V/O 投影层是什么？为什么需要？             │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【原始输入】每个 token 是 512 维向量 x
    #
    # 【问题】同一个向量 x，如何变出 Q、K、V 三个不同的东西？
    #
    # 【答案】用三个不同的矩阵去"投影"！
    #
    #   Q = x @ W_q  (W_q 是 q_proj 的权重)
    #   K = x @ W_k  (W_k 是 k_proj 的权重)
    #   V = x @ W_v  (W_v 是 v_proj 的权重)
    #
    # 不同的投影矩阵，让同一个输入变成不同角色:
    # - W_q 学习: "这个词想找什么信息"
    # - W_k 学习: "这个词能提供什么信息"
    # - W_v 学习: "这个词的实际内容是什么"
    #
    # 【O 投影 (输出投影)】
    #
    # 多头注意力的输出需要合并回 512 维:
    #   output = concat(head1, head2, ..., head8) @ W_o
    #
    # W_o 学习如何融合多个头的信息
    #
    # ═══════════════════════════════════════════════════════════════
    # 【核心疑问: W_q/W_k/W_v 怎么知道分别学习了什么？如何控制？】
    # ═══════════════════════════════════════════════════════════════
    #
    # 【答案: 我们不"控制"，是通过梯度下降自动学习出来的！】
    #
    # 【数学原理 - 为什么 Q/K/V 会自动分工】
    #
    # 1. 它们在注意力公式中的位置不同:
    #
    #    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d)) @ V
    #
    #    - Q 和 K 做点积，计算相似度
    #    - V 被加权求和，提供内容
    #
    # 2. 梯度传播路径不同:
    #
    #    损失 L 对 W_q 的梯度: ∂L/∂W_q
    #    损失 L 对 W_k 的梯度: ∂L/∂W_k
    #    损失 L 对 W_v 的梯度: ∂L/∂W_v
    #
    #    这三个梯度完全不同，因此学到的东西也不同！
    #
    # 【具体分析 - 为什么会形成 Q/K/V 的"分工"】
    #
    # (1) W_q 为什么学"想找什么"?
    #
    #     训练目标: 预测下一个词
    #     例: "我喜欢吃___"
    #
    #     为了预测"苹果"，"吃"这个词需要找到"喜欢"的信息
    #     梯度会推动 W_q 让 Q_吃 能够与 K_喜欢 产生高分
    #     所以 W_q 学会"这个词想找什么类型的信息"
    #
    # (2) W_k 为什么学"能提供什么"?
    #
    #     同上，梯度会推动 W_k 让 K_喜欢 能够被 Q_吃 找到
    #     所以 W_k 学会"这个词能提供什么类型的信息"
    #
    # (3) W_v 为什么学"实际内容"?
    #
    #     注意力输出 = 加权求和(V)
    #     V 的内容会直接影响最终输出
    #     梯度会推动 W_v 存储"能帮助预测的实际信息"
    #
    # 【数学证明 - 为什么不能只用一个矩阵】
    #
    #   假设 Q = K = V = x @ W (用同一个矩阵)
    #
    #   Attention = softmax(xW @ (xW)^T / sqrt(d)) @ xW
    #             = softmax(xW @ W^T x^T / sqrt(d)) @ xW
    #
    #   问题: Q 和 K 的内積会变成对称的!
    #         A 对 B 的注意力 = B 对 A 的注意力
    #
    #   但语言常常是不对称的:
    #   - "苹果"要关注"吃" (动宾关系)
    #   - 但"吃"可能更关注"我" (主谓关系)
    #
    #   分开 Q 和 K 允许这种不对称性！
    #
    # 【重要: 我们不显式控制，是结构决定功能】
    #
    #   ✔ 不需要手动设计 W_q 学什么
    #   ✔ 不需要添加额外损失来约束
    #   ✔ 只要结构设计正确，学习自然发生
    #
    #   类比: 像工具的设计决定它的用途
    #   - 锤子的结构决定了它能砸东西
    #   - Q/K/V 的位置决定了它们学不同的东西
    #
    # ═══════════════════════════════════════════════════════════════
    # 【实际观察 - 训练后 W_q/W_k/W_v 长什么样？】
    # ═══════════════════════════════════════════════════════════════
    #
    # 研究发现 (BertViz 等工具可视化):
    #
    # 【W_q 学到的模式】
    # - 某些头: 让动词的 Q 向量与宾语的 K 向量相似
    # - 某些头: 让名词的 Q 向量与形容词的 K 向量相似
    # - 某些头: 让任意词的 Q 向量与前一个词的 K 向量相似
    #
    # 【W_v 学到的模式】
    # - 编码"词性"信息 (动词/名词/形容词)
    # - 编码"语义角色"信息 (施事者/受事者)
    # - 编码"位置"信息 (距离当前词多远)
    #
    # 【实验验证】
    #
    # 如果把 W_q = W_k (强制共享参数):
    # - 训练损失上升 5-10%
    # - 注意力矩阵变得对称
    # - 证明了 Q/K 分开的必要性
    #
    # ┌──────────────────────────────────────────────────────────────┐
    # │           七、什么是 intermediate_size (FFN 中间层)？        │
    # └──────────────────────────────────────────────────────────────┘
    #
    # 【Transformer 的两大组件】
    # 1. Attention: 让词与词之间交流信息
    # 2. FFN (前馈网络): 对每个词独立做非线性变换
    #
    # 【FFN 结构】
    #   输入 512 维 → 扩展到 1376 维 → 压缩回 512 维
    #
    # intermediate_size = 1376 就是这个中间层的维度
    #
    # 【为什么要先扩展再压缩？】
    #
    # 就像画家画画:
    # - 先在大画布上画草稿 (扩展维度，有更多空间表达)
    # - 然后精选最好的部分 (压缩维度，提取精华)
    #
    # 中间层越大，模型的"表达空间"越大，能学到更复杂的模式
    #
    # ═══════════════════════════════════════════════════════════════
    # 【FFN 的完整数学算法 - SwiGLU 变体】
    # ═══════════════════════════════════════════════════════════════
    #
    # MiniMind 用的是 SwiGLU 激活函数，这是 LLaMA 系列的标配
    #
    # 【传统 FFN】 (ReLU 版本)
    #
    #   FFN(x) = ReLU(x @ W_up) @ W_down
    #
    #   计算步骤:
    #   1. x @ W_up:   [512] @ [512, 1376] → [1376]  扩展
    #   2. ReLU:       max(0, x)                      非线性激活
    #   3. @ W_down:   [1376] @ [1376, 512] → [512]  压缩
    #
    # 【SwiGLU FFN】 (LLaMA/MiniMind 版本)
    #
    #   FFN(x) = (SiLU(x @ W_gate) ⊗ (x @ W_up)) @ W_down
    #
    #   计算步骤:
    #   1. gate = x @ W_gate: [512] @ [512, 1376] → [1376]  门控信号
    #   2. gate = SiLU(gate): x * sigmoid(x)             平滑激活
    #   3. up = x @ W_up:     [512] @ [512, 1376] → [1376]  上投影
    #   4. hidden = gate ⊗ up: [1376] ⊗ [1376] → [1376]     逐元素相乘
    #   5. out = hidden @ W_down: [1376] @ [1376, 512] → [512] 下投影
    #
    # 【为什么要用门控机制 (GLU)？】
    #
    #   普通 FFN: 所有 1376 维都直接过激活函数
    #   GLU FFN:   gate 决定哪些维度“通过”，哪些“过滤”
    #
    #   gate ⊗ up 的效果:
    #   - 如果 gate[i] 接近 0 → 输出[i] 被"关闭"
    #   - 如果 gate[i] 接近 1 → 输出[i] = up[i] "打开"
    #
    #   这让模型能更精细地控制信息流动
    #
    # 【SiLU 激活函数是什么？】
    #
    #   SiLU(x) = x * sigmoid(x) = x * (1 / (1 + e^(-x)))
    #
    #   特点:
    #   - 平滑可导 (不像 ReLU 在 0 点不可导)
    #   - 允许负值通过 (不像 ReLU 会"杀死"负值)
    #   - 实验证明效果比 ReLU 好
    #
    #   图示:
    #   x:     -3   -2   -1    0    1    2    3
    #   SiLU: -0.14 -0.24 -0.27 0   0.73  1.76  2.86
    #
    # ═══════════════════════════════════════════════════════════════
    # 【完整数值示例】
    # ═══════════════════════════════════════════════════════════════
    #
    # 输入: x = [0.5, -0.3, 0.8, ...]  (512 维，这里只展示前几个)
    #
    # Step 1: gate = x @ W_gate
    #         假设 gate = [1.2, -0.5, 2.0, 0.1, ...]  (1376 维)
    #
    # Step 2: gate = SiLU(gate)
    #         SiLU(1.2)  = 1.2 * sigmoid(1.2) = 1.2 * 0.77 = 0.92
    #         SiLU(-0.5) = -0.5 * sigmoid(-0.5) = -0.5 * 0.38 = -0.19
    #         SiLU(2.0)  = 2.0 * sigmoid(2.0) = 2.0 * 0.88 = 1.76
    #         gate = [0.92, -0.19, 1.76, 0.05, ...]  (1376 维)
    #
    # Step 3: up = x @ W_up
    #         假设 up = [0.8, 1.5, -0.3, 0.6, ...]  (1376 维)
    #
    # Step 4: hidden = gate ⊗ up  (逐元素相乘)
    #         hidden = [0.92*0.8, -0.19*1.5, 1.76*(-0.3), 0.05*0.6, ...]
    #                = [0.74, -0.29, -0.53, 0.03, ...]  (1376 维)
    #
    # Step 5: out = hidden @ W_down
    #         out = [0.12, 0.45, -0.22, ...]  (512 维)
    #
    # ═══════════════════════════════════════════════════════════════
    # 【FFN 的实际作用 - 为什么需要它？】
    # ═══════════════════════════════════════════════════════════════
    #
    # 【问题: Attention 不够吗？】
    #
    # Attention 的局限性:
    # - Attention 只做线性加权和 (softmax 后加权平均 V)
    # - 线性运算无法表达复杂函数 (XOR 等)
    # - 需要非线性激活函数来增加表达能力
    #
    # 【FFN 的三大作用】
    #
    # 1️⃣ 提供非线性:
    #    - SiLU 激活函数让模型能学习复杂模式
    #    - 没有非线性，多层网络 = 一层网络
    #
    # 2️⃣ 存储知识:
    #    - FFN 的参数量占到模型的 2/3
    #    - 研究证明 FFN 存储了大量事实知识
    #    - 例: "巴黎 → 法国首都" 这种知识主要存在 FFN 里
    #
    # 3️⃣ 独立处理:
    #    - Attention: token 之间交流信息
    #    - FFN: 每个 token 独立做"深度思考"
    #    - 两者互补，缺一不可
    #
    # 【实验验证】
    #
    # 如果去掉 FFN (只保留 Attention):
    # - 模型性能暴跌 30-50%
    # - 无法回答事实性问题 ("北京是哪个国家的首都？")
    # - 证明 FFN 在存储知识中的重要性
    #
    # 【Attention vs FFN 的分工】
    #
    # ┌───────────┬─────────────────┬─────────────────┐
    # │           │ Attention        │ FFN              │
    # ├───────────┼─────────────────┼─────────────────┤
    # │ 作用范围   │ 全序列 (token间) │ 单个 token      │
    # │ 主要功能   │ 信息融合         │ 特征变换/存储知识 │
    # │ 参数占比   │ ~1/3            │ ~2/3            │
    # │ 计算特点   │ 与序列长度有关   │ 与序列长度无关   │
    # └───────────┴─────────────────┴─────────────────┘
    #
    # 【PyTorch 代码实现】
    #
    #   class MLP(nn.Module):
    #       def __init__(self, hidden_size, intermediate_size):
    #           self.gate_proj = nn.Linear(hidden_size, intermediate_size)
    #           self.up_proj = nn.Linear(hidden_size, intermediate_size)
    #           self.down_proj = nn.Linear(intermediate_size, hidden_size)
    #
    #       def forward(self, x):
    #           gate = F.silu(self.gate_proj(x))  # SiLU 激活
    #           up = self.up_proj(x)
    #           return self.down_proj(gate * up)  # 逗号后的 * 是逐元素相乘
    #
    # ================================================================
    # LLM 中所有 nn.Linear 层的完整清单（以 MiniMind 为例）
    # ================================================================
    #
    # 假设配置: hidden_size=512, num_attention_heads=8, num_key_value_heads=2
    #          intermediate_size=1376, vocab_size=6400
    #
    # 【1. Attention 层的投影矩阵】
    # ┌─────────────┬────────────────────────┬──────────────┬──────────┐
    # │ 名称        │ 作用                   │ 形状         │ 是方阵？ │
    # ├─────────────┼────────────────────────┼──────────────┼──────────┤
    # │ q_proj      │ 生成 Query             │ [512, 512]   │ ✓ 是     │
    # │ k_proj      │ 生成 Key (GQA 共享)    │ [128, 512]   │ ✗ 否     │
    # │ v_proj      │ 生成 Value (GQA 共享)  │ [128, 512]   │ ✗ 否     │
    # │ o_proj      │ 输出投影               │ [512, 512]   │ ✓ 是     │
    # └─────────────┴────────────────────────┴──────────────┴──────────┘
    #
    # 【2. FFN/MLP 层的投影矩阵】
    # ┌─────────────┬────────────────────────┬──────────────┬──────────┐
    # │ 名称        │ 作用                   │ 形状         │ 是方阵？ │
    # ├─────────────┼────────────────────────┼──────────────┼──────────┤
    # │ gate_proj   │ 门控信号 (SwiGLU)      │ [1376, 512]  │ ✗ 否     │
    # │ up_proj     │ 上投影 (扩展维度)      │ [1376, 512]  │ ✗ 否     │
    # │ down_proj   │ 下投影 (压缩回来)      │ [512, 1376]  │ ✗ 否     │
    # └─────────────┴────────────────────────┴──────────────┴──────────┘
    #
    # 【3. 其他层】
    # ┌─────────────┬────────────────────────┬──────────────┬──────────┐
    # │ 名称        │ 作用                   │ 形状         │ 是方阵？ │
    # ├─────────────┼────────────────────────┼──────────────┼──────────┤
    # │ embed_tokens│ 词嵌入 (Embedding)     │ [6400, 512]  │ ✗ 否     │
    # │ lm_head     │ 输出词表概率           │ [6400, 512]  │ ✗ 否     │
    # └─────────────┴────────────────────────┴──────────────┴──────────┘
    #
    # ================================================================
    # 为什么用 shape[0] == shape[1] 判断？
    # ================================================================
    #
    # 【核心洞察】在 LLM 中，只有 Attention 的 Q 和 O 投影是方阵！
    #
    # 原因：
    # - Q 投影: 输入 hidden_size → 输出 num_heads × head_dim
    #          = 512 → 8 × 64 = 512  （方阵！）
    #
    # - O 投影: 输入 num_heads × head_dim → 输出 hidden_size
    #          = 512 → 512  （方阵！）
    #
    # - K/V 投影: 因为 GQA (分组查询注意力) 让 KV 头数 < Q 头数
    #          = 512 → 2 × 64 = 128  （不是方阵）
    #
    # - FFN 层: 中间维度是 hidden_size 的 2.67 倍
    #          = 512 → 1376 或 1376 → 512  （不是方阵）
    #
    # 【这个判断的效果】
    # - 自动选中: q_proj, o_proj（每层 2 个）
    # - 自动跳过: k_proj, v_proj, gate_proj, up_proj, down_proj, embed, lm_head
    #
    # 【为什么只对 Q 和 O 加 LoRA？】
    # 1. 参数效率: 这两层对模型行为影响最大
    # 2. 研究表明: 微调 Q 和 O 就能达到很好的效果
    # 3. 内存节省: 只用 2 层而不是 7 层
    #
    # 【注意】这是一种简化策略！
    # - 原版 LoRA 论文建议对 Q/K/V/O 都加
    # - 有些实现还会对 FFN 加 LoRA
    # - 这里用方阵判断只是一种简便的实现方式
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: 代码用的是 nn.Linear，不是 nn.Parameter，怎么能训练？】
    # ════════════════════════════════════════════════════════════════════
    #
    # nn.Linear 内部就包含 nn.Parameter！
    #
    # 【nn.Linear 的源码 (简化版)】
    #
    #   class Linear(nn.Module):
    #       def __init__(self, in_features, out_features):
    #           self.weight = nn.Parameter(torch.randn(out_features, in_features))
    #                         ^^^^^^^^^^^^
    #                         nn.Linear 内部的 weight 就是 nn.Parameter！
    #           self.bias = nn.Parameter(torch.zeros(out_features))
    #
    # 所以 LoRA 的定义:
    #   self.up_proj = nn.Linear(in_features, rank)    # A 矩阵
    #   self.down_proj = nn.Linear(rank, out_features) # B 矩阵
    #
    # 等价于:
    #   self.up_proj.weight   是 nn.Parameter  ← 会被优化器更新！
    #   self.down_proj.weight 是 nn.Parameter  ← 会被优化器更新！
    #
    # ════════════════════════════════════════════════════════════════════
    # 【疑问: state_dict() 如何正确提取 A/B 矩阵？】
    # ════════════════════════════════════════════════════════════════════
    #
    # state_dict() 会自动收集所有 nn.Parameter！
    #
    # 【LoRA 类的 state_dict()】
    #
    #   class LoRA(nn.Module):
    #       def __init__(self, in_features, out_features, rank):
    #           self.up_proj = nn.Linear(in_features, rank)    # A
    #           self.down_proj = nn.Linear(rank, out_features) # B
    #
    #   module.lora.state_dict() 返回:
    #   {
    #       'up_proj.weight':   tensor([...]),   # A 矩阵 [rank, in_features]
    #       'down_proj.weight': tensor([...]),   # B 矩阵 [out_features, rank]
    #   }
    #
    # 【save_lora 的工作原理】
    #
    #   for name, module in model.named_modules():
    #       if hasattr(module, 'lora'):
    #           lora_state = module.lora.state_dict()
    #           # lora_state = {'up_proj.weight': A, 'down_proj.weight': B}
    #
    # state_dict() 是 PyTorch 内置方法，自动遍历所有子模块的 nn.Parameter！
    #
    # ================================================================
    
    for name, module in model.named_modules():
        # 只为方阵线性层添加 LoRA
        # 在本模型中，方阵 = Attention 的 q_proj 和 o_proj
        if isinstance(module, nn.Linear) and module.weight.shape[0] == module.weight.shape[1]:
            
            # ============================================================
            # .to(model.device) 是什么？为什么需要它？
            # ============================================================
            #
            # 【问题背景】
            # 神经网络的张量必须在同一个"设备"上才能计算:
            # - CPU: 普通内存，计算慢
            # - GPU (cuda:0, cuda:1...): 显卡显存，计算快
            #
            # 如果 model 在 GPU 上，而 LoRA 在 CPU 上:
            #   model(x) + lora(x)  →  报错！不同设备的张量不能相加
            #
            # 【.to(device) 做了什么？】
            # 把模块的所有参数（weight, bias 等）移动到指定设备:
            #   lora.to("cuda:0")  →  lora.A.weight 和 lora.B.weight 都移到 GPU
            #
            # 【model.device 是什么？】
            # 获取 model 所在的设备，通常通过第一个参数的设备来判断:
            #   model.device = next(model.parameters()).device
            #
            # 这样 LoRA 就和原模型在同一个设备上了，可以正常计算！
            #
            # ============================================================
            
            # 创建 LoRA 适配器，并移动到和模型相同的设备
            lora = LoRA(module.weight.shape[0], module.weight.shape[1], rank=rank).to(model.device)
            
            # ============================================================
            # setattr 是什么？
            # ============================================================
            #
            # setattr(obj, "name", value) 等价于 obj.name = value
            #
            # 这里的作用:
            #   module.lora = lora
            #
            # 把 LoRA 适配器"挂"到原始层上，方便后续访问和保存
            #
            # ============================================================
            
            setattr(module, "lora", lora)
            
            # ============================================================
            # 如何"劫持" forward 函数？这是 Python 闭包的魔法！
            # ============================================================
            #
            # 【第一步】保存原始 forward 函数的引用
            #
            # original_forward 现在指向 module 原来的 forward 方法
            # 即使后面 module.forward 被覆盖，original_forward 仍然有效！
            #
            # 这就像:
            #   原来: module.forward → 函数A
            #   保存: original_forward → 函数A  (直接指向函数A)
            #   覆盖: module.forward → 函数B
            #   调用 original_forward() 仍然执行函数A！
            #
            # ============================================================
            
            original_forward = module.forward

            # ============================================================
            # 闭包 (Closure) 和默认参数的技巧
            # ============================================================
            #
            # 【问题】为什么要写 layer1=original_forward, layer2=lora？
            #
            # 如果直接写:
            #   def forward_with_lora(x):
            #       return original_forward(x) + lora(x)  # ❌ 有 bug!
            #
            # 这会出问题！因为 Python 闭包的"延迟绑定"特性:
            # - 函数定义时不会立即读取 original_forward 和 lora 的值
            # - 而是在函数调用时才去查找这两个变量
            # - 循环结束后，所有函数都会指向最后一次循环的值！
            #
            # 【解决方案】用默认参数"立即绑定"
            #   def forward_with_lora(x, layer1=original_forward, layer2=lora):
            #
            # 默认参数在函数定义时就会计算并保存:
            # - layer1 立即绑定到当前的 original_forward
            # - layer2 立即绑定到当前的 lora
            # - 每次循环创建的函数都有自己独立的 layer1 和 layer2！
            #
            # 【layer1(x) 为什么能调用原来的 forward？】
            #
            # layer1 就是 original_forward，它是一个可调用对象:
            #   layer1(x)
            #   = original_forward(x)
            #   = module 原来的 forward 方法(x)
            #   = nn.Linear 的标准前向传播
            #   = x @ weight.T + bias
            #
            # Python 中，函数/方法都是"一等公民"，可以:
            # - 赋值给变量
            # - 作为参数传递
            # - 直接调用
            #
            # ============================================================
            
            # 修改 forward: 原始输出 + LoRA 输出
            def forward_with_lora(x, layer1=original_forward, layer2=lora):
                # layer1(x) = 原始 nn.Linear 的输出 = W @ x
                # layer2(x) = LoRA 的输出 = B @ A @ x
                # 最终输出 = W @ x + B @ A @ x
                return layer1(x) + layer2(x)

            # ============================================================
            # 覆盖原始 forward
            # ============================================================
            #
            # 现在 module.forward 指向新函数 forward_with_lora
            # 当其他代码调用 module(x) 时:
            #   1. Python 自动调用 module.forward(x)
            #   2. 执行 forward_with_lora(x)
            #   3. 返回 原始输出 + LoRA 输出
            #
            # 这就是"猴子补丁"(Monkey Patching)技术！
            # 在运行时动态修改对象的行为，无需改动原始代码。
            #
            # ============================================================
            
            module.forward = forward_with_lora


def load_lora(model, path):
    """
    加载 LoRA 权重
    
    【流程】
    1. 从文件加载状态字典
    2. 处理 DDP 前缀 (module.)
    3. 找到每个 LoRA 模块对应的权重
    4. 加载到模型中
    
    ========================================================================
    【核心疑问: state_dict 在哪里定义的？什么时候学习得到的？】
    ========================================================================
    
    【答案】state_dict 不是“定义”的，是训练过程中“学习”出来的！
    
    【完整流程】
    
    ════════════════════════════════════════════════════════════
    阶段1: 创建 LoRA 层 (apply_lora 函数)
    ════════════════════════════════════════════════════════════
    
    调用 apply_lora(model, rank=8) 时:
    
    lora = LoRA(512, 512, rank=8)  # 创建 LoRA 对象
    
    此时 LoRA 的参数被初始化:
    - self.A = nn.Parameter(torch.randn(8, 512) * 0.02)  # 随机初始化
    - self.B = nn.Parameter(torch.zeros(512, 8))          # 零初始化
    
    nn.Parameter 表示“这是可训练的参数”，会被优化器更新
    
    ════════════════════════════════════════════════════════════
    阶段2: 训练过程 (在 train_lora.py 等训练脚本中)
    ════════════════════════════════════════════════════════════
    
    训练循环的每一步:
    
    1. 前向传播:
       output = model(input)  # 内部会调用 forward_with_lora
                              # = W @ x + B @ A @ x
    
    2. 计算损失:
       loss = criterion(output, target)
    
    3. 反向传播:
       loss.backward()  # PyTorch 自动计算梯度
                        # ∂L/∂A 和 ∂L/∂B 被计算出来
                        # (原始 W 被冻结，不计算梯度)
    
    4. 更新参数:
       optimizer.step()  # A = A - lr * ∂L/∂A
                         # B = B - lr * ∂L/∂B
                         # 这就是“学习”的过程！
    
    经过成千上万步迭代，A 和 B 从随机值变成“有意义的值”
    
    ════════════════════════════════════════════════════════════
    阶段3: 保存 LoRA 权重 (save_lora 函数)
    ════════════════════════════════════════════════════════════
    
    训练完成后调用 save_lora(model, 'lora.pt'):
    
    - 遍历模型的所有 LoRA 模块
    - 收集它们的 A 和 B 矩阵
    - 保存成 state_dict 字典格式
    - 写入文件
    
    保存的文件内容 (示例):
    {
        'layers.0.self_attn.q_proj.lora.A': tensor([...]),  # [8, 512]
        'layers.0.self_attn.q_proj.lora.B': tensor([...]),  # [512, 8]
        'layers.0.self_attn.o_proj.lora.A': tensor([...]),
        'layers.0.self_attn.o_proj.lora.B': tensor([...]),
        ... # 每层的 q_proj 和 o_proj
    }
    
    ════════════════════════════════════════════════════════════
    阶段4: 加载 LoRA 权重 (本函数)
    ════════════════════════════════════════════════════════════
    
    推理时调用 load_lora(model, 'lora.pt'):
    
    1. torch.load('lora.pt') 读取文件，得到 state_dict
    2. 遍历模型找到每个 LoRA 模块
    3. 把对应的 A/B 矩阵加载进去
    
    这样模型就恢复到训练后的状态！
    
    【总结: state_dict 的生命周期】
    
    创建 (apply_lora)    →  随机初始化
           ↓
    训练 (train loop)    →  梯度下降不断更新 A/B
           ↓
    保存 (save_lora)     →  写入文件
           ↓
    加载 (load_lora)     →  从文件读取并恢复
    ========================================================================
    """
    state_dict = torch.load(path, map_location=model.device)
    # 移除可能的 DDP 前缀
    state_dict = {(k[7:] if k.startswith('module.') else k): v for k, v in state_dict.items()}

    for name, module in model.named_modules():
        if hasattr(module, 'lora'):
            # 提取该层的 LoRA 权重
            lora_state = {k.replace(f'{name}.lora.', ''): v for k, v in state_dict.items() if f'{name}.lora.' in k}
            module.lora.load_state_dict(lora_state)


def save_lora(model, path):
    """
    保存 LoRA 权重
    
    【注意】
    只保存 LoRA 参数，不保存原模型参数
    - 文件非常小 (通常几 MB)
    - 需要配合原模型使用
    """
    state_dict = {}
    for name, module in model.named_modules():
        if hasattr(module, 'lora'):
            # 处理 DDP 前缀
            clean_name = name[7:] if name.startswith("module.") else name
            # 收集该层的 LoRA 权重
            lora_state = {f'{clean_name}.lora.{k}': v for k, v in module.lora.state_dict().items()}
            state_dict.update(lora_state)
    torch.save(state_dict, path)
