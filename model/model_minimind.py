# 📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘
#                                             MiniMind Config
# 📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘
"""
【MiniMind 模型配置类】

这个文件定义了 MiniMind 语言模型的完整架构，包含以下核心组件:

1. MiniMindConfig: 模型配置类，定义所有超参数
2. RMSNorm: 均方根归一化层，比 LayerNorm 更高效
3. RoPE: 旋转位置编码，编码 token 的位置信息
4. Attention: 多头注意力机制，支持 GQA 和 Flash Attention
5. FeedForward: 前馈网络，使用 SwiGLU 激活函数
6. MoE: 混合专家系统，稀疏激活提升模型容量
7. MiniMindBlock: Transformer 基本块
8. MiniMindForCausalLM: 完整的因果语言模型

【学习路径建议】
1. 先理解 Config 类的各个参数含义
2. 学习 RMSNorm 和 RoPE 的数学原理
3. 深入理解 Attention 机制
4. 学习 SwiGLU 和 MoE
5. 最后理解完整的模型组装
"""

from transformers import PretrainedConfig


class MiniMindConfig(PretrainedConfig):
    """
    MiniMind 模型配置类
    
    【作用】存储模型的所有超参数，方便模型创建、保存和加载
    
    【关键参数解释】
    - hidden_size: 隐藏层维度，决定模型"宽度"(512/768)
    - num_hidden_layers: Transformer 层数，决定模型"深度"(8/16)
    - num_attention_heads: 注意力头数，多头注意力从不同角度理解输入
    - num_key_value_heads: GQA 中 KV 头数，通过共享 KV 减少计算量
    - vocab_size: 词表大小 (6400)
    - rope_theta: RoPE 基础频率，影响位置编码的波长
    - use_moe: 是否使用混合专家 (MoE) 架构
    """
    model_type = "minimind"

    def __init__(
            self,
            # ════════════════════════════════════════════════════════════════
            # ========== 基础参数 ==========
            # ════════════════════════════════════════════════════════════════
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │                    Dropout 详解                                │
            # └────────────────────────────────────────────────────────────────┘
            #
            # 【什么是 Dropout？】
            #
            # 训练时随机"关掉"一部分神经元，让模型不能依赖任何单个神经元
            #
            # 【高中生能懂的比喻】
            #
            # 想象一个班级做小组作业：
            # - 没有 Dropout: 每次都是学霸小明一个人做完，其他人划水
            # - 有 Dropout:   每次随机让一些人"请假"，逼迫所有人都学会
            #
            # 结果：考试时（推理时），每个人都有能力，团队更强！
            #
            # 【数学原理】
            #
            # dropout = 0.1 表示:
            # - 训练时: 每个神经元有 10% 概率被"关掉"（输出变成 0）
            # - 推理时: 所有神经元都工作，但输出要乘以 (1-0.1)=0.9 来平衡
            #
            # 【具体例子】
            #
            # 假设某层有 5 个神经元，输出是 [1.0, 2.0, 3.0, 4.0, 5.0]
            #
            # 训练时 (dropout=0.2):
            #   随机生成 mask: [1, 0, 1, 1, 0]  (0=关掉, 1=保留)
            #   输出 = [1.0, 0, 3.0, 4.0, 0] / 0.8  # 除以 (1-dropout) 补偿
            #        = [1.25, 0, 3.75, 5.0, 0]
            #
            # 推理时:
            #   输出 = [1.0, 2.0, 3.0, 4.0, 5.0]  # 全部保留，不变
            #
            # 【为什么默认 0.0？】
            #
            # 现代 LLM 通常不用 Dropout:
            # 1. 数据量足够大，过拟合风险低
            # 2. Dropout 会减慢收敛速度
            # 3. 有其他正则化手段（如权重衰减）
            #
            # 【什么时候用？】
            # - 小模型 + 小数据集: 设 0.1~0.3
            # - 大模型 + 大数据集: 保持 0.0
            #
            # ════════════════════════════════════════════════════════════════
            dropout: float = 0.0,              # Dropout 概率，训练时防止过拟合
            bos_token_id: int = 1,             # 序列开始 token 的 ID (Beginning Of Sequence)
            eos_token_id: int = 2,             # 序列结束 token 的 ID (End Of Sequence)
            hidden_act: str = 'silu',          # 激活函数，SiLU = x * sigmoid(x)，也叫 Swish
            hidden_size: int = 512,            # 隐藏层维度，决定模型容量
            intermediate_size: int = None,    # FFN 中间层维度，默认约 2.67 * hidden_size
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │              max_position_embeddings 详解                      │
            # └────────────────────────────────────────────────────────────────┘
            #
            # 【什么是最大序列长度？】
            #
            # 模型一次能处理的最多 token 数量
            #
            # 【高中生能懂的比喻】
            #
            # 想象你在看书：
            # - max_position_embeddings = 512  → 你一次只能记住 512 个字
            # - max_position_embeddings = 32768 → 你一次能记住一整本小说
            #
            # 超过这个长度的内容，模型就"看不到"了
            #
            # 【为什么有这个限制？】
            #
            # 1. 内存限制:
            #    注意力矩阵大小 = seq_len × seq_len
            #    - seq=512:   512×512 = 26万 个数字
            #    - seq=32768: 32768×32768 = 10亿 个数字！
            #
            # 2. 位置编码限制:
            #    RoPE 需要预计算每个位置的旋转角度
            #    超过预计算范围的位置，模型没见过
            #
            # 【实际应用】
            #
            # - GPT-3:     2048 tokens  (~1500 中文字)
            # - GPT-4:     8192/32768 tokens
            # - Claude:    100K+ tokens (能读整本书)
            # - MiniMind:  32768 tokens (配置值，实际受显存限制)
            #
            # 【注意】
            # 这是理论上限，实际能用多少取决于显存！
            # 8GB 显卡可能只能跑 2048 tokens
            #
            # ════════════════════════════════════════════════════════════════
            max_position_embeddings: int = 32768,  # 最大序列长度
            num_attention_heads: int = 8,      # 注意力头数量
            num_hidden_layers: int = 8,        # Transformer 层数
            num_key_value_heads: int = 2,      # GQA 中 KV 头数量 (小于 Q 头数可节省内存)
            vocab_size: int = 6400,            # 词表大小
            rms_norm_eps: float = 1e-05,       # RMSNorm 的 epsilon，防止除零
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │                    RoPE 和 rope_theta 详解                     │
            # └────────────────────────────────────────────────────────────────┘
            #
            # 【什么是 RoPE？】
            #
            # RoPE = Rotary Position Embedding = 旋转位置编码
            # 让模型知道每个 token 在句子中的位置
            #
            # 【高中生能懂的比喻】
            #
            # 想象一个时钟：
            # - 第1个字 → 时针指向 1 点钟方向
            # - 第2个字 → 时针指向 2 点钟方向
            # - 第3个字 → 时针指向 3 点钟方向
            # ...
            #
            # 通过"旋转角度"来表示位置！
            #
            # 【数学原理 (简化版)】
            #
            # 普通向量: x = [a, b]
            # 旋转 θ 度后: x' = [a×cos(θ) - b×sin(θ), a×sin(θ) + b×cos(θ)]
            #
            # 第 n 个位置的旋转角度: θ_n = n × base_angle
            #
            # 【rope_theta 的作用】
            #
            # rope_theta 决定"基础角度"有多小：
            #
            #   base_angle = 1 / (rope_theta ^ (2i/d))
            #
            # 其中 i 是维度索引，d 是总维度
            #
            # 【theta 值的影响】
            #
            # ┌──────────────┬─────────────────────────────────────────────┐
            # │ rope_theta   │ 效果                                        │
            # ├──────────────┼─────────────────────────────────────────────┤
            # │ 10000 (小)   │ 旋转快，适合短序列 (512-2048)               │
            # │ 1000000 (大) │ 旋转慢，适合长序列 (32768+)                 │
            # └──────────────┴─────────────────────────────────────────────┘
            #
            # 【直观理解】
            #
            # theta 小 → 每个位置旋转角度大 → 位置信息变化快 → 容易"转过头"
            # theta 大 → 每个位置旋转角度小 → 位置信息变化慢 → 能区分更远的位置
            #
            # ════════════════════════════════════════════════════════════════════
            # 【传统位置编码是什么样的？】
            # ════════════════════════════════════════════════════════════════════
            #
            # ┌──────────────────────────────────────────────────────────────────┐
            # │ 方法一: 可学习的绝对位置编码 (GPT-2, BERT)                       │
            # └──────────────────────────────────────────────────────────────────┘
            #
            # 【原理】
            # 给每个位置一个可学习的向量，直接加到 token 向量上
            #
            # 【代码】
            #   position_embeddings = nn.Embedding(max_seq_len, hidden_size)
            #   # position_embeddings.weight: [512, 768] (假设最大512位置)
            #
            #   x = token_embeddings + position_embeddings(positions)
            #   # 位置0的向量 + 位置1的向量 + ...
            #
            # 【具体例子】
            #
            #   输入 "我爱中国" (4个token)
            #
            #   token_embed("我") = [0.1, 0.2, 0.3, ...]  (768维)
            #   position_embed(0) = [0.01, 0.02, 0.01, ...] (位置0的可学习向量)
            #
            #   最终 "我" 的向量 = [0.11, 0.22, 0.31, ...]  (直接相加)
            #
            # 【问题】
            # 1. 位置 513 怎么办？没见过的位置 → 没有对应向量 → 报错！
            # 2. 无法外推: 训练时最大512，推理时不能用513
            # 3. 绝对位置: 位置10和位置11的关系，跟位置100和101不一样
            #
            # ┌──────────────────────────────────────────────────────────────────┐
            # │ 方法二: 正弦位置编码 (原版 Transformer)                          │
            # └──────────────────────────────────────────────────────────────────┘
            #
            # 【原理】
            # 用 sin/cos 函数生成位置向量，不需要学习
            #
            # 【公式】
            #   PE(pos, 2i)   = sin(pos / 10000^(2i/d))
            #   PE(pos, 2i+1) = cos(pos / 10000^(2i/d))
            #
            # 【具体例子】
            #
            #   位置 5，维度 0: sin(5 / 10000^0) = sin(5) = -0.96
            #   位置 5，维度 1: cos(5 / 10000^0) = cos(5) = 0.28
            #   位置 5，维度 2: sin(5 / 10000^0.03) = sin(4.7) = -0.99
            #   ...
            #
            #   PE(5) = [-0.96, 0.28, -0.99, 0.05, ...]
            #
            # 【使用方式】也是直接加到 token 向量上
            #   x = token_embeddings + positional_encoding
            #
            # 【优点】
            # - 不需要学习，公式生成
            # - 理论上可以外推（但实际效果不好）
            #
            # 【问题】
            # 1. 还是"加法"，位置信息容易被后续层冲淡
            # 2. 外推效果差，长序列性能下降严重
            #
            # ┌──────────────────────────────────────────────────────────────────┐
            # │ 方法三: RoPE (现代 LLM 标配)                                     │
            # └──────────────────────────────────────────────────────────────────┘
            #
            # 【关键区别】
            #
            # ┌────────────────┬─────────────────┬─────────────────┬─────────────┐
            # │                │ 绝对位置编码    │ 正弦位置编码    │ RoPE        │
            # ├────────────────┼─────────────────┼─────────────────┼─────────────┤
            # │ 作用位置       │ Embedding后     │ Embedding后     │ Attention中 │
            # │ 作用方式       │ 加法            │ 加法            │ 乘法(旋转)  │
            # │ 是否可学习     │ 是              │ 否              │ 否          │
            # │ 外推能力       │ ✗ 不能          │ △ 很差          │ ✓ 较好      │
            # │ 相对位置       │ ✗ 只有绝对      │ △ 隐式          │ ✓ 显式      │
            # │ 计算位置       │ 只在输入层      │ 只在输入层      │ 每层Attn    │
            # └────────────────┴─────────────────┴─────────────────┴─────────────┘
            #
            # 【RoPE 的核心优势】
            #
            # 1. 乘法而非加法 → 位置信息不会被冲淡
            # 2. 每层都旋转 → 位置信息贯穿整个网络
            # 3. 相对位置 → Q_m·K_n 只跟 (m-n) 有关
            # 4. 外推能力 → 配合 YaRN 可以处理超长序列
            #
            # ════════════════════════════════════════════════════════════════════
            # 【绝对位置 vs 相对位置 - 具体例子】
            # ════════════════════════════════════════════════════════════════════
            #
            # 【场景】理解"昨天"这个词的含义
            #
            # 句子A: "我 昨天 吃了 火锅"     (昨天在位置1)
            # 句子B: "上周我去北京，昨天 回来了" (昨天在位置5)
            #
            # ┌──────────────────────────────────────────────────────────────────┐
            # │ 绝对位置编码的问题                                               │
            # └──────────────────────────────────────────────────────────────────┘
            #
            # 绝对位置编码给"昨天"的向量:
            #
            #   句子A: embed("昨天") + position_embed(1)  = 向量A
            #   句子B: embed("昨天") + position_embed(5)  = 向量B
            #
            # 问题: 向量A ≠ 向量B！
            #
            # 同一个词"昨天"，因为位置不同，得到完全不同的向量！
            # 模型必须分别学习"位置1的昨天"和"位置5的昨天"是一样的意思
            # → 学习效率低，泛化能力差
            #
            # ┌──────────────────────────────────────────────────────────────────┐
            # │ 相对位置编码的优势                                               │
            # └──────────────────────────────────────────────────────────────────┘
            #
            # 相对位置关心的是: "昨天"和周围词的距离
            #
            # 句子A: "我 昨天 吃了"
            #         ↑   ↑   ↑
            #        -1   0  +1  (相对"昨天"的距离)
            #
            # 句子B: "北京，昨天 回来"
            #          ↑    ↑    ↑
            #         -1    0   +1  (相对"昨天"的距离)
            #
            # 【关键】两个句子中，"昨天"和前后词的相对距离是一样的！
            # → 模型只需要学习"距离1的词"和"距离2的词"的关系
            # → 不管这个词在句子的什么位置，学到的关系都能复用
            #
            # ════════════════════════════════════════════════════════════════════
            # 【相对位置是相对于谁？训练时怎么算？】
            # ════════════════════════════════════════════════════════════════════
            #
            # 【核心理解】相对位置不是"相对于当前token"，而是"任意两个token之间的距离"
            #
            # 【训练时的实际情况】
            #
            # 输入: "我 爱 中 国"  (位置 0, 1, 2, 3)
            #
            # Attention 是 全部两两计算！不是只算"当前token"
            #
            #   ┌─────────────────────────────────────────────────────────────┐
            #   │                  Attention 矩阵 (4×4)                       │
            #   │                                                             │
            #   │         K_我(0)   K_爱(1)   K_中(2)   K_国(3)                │
            #   │  Q_我(0)  0-0=0    0-1=-1   0-2=-2   0-3=-3   ← 位置差      │
            #   │  Q_爱(1)  1-0=+1   1-1=0    1-2=-1   1-3=-2                  │
            #   │  Q_中(2)  2-0=+2   2-1=+1   2-2=0    2-3=-1                  │
            #   │  Q_国(3)  3-0=+3   3-1=+2   3-2=+1   3-3=0                   │
            #   └─────────────────────────────────────────────────────────────┘
            #
            # 【每个格子】都是一对 Q-K 的注意力分数
            # 【每个格子】的位置差 = Q的位置 - K的位置
            #
            # 【RoPE 做的事】
            #
            #   score[i,j] = Q_i · K_j = |Q||K| × cos(角度_i - 角度_j)
            #                                    ↑
            #                              只跟 (i-j) 有关！
            #
            # 【训练时学到了什么？】
            #
            # 模型一次看到整个句子，同时学习:
            # - 位置差 +1 时（后一个词看前一个词）的关系模式
            # - 位置差 +2 时（隔一个词）的关系模式
            # - 位置差 -1 时（前一个词看后一个词）的关系模式
            # ...
            #
            # 【因果注意力 (Causal Attention)】
            #
            # 实际上，LLM 用的是因果注意力，只能看前面的词:
            #
            #   ┌─────────────────────────────────────────────────────────────┐
            #   │         K_我(0)   K_爱(1)   K_中(2)   K_国(3)                │
            #   │  Q_我(0)  ✓        ✗        ✗        ✗                      │
            #   │  Q_爱(1)  ✓        ✓        ✗        ✗                      │
            #   │  Q_中(2)  ✓        ✓        ✓        ✗                      │
            #   │  Q_国(3)  ✓        ✓        ✓        ✓   ← 只有下三角有值   │
            #   └─────────────────────────────────────────────────────────────┘
            #
            # 【所以】训练时:
            # - "国" 学习关注 "我/爱/中" (位置差 +3/+2/+1)
            # - "中" 学习关注 "我/爱" (位置差 +2/+1)
            # - "爱" 学习关注 "我" (位置差 +1)
            #
            # 每一行都在学习不同的位置差关系！
            #
            # 【推理时】
            #
            # 假设已生成 "我爱中"，现在要预测下一个词:
            #
            #   只算最后一行: Q_?(3) 对 K_我(0), K_爱(1), K_中(2) 的注意力
            #   位置差分别是: +3, +2, +1
            #
            # 这些位置差模式，训练时已经见过无数次了！
            #
            # 【总结】
            # - 相对位置 = 任意两个位置的差，不是"相对于某个特定token"
            # - 训练时整个句子同时计算，每对词都学到位置差关系
            # - 推理时用 KV Cache，只算新 token 对所有旧 token 的注意力
            #
            # ┌──────────────────────────────────────────────────────────────────┐
            # │ 更具体的数值例子                                                 │
            # └──────────────────────────────────────────────────────────────────┘
            #
            # 【绝对位置 - 注意力计算】
            #
            # 计算"吃了"对"昨天"的注意力:
            #
            #   Q_吃了 = embed(吃了) + pos(2)   # 位置2的"吃了"
            #   K_昨天 = embed(昨天) + pos(1)   # 位置1的"昨天"
            #
            #   attention = Q_吃了 · K_昨天
            #             = (embed_吃了 + pos_2) · (embed_昨天 + pos_1)
            #             = embed_吃了·embed_昨天 + embed_吃了·pos_1 
            #               + pos_2·embed_昨天 + pos_2·pos_1
            #                                     ↑
            #                           这项只跟绝对位置 (2,1) 有关！
            #
            # 如果"昨天"在位置100，"吃了"在位置101:
            #   attention 包含 pos_101·pos_100
            #   这是完全不同的值！模型要重新学
            #
            # ┌──────────────────────────────────────────────────────────────────┐
            # │ RoPE 相对位置 - 注意力计算                                       │
            # └──────────────────────────────────────────────────────────────────┘
            #
            # 计算"吃了"对"昨天"的注意力:
            #
            #   Q_吃了 = rotate(embed(吃了), 角度2)   # 旋转位置2的角度
            #   K_昨天 = rotate(embed(昨天), 角度1)   # 旋转位置1的角度
            #
            #   attention = Q_吃了 · K_昨天
            #             = |Q||K| × cos(角度2 - 角度1)
            #             = |Q||K| × cos(1个位置的角度差)
            #                         ↑
            #                    只跟位置差 (2-1=1) 有关！
            #
            # 如果"昨天"在位置100，"吃了"在位置101:
            #   attention = |Q||K| × cos(101 - 100) = 同样是1个位置的角度差
            #   完全一样！不需要重新学！
            #
            # ┌──────────────────────────────────────────────────────────────────┐
            # │ 总结：绝对 vs 相对                                               │
            # └──────────────────────────────────────────────────────────────────┘
            #
            # 【绝对位置】
            # - "位置1的昨天" 和 "位置100的昨天" 是不同的东西
            # - 模型需要分别学习每个位置的每个词
            # - 没见过的位置 → 不会处理
            #
            # 【相对位置】
            # - "昨天的后一个词" 不管在哪，关系都一样
            # - 模型只需要学习"距离关系"
            # - 距离1就是距离1，不管在句首还是句尾
            # - 外推自然成立：距离1000也是距离1000
            #
            # ════════════════════════════════════════════════════════════════════
            #
            # 【为什么 RoPE 比传统位置编码好？】(总结)
            #
            # 1. 相对位置: 两个 token 的位置差 = 旋转角度差（不需要绝对位置）
            # 2. 外推能力: 训练时用短序列，推理时能用更长序列
            # 3. 计算高效: 只是乘法运算，没有额外参数
            #
            # ════════════════════════════════════════════════════════════════
            #
            # ════════════════════════════════════════════════════════════════════
            # 【RoPE 在 LLM 的哪一步生效？】
            # ════════════════════════════════════════════════════════════════════
            #
            # RoPE 只作用于 Attention 层的 Q 和 K，在计算注意力分数之前！
            #
            # 【完整流程】
            #
            #   输入 x: [batch, seq, 512]
            #        ↓
            #   Q = x @ W_q  →  [batch, seq, 512]   ← 普通的线性变换
            #   K = x @ W_k  →  [batch, seq, 128]
            #        ↓
            #   ┌─────────────────────────────────────┐
            #   │  Q_rotated = RoPE(Q, positions)     │  ← RoPE 在这里！
            #   │  K_rotated = RoPE(K, positions)     │
            #   └─────────────────────────────────────┘
            #        ↓
            #   scores = Q_rotated @ K_rotated.T     ← 用旋转后的Q/K算注意力
            #        ↓
            #   output = softmax(scores) @ V
            #
            # 【注意】V 不需要 RoPE！因为 V 是"内容"，不需要位置信息
            #
            # ════════════════════════════════════════════════════════════════════
            # 【positions 是哪里来的？包含什么数据？】
            # ════════════════════════════════════════════════════════════════════
            #
            # 【来源】模型初始化时预计算，存在 buffer 里
            #
            #   # 在 MiniMind.__init__ 中:
            #   freqs_cos, freqs_sin = precompute_freqs_cis(dim=64, end=32768)
            #   self.register_buffer("freqs_cos", freqs_cos)  # [32768, 64]
            #   self.register_buffer("freqs_sin", freqs_sin)  # [32768, 64]
            #
            # 【数据内容】一个预计算的 cos/sin 表格
            #
            #   freqs_cos[pos, dim] = cos(pos × freq_dim)
            #   freqs_sin[pos, dim] = sin(pos × freq_dim)
            #
            # 【具体例子】
            #
            #   freqs_cos[0, :] = [1.0, 1.0, 1.0, ...]  # 位置0，角度都是0，cos(0)=1
            #   freqs_cos[1, :] = [0.54, 0.99, 0.99, ...]  # 位置1
            #   freqs_cos[5, :] = [0.28, 0.96, 0.99, ...]  # 位置5
            #   ...
            #   freqs_cos[32767, :] = [...]  # 最大位置
            #
            # 【使用时】根据当前序列长度切片
            #
            #   # 输入 "我爱中国" (seq_len=4)
            #   cos = self.freqs_cos[:4]  # 只取前4行: 位置 0,1,2,3
            #   sin = self.freqs_sin[:4]
            #
            # ════════════════════════════════════════════════════════════════════
            # 【RoPE 函数做了什么？】
            # ════════════════════════════════════════════════════════════════════
            #
            # 【核心代码】apply_rotary_pos_emb 函数
            #
            #   def rotate_half(x):
            #       # [a,b,c,d] → [-c,-d,a,b]
            #       return torch.cat((-x[..., d//2:], x[..., :d//2]), dim=-1)
            #
            #   q_rotated = q * cos + rotate_half(q) * sin
            #   k_rotated = k * cos + rotate_half(k) * sin
            #
            # 【数值例子】
            #
            # 假设 q = [1, 2, 3, 4] (4维向量)，位置5，cos=[0.28, 0.28, 0.96, 0.96]，sin=[-0.96, -0.96, 0.28, 0.28]
            #
            #   rotate_half(q) = [-3, -4, 1, 2]
            #
            #   q_rotated = [1, 2, 3, 4] × [0.28, 0.28, 0.96, 0.96]
            #             + [-3, -4, 1, 2] × [-0.96, -0.96, 0.28, 0.28]
            #             = [0.28, 0.56, 2.88, 3.84] + [2.88, 3.84, 0.28, 0.56]
            #             = [3.16, 4.40, 3.16, 4.40]
            #
            # 【本质】把向量在复数平面上旋转一个角度！
            #
            # ════════════════════════════════════════════════════════════════════
            # 【多头注意力中，RoPE 用 512 还是 64？】
            # ════════════════════════════════════════════════════════════════════
            #
            # 【答案】用 head_dim = 64，不是 hidden_size = 512！
            #
            # 【原因】RoPE 在"拆分成多头之后"应用，每个头独立旋转
            #
            # 【完整流程】
            #
            #   输入 x: [batch, seq, 512]
            #        ↓
            #   Q = x @ W_q  →  [batch, seq, 512]    (还是512维)
            #        ↓
            #   Q = Q.view(batch, seq, 8, 64)        (拆成8个头，每头64维)
            #        ↓
            #   ┌────────────────────────────────────────────────────────────┐
            #   │  对每个头的 64 维向量应用 RoPE:                            │
            #   │                                                            │
            #   │  头0: Q[:,:,0,:] (64维) → rotate(Q[:,:,0,:], cos, sin)     │
            #   │  头1: Q[:,:,1,:] (64维) → rotate(Q[:,:,1,:], cos, sin)     │
            #   │  ...                                                       │
            #   │  头7: Q[:,:,7,:] (64维) → rotate(Q[:,:,7,:], cos, sin)     │
            #   │                                                            │
            #   │  cos, sin 的维度是 [seq, 64]，不是 [seq, 512]！            │
            #   └────────────────────────────────────────────────────────────┘
            #        ↓
            #   Q @ K.T → Attention
            #
            # 【代码证据】
            #
            #   # precompute_freqs_cis 的参数:
            #   dim = config.hidden_size // config.num_attention_heads
            #       = 512 // 8
            #       = 64   ← 用的是 head_dim！
            #
            #   freqs_cos, freqs_sin = precompute_freqs_cis(dim=64, ...)
            #
            # 【为什么用 64 不用 512？】
            #
            # 1. 多头注意力的计算是在每个头内部进行的
            #    - 每个头只看自己的 64 维，不看其他头
            #    - RoPE 也只需要旋转这 64 维
            #
            # 2. 如果用 512 维:
            #    - cos/sin 表会是 [max_pos, 512]，太大了
            #    - 而且拆成多头后也用不上
            #
            # 3. 每个头的 RoPE 是相同的
            #    - 8 个头用同一套 cos/sin
            #    - 但各自独立旋转自己的 64 维向量
            #
            # 【具体例子】
            #
            # 位置 5 的 Q 向量:
            #
            #   Q_original: [batch, 1, 512]
            #        ↓ view
            #   Q_heads: [batch, 1, 8, 64]
            #        ↓ RoPE (用 64 维的 cos/sin)
            #   Q_rotated: [batch, 1, 8, 64]
            #
            #   头0的64维: [q0, q1, ..., q63] → 用 cos[5], sin[5] 旋转
            #   头1的64维: [q0, q1, ..., q63] → 用同样的 cos[5], sin[5] 旋转
            #   ...
            #
            # 【总结】
            # - RoPE 维度 = head_dim = 64
            # - 每个头独立但使用相同的旋转角度
            # - 位置信息在每个头内部都有体现
            #
            # ════════════════════════════════════════════════════════════════════
            # 【为什么不同 token 旋转角度不同有意义？】
            # ════════════════════════════════════════════════════════════════════
            #
            # 【关键洞察】旋转角度的"差"决定了注意力分数！
            #
            # 假设句子 "我 爱 中国" (位置 0, 1, 2)
            #
            #   Q_爱 = rotate(embed(爱), 角度1)
            #   K_我 = rotate(embed(我), 角度0)
            #   K_中国 = rotate(embed(中国), 角度2)
            #
            # 计算"爱"对"我"的注意力:
            #   score = Q_爱 · K_我 ∝ cos(角度1 - 角度0) = cos(1个位置差)
            #
            # 计算"爱"对"中国"的注意力:
            #   score = Q_爱 · K_中国 ∝ cos(角度1 - 角度2) = cos(-1个位置差)
            #
            # 【意义】
            # - 不同 token 旋转不同角度 → 角度差 = 位置差
            # - 注意力分数只跟"位置差"有关，不管绝对位置
            # - 前一个词和后一个词的区别，靠角度差的正负！
            #
            # ════════════════════════════════════════════════════════════════════
            # 【核心问题：词与词的关系存储在哪里？】
            # ════════════════════════════════════════════════════════════════════
            #
            # 这是最重要的问题！答案是：
            #
            # ┌──────────────────────────────────────────────────────────────────┐
            # │ 词与词的关系 存储在 W_q, W_k, W_v, W_o 这 4 个权重矩阵里！       │
            # └──────────────────────────────────────────────────────────────────┘
            #
            # 【详细解释】
            #
            # 1. RoPE 本身没有可学习参数！它只是一个确定的数学变换
            #    - freqs_cos, freqs_sin 是固定的数学表格，不参与训练
            #
            # 2. 词与词的关系是这样学习的:
            #
            #    Q = x @ W_q   # W_q 学习"我想找什么类型的词"
            #    K = x @ W_k   # W_k 学习"我是什么类型的词"
            #    V = x @ W_v   # W_v 学习"我能提供什么信息"
            #
            #    score = Q @ K.T  # Q和K的匹配度 = 词与词的关系强度
            #    output = score @ V  # 根据关系强度加权取信息
            #
            # 3. 【具体例子】"猫 吃 鱼"
            #
            #    训练时，模型看到很多 "X 吃 Y" 的句子:
            #    - "猫吃鱼"、"狗吃肉"、"人吃饭"...
            #
            #    W_q, W_k 会学习到:
            #    - Q("吃") 和 K("猫"/"狗"/"人") 的匹配度高 (主语)
            #    - Q("吃") 和 K("鱼"/"肉"/"饭") 的匹配度高 (宾语)
            #    - Q("吃") 和 K("桌子"/"天空") 的匹配度低 (无关)
            #
            #    这些关系全部编码在 W_q 和 W_k 的参数里！
            #
            # ┌──────────────────────────────────────────────────────────────────┐
            # │ 参数存储位置总结                                                 │
            # └──────────────────────────────────────────────────────────────────┘
            #
            # ┌────────────────────┬────────────────────────────────────────────┐
            # │ 参数               │ 存储的信息                                 │
            # ├────────────────────┼────────────────────────────────────────────┤
            # │ token_embedding    │ 每个词的"身份证"（语义向量）               │
            # │ W_q (q_proj)       │ "这个词想找什么类型的词配合"               │
            # │ W_k (k_proj)       │ "这个词是什么类型，能被谁找到"             │
            # │ W_v (v_proj)       │ "这个词能提供什么信息给别人"               │
            # │ W_o (o_proj)       │ "如何整合从别的词拿到的信息"               │
            # │ FFN (gate/up/down) │ "拿到信息后如何深度加工"                   │
            # │ RoPE (cos/sin)     │ 位置信息 (固定，不学习)                    │
            # └────────────────────┴────────────────────────────────────────────┘
            #
            # 【整句话训练时发生了什么？】
            #
            # 输入: "明天 天气 很 好"
            #
            # 1. 每个词都会"看"其他所有词 (或之前的词)
            #    - "天气" 看 "明天" → 学到"天气"常和时间词搭配
            #    - "好" 看 "天气" → 学到"好"常修饰"天气"
            #    - "很" 看 "好" → 学到程度副词搭配
            #
            # 2. 通过大量句子，W_q, W_k, W_v 学到通用的词关系模式
            #
            # 3. 这些模式存储在权重矩阵的数值里！
            #    - W_q: [512, 512] 共 262,144 个参数
            #    - W_k: [512, 128] 共 65,536 个参数
            #    - 这些数字编码了"什么词应该关注什么词"
            #
            # ════════════════════════════════════════════════════════════════════
            # 【每个 token 的角度怎么算？具体数值！】
            # ════════════════════════════════════════════════════════════════════
            #
            # 假设: head_dim=64, rope_theta=10000
            #
            # 【Step 1: 计算每个维度的"基础频率"】
            #
            # 维度 i 的频率: freq_i = 1 / (theta ^ (2i / head_dim))
            #
            # 具体计算 (head_dim=64, theta=10000):
            #
            #   维度 0,1:  freq = 1 / (10000 ^ (0/64))  = 1 / 1     = 1.0
            #   维度 2,3:  freq = 1 / (10000 ^ (2/64))  = 1 / 1.19  = 0.84
            #   维度 4,5:  freq = 1 / (10000 ^ (4/64))  = 1 / 1.41  = 0.71
            #   ...
            #   维度 62,63: freq = 1 / (10000 ^ (62/64)) = 1 / 7499 = 0.00013
            #
            # 【规律】低维度频率高（变化快），高维度频率低（变化慢）
            #
            # 【Step 2: 计算每个位置的旋转角度】
            #
            # 位置 pos 在维度 i 的角度: angle = pos × freq_i
            #
            # 例如位置 5 的各维度角度:
            #
            #   维度 0,1:  angle = 5 × 1.0    = 5.0 弧度 ≈ 286°
            #   维度 2,3:  angle = 5 × 0.84   = 4.2 弧度 ≈ 241°
            #   维度 4,5:  angle = 5 × 0.71   = 3.55 弧度 ≈ 203°
            #   ...
            #   维度 62,63: angle = 5 × 0.00013 = 0.00065 弧度 ≈ 0.04°
            #
            # 【Step 3: 对 Q/K 向量进行旋转】
            #
            # 把 64 维向量看成 32 对 (a, b)，每对旋转对应角度:
            #
            #   原始 Q[0:2] = [a, b]
            #   旋转后 = [a×cos(θ) - b×sin(θ), a×sin(θ) + b×cos(θ)]
            #
            # 【具体数值例子】
            #
            # 假设 Q 的前两维是 [1.0, 0.5]，位置 5，维度 0-1 的角度是 5.0 弧度
            #
            #   cos(5.0) = 0.284
            #   sin(5.0) = -0.959
            #
            #   旋转后 = [1.0 × 0.284 - 0.5 × (-0.959), 1.0 × (-0.959) + 0.5 × 0.284]
            #          = [0.284 + 0.48, -0.959 + 0.142]
            #          = [0.764, -0.817]
            #
            # ════════════════════════════════════════════════════════════════════
            # 【为什么这样能编码位置？】
            # ════════════════════════════════════════════════════════════════════
            #
            # 【关键洞察】两个 token 的注意力分数只跟"位置差"有关！
            #
            # 数学证明 (简化版):
            #
            #   Q_m = 原始Q 旋转 m×θ
            #   K_n = 原始K 旋转 n×θ
            #
            #   Q_m · K_n = |Q||K| × cos(m×θ - n×θ)
            #                        ↑
            #                只跟 (m-n) 有关！
            #
            # 【实际意义】
            #
            # - 位置 10 和位置 12 的注意力
            # - 位置 100 和位置 102 的注意力
            #
            # 都只跟"差 2 个位置"有关，不管绝对位置是多少！
            #
            # ════════════════════════════════════════════════════════════════════
            # 【theta=1000000 vs theta=10000 的实际影响】
            # ════════════════════════════════════════════════════════════════════
            #
            # theta=10000 时，位置 2048 的角度:
            #   维度 0,1: 2048 × 1.0 = 2048 弧度 = 转了 326 圈！
            #   → 位置 2049 和位置 1 几乎同一角度 → 模型分不清！
            #
            # theta=1000000 时，位置 2048 的角度:
            #   维度 0,1: 2048 × 1.0 = 2048 弧度 (不变，这个维度和theta无关)
            #   但高维度: 2048 × 0.000001 = 0.002 弧度
            #   → 高维度能区分非常远的位置
            #
            # 【结论】theta 越大，高维度旋转越慢，能编码更长的序列
            #
            # ════════════════════════════════════════════════════════════════════
            rope_theta: int = 1000000.0,       # RoPE 基础频率，越大支持越长序列
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │              inference_rope_scaling 详解                       │
            # └────────────────────────────────────────────────────────────────┘
            #
            # 【什么是 RoPE 外推/Scaling？】
            #
            # 让模型处理比训练时更长的序列
            #
            # 【问题背景】
            #
            # 假设模型训练时只见过 2048 tokens 的文本：
            # - 推理时输入 4096 tokens → 后半部分的位置编码"没见过"！
            # - 模型可能会胡言乱语
            #
            # 【高中生能懂的比喻】
            #
            # 就像你只学过 1-100 的数字：
            # - 有人问你 150 是多少 → 你懵了
            # - 外推技巧: 把 150 "压缩"成 75，你就能处理了！
            #
            # 【Scaling 的原理】
            #
            # 原来:    位置 n → 旋转 n × base_angle
            # Scaling: 位置 n → 旋转 (n / factor) × base_angle
            #
            # factor = 16 时:
            #   位置 32768 → 实际当作位置 2048 来处理
            #
            # 【YaRN 方法 (本模型使用)】
            #
            # 不是简单地除以 factor，而是:
            # - 高频维度: 保持原样 (保留细节)
            # - 低频维度: 压缩 (处理长距离)
            #
            # 配置参数:
            #   "factor": 16        → 最大压缩 16 倍
            #   "original_max_position_embeddings": 2048  → 原始训练长度
            #   "beta_fast": 32     → 高频阈值
            #   "beta_slow": 1      → 低频阈值
            #
            # 【效果】
            #
            # 训练长度 2048 × factor 16 = 理论支持 32768 tokens！
            #
            # 【什么时候开启？】
            #
            # - 训练时: False (用原始位置编码)
            # - 推理时需要长上下文: True
            #
            # ════════════════════════════════════════════════════════════════
            inference_rope_scaling: bool = False,  # 是否启用 RoPE 外推 (扩展上下文长度)
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │                   Flash Attention 详解                         │
            # └────────────────────────────────────────────────────────────────┘
            #
            # 【什么是 Flash Attention？】
            #
            # 一种更快、更省内存的注意力计算方法
            # 数学结果完全相同，只是计算方式更聪明
            #
            # 【高中生能懂的比喻】
            #
            # 普通注意力 vs Flash Attention，就像:
            #
            # 【普通方法】做一道大菜
            #   1. 把所有食材都摆到超大桌子上 (占用大量内存)
            #   2. 一次性处理所有食材
            #   3. 桌子放不下就做不了 (OOM 内存溢出)
            #
            # 【Flash Attention】
            #   1. 一次只取一小部分食材 (省内存)
            #   2. 处理完放回去，再取下一批
            #   3. 最后结果完全一样，但桌子可以很小
            #
            # 【技术原理】
            #
            # 普通注意力:
            #   1. 计算完整的 Q @ K.T 矩阵 [seq, seq] → 占用 O(n²) 内存
            #   2. 对整个矩阵做 softmax
            #   3. 乘以 V
            #
            # Flash Attention:
            #   1. 把 Q, K, V 分成小块 (tiles)
            #   2. 每次只计算一小块的注意力
            #   3. 用数学技巧合并各块结果
            #   4. 内存只需要 O(n) 而不是 O(n²)！
            #
            # 【性能对比】
            #
            # ┌──────────────┬────────────┬────────────┬────────────┐
            # │ 序列长度      │ 普通内存    │ Flash 内存  │ 速度提升    │
            # ├──────────────┼────────────┼────────────┼────────────┤
            # │ 512          │ 1MB        │ 0.5MB      │ 1.5x       │
            # │ 2048         │ 16MB       │ 2MB       │ 2x         │
            # │ 8192         │ 256MB      │ 8MB        │ 3-4x       │
            # └──────────────┴────────────┴────────────┴────────────┘
            #
            # 【为什么默认开启？】
            #
            # 优点太明显:
            # 1. 省内存 → 能跑更长的序列
            # 2. 更快 → 训练和推理都加速
            # 3. 结果完全一样 → 没有精度损失
            #
            # 【什么时候关闭？】
            #
            # - 调试时想看中间的注意力矩阵
            # - 硬件不支持 (需要 CUDA + 特定 GPU)
            #
            # ════════════════════════════════════════════════════════════════
            flash_attn: bool = True,           # 是否使用 Flash Attention (更快更省内存)
            ####################################################
            # MoE (混合专家) 相关配置
            # 当 use_moe=False 时，以下参数无效
            ####################################################
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │                    MoE 混合专家 详解                           │
            # └────────────────────────────────────────────────────────────────┘
            #
            # 【什么是 MoE？】
            #
            # MoE = Mixture of Experts = 混合专家
            # 用多个"专家"网络处理不同类型的输入，每次只激活部分专家
            #
            # 【高中生能懂的比喻】
            #
            # 想象一个医院：
            #
            # 【普通模型】= 一个全科医生
            #   - 所有病人都找这一个医生
            #   - 医生必须什么都会 → 需要很多年培训 (参数多)
            #   - 每个病人都要占用这个医生全部时间 (计算量大)
            #
            # 【MoE 模型】= 专科医院
            #   - 有 4 个专科医生 (n_routed_experts=4)
            #   - 每个病人先去导诊台 (Router/门控)
            #   - 导诊台根据症状分配 2 个最合适的专家 (num_experts_per_tok=2)
            #   - 还有 1 个值班医生所有人都要见 (n_shared_experts=1)
            #
            # 结果：
            #   - 医院总能力 = 4 个专家 (参数多，能力强)
            #   - 每个病人只用 2+1=3 个医生的时间 (计算量小)
            #
            # 【MoE 的核心优势】
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │  参数量 ↑↑↑  但  计算量 ≈ 不变                                 │
            # └────────────────────────────────────────────────────────────────┘
            #
            # 例如：
            # - 普通模型: 1 个 FFN，参数 100M，计算 100M
            # - MoE 模型: 4 个 FFN，参数 400M，但每次只用 2 个 = 计算 200M
            #
            # 【具体流程】
            #
            # 输入: token 向量 x [512维]
            #
            # Step 1: Router (门控) 打分
            #   scores = x @ W_gate  # [512] @ [512, 4] = [4]
            #   scores = [0.1, 0.5, 0.3, 0.1]  (4个专家的分数)
            #
            # Step 2: 选 Top-K 个专家
            #   top_k = 2
            #   选中: 专家1 (0.5), 专家2 (0.3)
            #
            # Step 3: 归一化权重 (norm_topk_prob=True)
            #   weights = softmax([0.5, 0.3]) = [0.625, 0.375]
            #
            # Step 4: 各专家计算
            #   out_1 = Expert_1(x)  # 专家1 处理
            #   out_2 = Expert_2(x)  # 专家2 处理
            #
            # Step 5: 加权合并
            #   output = 0.625 × out_1 + 0.375 × out_2
            #
            # Step 6: 加上共享专家 (如果有)
            #   output += Shared_Expert(x)
            #
            # ════════════════════════════════════════════════════════════════
            #
            # ════════════════════════════════════════════════════════════════════
            # 【MoE 的 Router 门控 vs FFN 的 SiLU Gate：有什么区别？】
            # ════════════════════════════════════════════════════════════════════
            #
            # 虽然都叫"门控"，但它们完全是两回事！
            #
            # ┌──────────────────────────────────────────────────────────────────┐
            # │ FFN 的 SiLU Gate (SwiGLU)                                        │
            # └──────────────────────────────────────────────────────────────────┘
            #
            # 【作用】控制"哪些特征维度"被激活
            # 【粒度】元素级别 (512 个维度中每个维度独立控制)
            # 【位置】在 FFN 内部
            #
            # 【代码】
            #   gate = silu(x @ W_gate)    # [512] → [1408]，每个元素是 0~1 的权重
            #   up   = x @ W_up            # [512] → [1408]
            #   hidden = gate * up         # 逐元素相乘，1408 个乘法
            #   output = hidden @ W_down   # [1408] → [512]
            #
            # 【例子】
            #   gate = [0.9, 0.1, 0.8, 0.2, ...]  # 1408 维，每个是开关
            #   up   = [2.0, 3.0, 1.0, 5.0, ...]  # 1408 维，原始信息
            #   hidden = [1.8, 0.3, 0.8, 1.0, ...]  # 有些特征被"关小"了
            #
            # 【本质】特征过滤器，决定 512 个语义特征哪些重要
            #
            # ┌──────────────────────────────────────────────────────────────────┐
            # │ MoE 的 Router Gate (路由门控)                                    │
            # └──────────────────────────────────────────────────────────────────┘
            #
            # 【作用】控制"选择哪几个专家网络"
            # 【粒度】专家级别 (4 个专家中选 2 个)
            # 【位置】在多个 FFN 之前，决定用哪个 FFN
            #
            # 【代码】
            #   scores = x @ W_router      # [512] @ [512, 4] = [4]，4 个专家的分数
            #   weights, indices = topk(softmax(scores), k=2)  # 选 top-2 专家
            #   
            #   # 只计算被选中的专家
            #   out = weights[0] * Expert_0(x) + weights[1] * Expert_1(x)
            #
            # 【例子】
            #   scores = [0.1, 0.5, 0.3, 0.1]  # 4 个专家的分数
            #   选中专家 1 和专家 2，权重 [0.625, 0.375]
            #   输出 = 0.625 × FFN_1(x) + 0.375 × FFN_2(x)
            #
            # 【本质】网络选择器，决定用哪几个 FFN 网络
            #
            # ┌──────────────────────────────────────────────────────────────────┐
            # │ 对比总结                                                         │
            # └──────────────────────────────────────────────────────────────────┘
            #
            # ┌────────────────┬─────────────────────┬─────────────────────────┐
            # │                │ SiLU Gate (SwiGLU)  │ MoE Router Gate         │
            # ├────────────────┼─────────────────────┼─────────────────────────┤
            # │ 控制什么       │ 特征维度 (1408个)   │ 专家网络 (4个)          │
            # │ 粒度           │ 元素级 (逐维度)     │ 网络级 (整个FFN)        │
            # │ 输出           │ 连续权重 [0,1]      │ Top-K 选择 + 权重       │
            # │ 计算量影响     │ 不影响              │ 减少计算 (只算K个专家)  │
            # │ 参数           │ W_gate [512, 1408]  │ W_router [512, 4]       │
            # └────────────────┴─────────────────────┴─────────────────────────┘
            #
            # 【它们会同时存在吗？】
            #
            # 是的！MoE 中每个专家内部还是用 SwiGLU：
            #
            #   Router 选中专家 1 和专家 2
            #        ↓
            #   Expert_1(x):
            #     gate = silu(x @ W_gate_1)    ← SiLU Gate
            #     up = x @ W_up_1
            #     out_1 = (gate * up) @ W_down_1
            #
            #   Expert_2(x):
            #     gate = silu(x @ W_gate_2)    ← SiLU Gate
            #     up = x @ W_up_2
            #     out_2 = (gate * up) @ W_down_2
            #        ↓
            #   output = 0.625 × out_1 + 0.375 × out_2
            #
            # 【两层门控】
            # 1. 第一层：Router 选专家 (网络级)
            # 2. 第二层：每个专家内部 SiLU Gate (特征级)
            #
            # ════════════════════════════════════════════════════════════════════
            # 【MoE Router 的参数需要学习吗？存在哪里？】
            # ════════════════════════════════════════════════════════════════════
            #
            # 【答案】需要学习！存在 MoEGate.weight 里
            #
            # 【代码位置】class MoEGate 的 __init__:
            #
            #   self.weight = nn.Parameter(torch.empty((n_routed_experts, hidden_size)))
            #               = nn.Parameter([4, 512])  # 4个专家，512维输入
            #
            # 【这就是 W_router！】
            #
            #   scores = x @ W_router.T
            #          = [batch×seq, 512] @ [512, 4]
            #          = [batch×seq, 4]   # 每个 token 对 4 个专家的分数
            #
            # 【参数量】
            #
            #   W_router: 4 × 512 = 2,048 个参数
            #   (相比之下，一个 FFN 的 W_gate 就有 512 × 1408 = 720,896 个参数)
            #   Router 参数量很小！
            #
            # 【如何学习？】
            #
            # 跟其他参数一样，通过反向传播学习！
            #
            # 1. 前向传播:
            #    scores = softmax(x @ W_router.T)   # Router 给每个专家打分
            #    output = Σ(score_i × Expert_i(x))  # 加权求和
            #
            # 2. 计算损失:
            #    loss = CrossEntropy(output, target) + aux_loss (辅助损失)
            #
            # 3. 反向传播:
            #    ∂loss/∂W_router 被计算出来
            #    W_router -= learning_rate × ∂loss/∂W_router
            #
            # 【学到了什么？】
            #
            # W_router 学习: "什么类型的 token 应该去找什么专家"
            #
            # 例如:
            # - 数学相关的 token → 专家 0 分数高
            # - 语言相关的 token → 专家 1 分数高
            # - 代码相关的 token → 专家 2 分数高
            # - 常识相关的 token → 专家 3 分数高
            #
            # 【state_dict 中的位置】
            #
            # 保存模型时，Router 参数在这里:
            #
            #   model.state_dict() = {
            #     ...
            #     'layers.0.feed_forward.gate.weight': tensor([4, 512]),  # 第0层的Router
            #     'layers.1.feed_forward.gate.weight': tensor([4, 512]),  # 第1层的Router
            #     ...
            #   }
            #
            # 每一层 Transformer 都有自己的 Router！
            # (因为不同层可能需要不同的专家分配策略)
            #
            # ════════════════════════════════════════════════════════════════════
            use_moe: bool = False,             # 是否使用 MoE 架构
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │                    num_experts_per_tok 详解                    │
            # └────────────────────────────────────────────────────────────────┘
            #
            # 每个 token 激活几个专家
            #
            # 【设置建议】
            #
            # ┌──────────────────┬─────────────────────────────────────────────┐
            # │ num_experts_per_tok │ 效果                                     │
            # ├──────────────────┼─────────────────────────────────────────────┤
            # │ 1                │ 最快，但可能不够灵活                        │
            # │ 2 (常用)         │ 平衡速度和效果                              │
            # │ 4                │ 更强，但计算量大                            │
            # └──────────────────┴─────────────────────────────────────────────┘
            #
            # ════════════════════════════════════════════════════════════════
            num_experts_per_tok: int = 2,      # 每个 token 激活的专家数量
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │                    n_routed_experts 详解                       │
            # └────────────────────────────────────────────────────────────────┘
            #
            # 总共有几个可选专家
            #
            # 【计算量分析】
            #
            # 实际计算量 = (num_experts_per_tok / n_routed_experts) × 总专家参数
            #
            # 例如: 4 个专家，每次选 2 个 → 计算量 = 50% 参数量
            #
            # ════════════════════════════════════════════════════════════════
            n_routed_experts: int = 4,         # 路由专家总数
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │                    n_shared_experts 详解                       │
            # └────────────────────────────────────────────────────────────────┘
            #
            # 共享专家 = 所有 token 都会经过的专家
            #
            # 【为什么要有共享专家？】
            #
            # 1. 学习通用知识 (所有 token 都需要的基础能力)
            # 2. 稳定训练 (避免某些专家从不被选中)
            # 3. DeepSeek-V2 论文发现这样效果更好
            #
            # 【计算流程】
            #
            # output = Router选中的专家输出 + 共享专家输出
            #
            # ════════════════════════════════════════════════════════════════
            n_shared_experts: int = 1,         # 共享专家数量 (所有 token 都会经过)
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │                    scoring_func 详解                           │
            # └────────────────────────────────────────────────────────────────┘
            #
            # Router 用什么函数给专家打分
            #
            # 'softmax': 把分数转成概率 (加起来=1)
            #   scores = [2.0, 1.0, 0.5, 0.5]
            #   → softmax → [0.47, 0.26, 0.14, 0.13]
            #
            # ════════════════════════════════════════════════════════════════
            scoring_func: str = 'softmax',     # 门控评分函数
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │                    aux_loss_alpha 详解                         │
            # └────────────────────────────────────────────────────────────────┘
            #
            # 辅助损失 (Auxiliary Loss) 的权重
            #
            # 【什么是辅助损失？】
            #
            # 一个额外的损失项，用来让专家"负载均衡"
            #
            # 【为什么需要负载均衡？】
            #
            # 【问题】如果不加约束，可能出现：
            #
            #   - 专家1: 被选中 90% 的 token → 累死
            #   - 专家2: 被选中 5% → 几乎没用
            #   - 专家3: 被选中 3% → 几乎没用
            #   - 专家4: 被选中 2% → 几乎没用
            #
            # 这叫"专家坍塌"(Expert Collapse)，浪费了其他专家的参数
            #
            # 【高中生能懂的比喻】
            #
            # 像食堂打饭：
            # - 没有辅助损失: 大家都排网红窗口，其他窗口没人
            # - 有辅助损失:   给排队少的窗口"打折"，引导分流
            #
            # 【数学原理】
            #
            # 辅助损失 = α × Σ(专家被选概率 × 专家实际处理的token比例)
            #
            # 如果某专家又热门、又处理多，这个值就大 → 惩罚
            # 逼迫 Router 更均匀地分配任务
            #
            # 【alpha 值的影响】
            #
            # ┌──────────────┬─────────────────────────────────────────────┐
            # │ aux_loss_alpha │ 效果                                      │
            # ├──────────────┼─────────────────────────────────────────────┤
            # │ 0.0          │ 不管负载均衡，可能坍塌                      │
            # │ 0.01 (常用)  │ 轻微引导，主要还是看任务                    │
            # │ 0.1          │ 强制均衡，可能影响性能                      │
            # └──────────────┴─────────────────────────────────────────────┘
            #
            # ════════════════════════════════════════════════════════════════
            aux_loss_alpha: float = 0.01,      # 辅助损失权重 (用于负载均衡)
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │                    seq_aux 详解                                │
            # └────────────────────────────────────────────────────────────────┘
            #
            # 辅助损失是在序列级别计算，还是在 batch 级别
            #
            # 【True (序列级别)】
            #   - 每个序列内部专家要均衡
            #   - 更细粒度的控制
            #
            # 【False (batch级别)】
            #   - 整个 batch 的专家使用要均衡
            #   - 计算更简单
            #
            # 一般用 True，效果更好
            #
            # ════════════════════════════════════════════════════════════════
            seq_aux: bool = True,              # 是否在序列级别计算辅助损失
            #
            # ┌────────────────────────────────────────────────────────────────┐
            # │                    norm_topk_prob 详解                         │
            # └────────────────────────────────────────────────────────────────┘
            #
            # 是否对选中的 top-k 专家的权重重新归一化
            #
            # 【例子】
            #
            # 假设 4 个专家的原始分数: [0.4, 0.3, 0.2, 0.1]
            # 选 top-2: 专家0 (0.4), 专家1 (0.3)
            #
            # 【norm_topk_prob=False】
            #   权重直接用: [0.4, 0.3]
            #   output = 0.4 × out_0 + 0.3 × out_1
            #   (权重和 = 0.7，不是 1)
            #
            # 【norm_topk_prob=True】
            #   重新归一化: [0.4/(0.4+0.3), 0.3/(0.4+0.3)] = [0.57, 0.43]
            #   output = 0.57 × out_0 + 0.43 × out_1
            #   (权重和 = 1)
            #
            # 【为什么要归一化？】
            #
            # 保证输出的尺度稳定，不会因为选了多少专家而变化
            #
            # ════════════════════════════════════════════════════════════════
            norm_topk_prob: bool = True,       # 是否归一化 top-k 概率
            **kwargs
    ):
        super().__init__(**kwargs)
        self.dropout = dropout
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.hidden_act = hidden_act
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.num_key_value_heads = num_key_value_heads
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.rope_theta = rope_theta
        self.inference_rope_scaling = inference_rope_scaling
        # 外推长度 = factor * original_max_position_embeddings = 32768
        #
        # ════════════════════════════════════════════════════════════════════
        # 【YaRN 参数详解】每个参数的原理和作用
        # ════════════════════════════════════════════════════════════════════
        #
        # YaRN = Yet another RoPE extensioN，用于让模型处理超过训练长度的序列
        #
        # ┌──────────────────────────────────────────────────────────────────┐
        # │ factor = 16                                                      │
        # └──────────────────────────────────────────────────────────────────┘
        #
        # 【作用】位置缩放因子，决定能扩展多少倍
        #
        # 【原理】把"虚拟位置"压缩到模型能理解的范围
        #
        #   原始: 位置 0, 1, 2, ..., 32767
        #   压缩后: 位置 0, 1/16, 2/16, ..., 32767/16 = 0, 0.0625, 0.125, ..., 2047.9
        #
        # 【效果】
        #   训练长度 2048 × factor 16 = 理论支持 32768 tokens
        #
        # ┌──────────────────────────────────────────────────────────────────┐
        # │ original_max_position_embeddings = 2048                          │
        # └──────────────────────────────────────────────────────────────────┘
        #
        # 【作用】模型原始训练时的最大序列长度
        #
        # 【用途】作为基准，判断当前序列是否需要外推
        #   - 序列长度 ≤ 2048: 不需要外推，正常计算
        #   - 序列长度 > 2048: 需要外推，应用 YaRN
        #
        # ┌──────────────────────────────────────────────────────────────────┐
        # │ beta_fast = 32, beta_slow = 1                                    │
        # └──────────────────────────────────────────────────────────────────┘
        #
        # 【作用】控制哪些维度需要插值，哪些保持不变
        #
        # 【背景】RoPE 不同维度的频率不同:
        #   - 低维度 (dim 0,1,2...): 高频，变化快，编码短距离信息
        #   - 高维度 (dim 60,61,62...): 低频，变化慢，编码长距离信息
        #
        # 【YaRN 的关键洞察】
        #   - 高频维度: 主要编码局部关系，不需要改变
        #   - 低频维度: 主要编码远距离关系，需要插值压缩
        #
        # 【beta 的作用】
        #   - beta_fast = 32: "高频阈值"，频率 > 1/(32×2π) 的维度不变
        #   - beta_slow = 1:  "低频阈值"，频率 < 1/(1×2π) 的维度完全插值
        #   - 中间的维度: 线性过渡 (ramp)
        #
        # 【具体计算】
        #   inv_dim(beta) 计算频率=1/(beta×2π) 对应的维度索引
        #   - inv_dim(32) ≈ 低维度边界
        #   - inv_dim(1) ≈ 高维度边界
        #
        # ┌──────────────────────────────────────────────────────────────────┐
        # │ attention_factor = 1.0                                           │
        # └──────────────────────────────────────────────────────────────────┘
        #
        # 【作用】注意力分数的缩放因子
        #
        # 【原理】外推后注意力分数可能偏小，用这个因子补偿
        #   - 1.0 = 不做额外缩放
        #   - >1.0 = 增强注意力
        #
        # ┌──────────────────────────────────────────────────────────────────┐
        # │ type = "yarn"                                                    │
        # └──────────────────────────────────────────────────────────────────┘
        #
        # 【作用】指定使用 YaRN 外推方法
        #
        # 【其他方法对比】
        #   - "linear": 简单线性插值，效果差
        #   - "dynamic": 动态 NTK，效果中等
        #   - "yarn": YaRN，效果最好 (分频率处理)
        #
        # ════════════════════════════════════════════════════════════════════
        self.rope_scaling = {
            "beta_fast": 32,
            "beta_slow": 1,
            "factor": 16,
            "original_max_position_embeddings": 2048,
            "attention_factor": 1.0,
            "type": "yarn"
        } if self.inference_rope_scaling else None
        self.flash_attn = flash_attn
        ####################################################
        # Here are the specific configurations of MOE
        # When use_moe is false, the following is invalid
        ####################################################
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # 每个token选择的专家数量
        self.n_routed_experts = n_routed_experts  # 总的专家数量
        self.n_shared_experts = n_shared_experts  # 共享专家
        self.scoring_func = scoring_func  # 评分函数，默认为'softmax'
        self.aux_loss_alpha = aux_loss_alpha  # 辅助损失的alpha参数
        self.seq_aux = seq_aux  # 是否在序列级别上计算辅助损失
        self.norm_topk_prob = norm_topk_prob  # 是否标准化top-k概率


# 📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘
#                                             MiniMind Model
# 📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘📘

import math
import torch
import torch.nn.init as init
import torch.nn.functional as F
from torch import nn
from transformers.activations import ACT2FN
from typing import Optional, Tuple, List, Union
from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig
from transformers.modeling_outputs import CausalLMOutputWithPast


class RMSNorm(torch.nn.Module):
    """
    RMS Layer Normalization (均方根层归一化)
    
    【背景】
    传统的 LayerNorm 需要计算均值和方差，而 RMSNorm 只计算均方根，更高效。
    
    【数学原理】
    标准 LayerNorm: y = (x - μ) / σ * γ + β，其中 μ=mean(x), σ=std(x)
    RMSNorm: y = x / RMS(x) * γ，其中 RMS(x) = sqrt(mean(x²) + ε)
    
    【优势】
    1. 计算更快 (不需要计算均值)
    2. 参数更少 (没有偏置项 β)
    3. 实际效果与 LayerNorm 相当
    
    【参数】
    - dim: 归一化维度 (通常是 hidden_size)
    - eps: 防止除零的小常数 (1e-5)
    """
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        # weight 是可学习的缩放参数 γ，初始化为全 1
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        """
        计算 RMS 归一化: x / sqrt(mean(x²) + eps)
        
        torch.rsqrt() 是 1/sqrt() 的高效实现
        """
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        # 转 float32 计算以保持数值稳定，然后转回原类型
        return self.weight * self._norm(x.float()).type_as(x)


def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), rope_base: float = 1e6,
                         rope_scaling: Optional[dict] = None):
    """
    预计算 RoPE (Rotary Position Embedding) 的频率
    
    【RoPE 原理】
    RoPE 通过旋转向量来编码位置信息，具有以下优点:
    1. 相对位置信息: 只关心 token 之间的相对距离
    2. 衰减性: 距离越远相关性越弱 (通过旋转实现)
    3. 可外推性: 可处理训练时未见过的长度
    
    【数学公式】
    对于位置 m 和维度 d:
        θ_d = 1 / (base^(2d/dim))  # 每个维度有不同的频率
        旋转角度 = m * θ_d         # 位置越大，旋转越多
    
    【参数】
    - dim: 每个注意力头的维度
    - end: 预计算的最大位置数 (32768)
    - rope_base: 基础频率 (1e6)
    - rope_scaling: YaRN 外推配置
    
    【返回】
    - freqs_cos: [end, dim] 的余弦频率表
    - freqs_sin: [end, dim] 的正弦频率表
    """
    # 计算基础频率 θ_d = 1 / (base^(2d/dim))
    # 例如 dim=64, base=1e6: θ = [1, 1/1e6^(2/64), 1/1e6^(4/64), ...]
    freqs, attn_factor = 1.0 / (rope_base ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)), 1.0
    
    # YaRN 外推: 当需要处理超过训练长度的序列时使用
    if rope_scaling is not None:
        orig_max, factor, beta_fast, beta_slow, attn_factor = (
            rope_scaling.get("original_max_position_embeddings", 2048), rope_scaling.get("factor", 16),
            rope_scaling.get("beta_fast", 32.0), rope_scaling.get("beta_slow", 1.0), rope_scaling.get("attention_factor", 1.0)
        )
        if end / orig_max > 1.0:
            # ════════════════════════════════════════════════════════════════
            # 【YaRN 外推逻辑详解】数学原理逐行分析
            # ════════════════════════════════════════════════════════════════
            #
            # 【问题】模型训练时最大 2048 位置，现在要处理 32768 位置
            #
            # 【YaRN 的核心思想】
            # 不同维度的频率不同，需要区别对待:
            #   - 高频维度 (变化快): 不改变，保持局部位置精度
            #   - 低频维度 (变化慢): 压缩频率，让它能"装下"更多位置
            #
            # ────────────────────────────────────────────────────────────────
            # 【第1步】inv_dim 函数: 计算频率对应的维度索引
            # ────────────────────────────────────────────────────────────────
            #
            # inv_dim(b) = (dim × log(orig_max / (b×2π))) / (2 × log(rope_base))
            #
            # 【推导】
            # RoPE 频率公式: freq_i = 1 / (rope_base ^ (2i/dim))
            # 我们想找: 当频率 = 1/(b×2π) 时，对应哪个维度 i？
            #
            # 设 freq_i = 1/(b×2π)
            # → 1/(rope_base^(2i/dim)) = 1/(b×2π)
            # → rope_base^(2i/dim) = b×2π
            # → 2i/dim × log(rope_base) = log(b×2π)
            # → i = dim × log(b×2π) / (2×log(rope_base))
            #
            # 【例子】dim=64, rope_base=1e6, orig_max=2048
            #   inv_dim(32) ≈ 5   → 维度5以下是高频
            #   inv_dim(1) ≈ 20   → 维度20以上是低频
            #
            # ────────────────────────────────────────────────────────────────
            # 【第2步】计算 low 和 high 边界
            # ────────────────────────────────────────────────────────────────
            #
            # low = inv_dim(beta_fast) = inv_dim(32) ≈ 5
            # high = inv_dim(beta_slow) = inv_dim(1) ≈ 20
            #
            # 【含义】
            #   - 维度 0~5 (低于low): 高频，γ=0，不插值
            #   - 维度 5~20 (low到high): 中间，γ从0到1线性过渡
            #   - 维度 20~32 (高于high): 低频，γ=1，完全插值
            #
            # ────────────────────────────────────────────────────────────────
            # 【第3步】计算 ramp (γ)
            # ────────────────────────────────────────────────────────────────
            #
            # ramp = clamp((i - low) / (high - low), 0, 1)
            #
            # 【例子】low=5, high=20
            #   维度 0:  ramp = (0-5)/(20-5) = -0.33 → clamp → 0
            #   维度 5:  ramp = (5-5)/(20-5) = 0
            #   维度 12: ramp = (12-5)/(20-5) = 0.47
            #   维度 20: ramp = (20-5)/(20-5) = 1
            #   维度 30: ramp = (30-5)/(20-5) = 1.67 → clamp → 1
            #
            # ────────────────────────────────────────────────────────────────
            # 【第4步】应用插值公式
            # ────────────────────────────────────────────────────────────────
            #
            # freqs_new = freqs × (1 - ramp + ramp/factor)
            #           = freqs × ((1-γ) + γ/s)
            #
            # 【分析】
            #   - γ=0 (高频): freqs_new = freqs × 1 = freqs (不变)
            #   - γ=1 (低频): freqs_new = freqs × (1/factor) = freqs/16 (压缩16倍)
            #   - γ=0.5 (中间): freqs_new = freqs × (0.5 + 0.5/16) ≈ freqs × 0.53
            #
            # 【效果】
            # 低频维度的频率被压缩 → 相同角度变化覆盖更大的位置范围
            # 高频维度不变 → 保持局部位置的精确区分
            #
            # ════════════════════════════════════════════════════════════════
            #
            # ════════════════════════════════════════════════════════════════
            # 【YaRN 究竟怎么扩展上下文的？核心原理！】
            # ════════════════════════════════════════════════════════════════
            #
            # 【你的疑问】
            # "原来训练的 2048 不还是 2048 吗？权重也没变，哪里扩展了？"
            #
            # 【关键洞察】
            # RoPE 不是直接存储位置编号，而是存储"旋转角度"！
            # 模型学习的是"角度模式"，不是"位置编号"！
            #
            # ────────────────────────────────────────────────────────────────
            # 【RoPE 的本质】
            # ────────────────────────────────────────────────────────────────
            #
            # 位置 m 的旋转角度: angle = m × freq
            #
            # 训练时 m ∈ [0, 2047]，模型见过的角度范围:
            #   angle_max = 2047 × freq
            #
            # 推理时 m = 4096，如果不处理:
            #   angle = 4096 × freq  ← 超出训练范围！模型没见过！
            #
            # ────────────────────────────────────────────────────────────────
            # 【YaRN 的魔法：压缩频率】
            # ────────────────────────────────────────────────────────────────
            #
            # YaRN 把频率压缩 16 倍: freq_new = freq / 16
            #
            # 现在位置 32768 的角度:
            #   angle = 32768 × (freq / 16) = 2048 × freq
            #
            # 这个角度 ≈ 原来位置 2048 的角度！模型见过！
            #
            # 【具体例子】
            #
            # 假设某维度 freq = 0.01:
            #
            # ┌─────────────────────────────────────────────────────────────┐
            # │ 不用 YaRN:                                                  │
            # │   位置 2048: angle = 2048 × 0.01 = 20.48 ✓ 训练见过         │
            # │   位置 4096: angle = 4096 × 0.01 = 40.96 ✗ 没见过！         │
            # │   位置 32768: angle = 32768 × 0.01 = 327.68 ✗ 完全陌生！    │
            #
            # ────────────────────────────────────────────────────────────────
            # 【为什么说"没见过"？RoPE 训练时学了什么？】
            # ────────────────────────────────────────────────────────────────
            #
            # 【RoPE 不存储角度，而是"学会理解"角度】
            #
            # RoPE 本身只是一个固定的数学变换:
            #   rotated_q = q × cos(angle) + rotate_half(q) × sin(angle)
            #
            # 模型学习的是: "当 Q 和 K 有某种角度差时，该如何计算注意力"
            #
            # 【训练时发生了什么？】
            #
            # 训练数据: 序列长度最大 2048
            # 训练时模型看到的角度范围: 0 ~ 2048 × freq
            #
            # 对于 freq = 0.01:
            #   训练时: angle ∈ [0, 20.48]  (因为 pos ∈ [0, 2048])
            #
            # 模型的权重 (W_q, W_k, W_v, FFN 等) 被优化为:
            #   "当 angle 在 [0, 20.48] 范围时，我知道怎么处理"
            #
            # 【推理时为什么会失败？】
            #
            # 推理时 pos = 4096:
            #   angle = 4096 × 0.01 = 40.96
            #
            # 这个 40.96 在训练时从未出现过！
            #
            # 就像一个学生:
            #   - 只做过 1~100 的加法练习
            #   - 突然让他算 1000 + 2000
            #   - 虽然原理一样，但他对大数字没有"感觉"
            #
            # 【神经网络的"泛化边界"】
            #
            # 神经网络只能在训练分布附近泛化:
            #
            #   训练分布: angle ∈ [0, 20]
            #            ↓
            #   ┌──────────────────────────────────────────┐
            #   │  0    5    10   15   20   25   30   40  │
            #   │  ✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓✓  ??   ??   ✗✗ │
            #   │  ←─── 训练范围 ────→  ← 可能OK →  失败  │
            #   └──────────────────────────────────────────┘
            #
            # 【cos/sin 函数本身不是问题】
            #
            # 你可能会想: cos(20) 和 cos(40) 不都是数字吗？
            #
            # 是的，但问题是 W_q、W_k 等参数是在"角度≤20"的条件下优化的
            # 它们学会的是:
            #   "当我看到 cos(5) 时，这代表相邻词"
            #   "当我看到 cos(15) 时，这代表较远的词"
            #
            # 当突然出现 cos(40) 时:
            #   模型: "这是什么？我没见过这种模式！"
            #
            # 【YaRN 如何解决？】
            #
            # YaRN 压缩频率: freq_new = freq / 16
            #
            # 现在 pos = 32768:
            #   angle = 32768 × (0.01/16) = 20.48
            #
            # 20.48 在训练范围内！模型知道怎么处理！
            #
            # ────────────────────────────────────────────────────────────────
            # │                                                             │
            # │ 用 YaRN (factor=16):                                        │
            # │   freq_new = 0.01 / 16 = 0.000625                           │
            # │   位置 2048: angle = 2048 × 0.000625 = 1.28                 │
            # │   位置 4096: angle = 4096 × 0.000625 = 2.56                 │
            # │   位置 32768: angle = 32768 × 0.000625 = 20.48 ✓ 见过！     │
            # └─────────────────────────────────────────────────────────────┘
            #
            # ────────────────────────────────────────────────────────────────
            # 【但这不会丢失位置区分能力吗？】
            # ────────────────────────────────────────────────────────────────
            #
            # 【问题】压缩后，位置 0 和位置 16 的角度差变小了！
            #
            #   原来: pos0 = 0°, pos16 = 16×0.01 = 0.16°, 差 0.16°
            #   压缩后: pos0 = 0°, pos16 = 16×0.000625 = 0.01°, 差 0.01°
            #
            # 【YaRN 的聪明之处】
            # 只压缩低频维度，保持高频维度不变！
            #
            #   高频维度 (dim 0-5): 不压缩，保持局部位置区分能力
            #   低频维度 (dim 20-32): 压缩16倍，扩展远距离范围
            #
            # 【类比】
            # 想象一个尺子:
            #   - 高频 = 毫米刻度，测量近距离，精度高
            #   - 低频 = 厘米刻度，测量远距离，范围大
            #
            # YaRN: 把厘米刻度换成分米刻度，能量更远，但毫米刻度不变
            #
            # ────────────────────────────────────────────────────────────────
            # 【所以 YaRN 改变的是什么？】
            # ────────────────────────────────────────────────────────────────
            #
            # ┌────────────────┬────────────────┬────────────────────────────┐
            # │ 组件           │ 是否改变？     │ 说明                       │
            # ├────────────────┼────────────────┼────────────────────────────┤
            # │ 模型权重       │ ✗ 不变         │ W_q, W_k, W_v 等都不变     │
            # │ KV Cache 大小  │ ✗ 不变         │ 还是每层存 [seq, head_dim] │
            # │ 位置编号       │ ✗ 不变         │ 还是 0, 1, 2, ..., 32767   │
            # │ 旋转频率       │ ✓ 改变！       │ 低频维度的 freq 被压缩     │
            # │ 旋转角度       │ ✓ 改变！       │ angle = pos × freq_new     │
            # └────────────────┴────────────────┴────────────────────────────┘
            #
            # 【核心】
            # 模型学的是"角度"不是"位置"
            # 压缩频率 → 大位置产生小角度 → 角度回到训练范围 → 模型能处理
            #
            # ────────────────────────────────────────────────────────────────
            # 【为什么这样就能工作？】
            # ────────────────────────────────────────────────────────────────
            #
            # 【RoPE 的相对位置性质】
            #
            # Q_m @ K_n 的点积只依赖于 (m-n)，即相对距离
            #
            # 训练时模型学到:
            #   "角度差 0.1 = 相邻词"
            #   "角度差 1.0 = 隔10个词"
            #   "角度差 10.0 = 隔100个词"
            #
            # YaRN 压缩后:
            #   位置 30000 和 30001 的角度差 ≈ 0.1 (还是像相邻词)
            #   位置 30000 和 30100 的角度差 ≈ 1.0 (还是像隔10个词)
            #
            # 模型仍然能正确理解相对位置！
            #
            # ════════════════════════════════════════════════════════════════
            # YaRN 公式: f'(i) = f(i) * ((1-γ) + γ/s), γ∈[0,1] 是线性 ramp
            # 低频维度使用插值，高频维度保持不变
            inv_dim = lambda b: (dim * math.log(orig_max / (b * 2 * math.pi))) / (2 * math.log(rope_base))
            low, high = max(math.floor(inv_dim(beta_fast)), 0), min(math.ceil(inv_dim(beta_slow)), dim // 2 - 1)
            ramp = torch.clamp((torch.arange(dim // 2, device=freqs.device).float() - low) / max(high - low, 0.001), 0, 1)
            freqs = freqs * (1 - ramp + ramp / factor)

    # 生成位置索引 [0, 1, 2, ..., end-1]
    t = torch.arange(end, device=freqs.device)
    # 外积得到 [end, dim//2] 的频率矩阵: 每个位置在每个维度的旋转角度
    freqs = torch.outer(t, freqs).float()
    # 计算 cos 和 sin，并拼接 (因为要同时应用于向量的两半)
    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1) * attn_factor
    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1) * attn_factor
    return freqs_cos, freqs_sin


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """
    应用旋转位置编码到 Query 和 Key
    
    【数学原理】
    RoPE 将向量视为复数，通过旋转编码位置:
    
    对于向量 x = [x_0, x_1, ..., x_{d-1}]:
    1. 分成两半: x_first = x[:d/2], x_second = x[d/2:]
    2. rotate_half(x) = [-x_second, x_first]
    3. rotated(x) = x * cos(mθ) + rotate_half(x) * sin(mθ)
    
    这等价于在复数平面上将 (x_i + i*x_{i+d/2}) 旋转 mθ_i 角度
    
    【参数】
    - q, k: Query 和 Key 张量 [batch, seq_len, heads, head_dim]
    - cos, sin: 预计算的频率 [seq_len, head_dim]
    """
    def rotate_half(x):
        """将向量的前后两半交换并取负: [a,b,c,d] -> [-c,-d,a,b]"""
        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), dim=-1)

    # ════════════════════════════════════════════════════════════════════
    # 【unsqueeze 是干嘛的？】详细解释
    # ════════════════════════════════════════════════════════════════════
    #
    # 【问题】q 和 cos 的维度不匹配，无法直接相乘
    #
    #   q:   [batch, seq_len, num_heads, head_dim] = [2, 10, 8, 64]
    #   cos: [seq_len, head_dim]                   = [10, 64]
    #
    # 【unsqueeze(1) 做了什么？】在指定位置插入一个维度
    #
    #   cos.unsqueeze(1) = cos.unsqueeze(unsqueeze_dim)
    #                    = [10, 64] → [10, 1, 64]
    #                           ↑ 在位置1插入维度
    #
    # 【为什么要插入？】让广播机制生效
    #
    #   q:               [batch, seq_len, num_heads, head_dim]
    #                    [2,     10,      8,         64]
    #
    #   cos.unsqueeze(1):[seq_len, 1, head_dim]
    #                    [10,      1, 64]
    #
    #   广播后:          [2, 10, 8, 64] × [10, 1, 64]
    #                           ↓ 广播
    #                    [2, 10, 8, 64] × [1, 10, 1, 64]  # batch和heads维度自动广播
    #                           ↓
    #                    [2, 10, 8, 64]  # 每个头都用相同的cos/sin
    #
    # 【广播规则】
    # - 维度为1的可以扩展到任意大小
    # - 从右向左对齐，缺少的维度自动补1
    #
    # 【如果不 unsqueeze？】
    #   [2, 10, 8, 64] × [10, 64]  ← 维度数不同，无法广播，报错！
    #
    # ════════════════════════════════════════════════════════════════════
    # 旋转公式: x * cos + rotate_half(x) * sin
    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))
    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))
    return q_embed, k_embed


def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    重复 Key/Value 张量以匹配 Query 头数 (用于 GQA)
    
    【GQA (Grouped Query Attention) 原理】
    - 标准 MHA: Q_heads = K_heads = V_heads = 8
    - GQA: Q_heads = 8, K_heads = V_heads = 2
    - 每个 KV 头被 4 个 Q 头共享
    - 优势: 减少 4 倍 KV Cache 内存，推理更快
    
    【参数】
    - x: [batch, seq_len, num_kv_heads, head_dim]
    - n_rep: 每个 KV 头需要复制的次数
    
    【返回】
    - [batch, seq_len, num_kv_heads * n_rep, head_dim]
    """
    bs, slen, num_key_value_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    # ════════════════════════════════════════════════════════════════════
    # 【repeat_kv 代码逐步分解】
    # ════════════════════════════════════════════════════════════════════
    #
    # 【目标】把 2 个 KV 头复制成 8 个，以匹配 8 个 Q 头
    #
    # 【输入】x: [batch=2, seq=10, kv_heads=2, head_dim=64]
    # 【输出】  [batch=2, seq=10, q_heads=8, head_dim=64]
    #
    # ────────────────────────────────────────────────────────────────────
    # 【第1步】x[:, :, :, None, :]  在位置3插入新维度
    # ────────────────────────────────────────────────────────────────────
    #
    #   [2, 10, 2, 64] → [2, 10, 2, 1, 64]
    #                           ↑ 新增维度，大小为1
    #
    # 【数据结构变化】
    #   原来: kv_head_0: [64维向量], kv_head_1: [64维向量]
    #   现在: kv_head_0: [[64维向量]], kv_head_1: [[64维向量]]
    #                     ↑ 多套了一层
    #
    # ────────────────────────────────────────────────────────────────────
    # 【第2步】.expand(..., n_rep, ...)  在新维度上扩展
    # ────────────────────────────────────────────────────────────────────
    #
    #   [2, 10, 2, 1, 64] → [2, 10, 2, 4, 64]
    #                              ↑ 从1扩展到4 (n_rep=4)
    #
    # 【expand 的特点】
    #   - 不复制数据，只改变"视图"
    #   - 内存不变，4份数据指向同一块内存
    #   - 高效！
    #
    # 【数据结构变化】
    #   kv_head_0: [copy0, copy1, copy2, copy3] ← 4份相同的64维向量
    #   kv_head_1: [copy0, copy1, copy2, copy3]
    #
    # ────────────────────────────────────────────────────────────────────
    # 【第3步】.reshape(..., kv_heads * n_rep, ...)  合并维度
    # ────────────────────────────────────────────────────────────────────
    #
    #   [2, 10, 2, 4, 64] → [2, 10, 8, 64]
    #           ↑  ↑           ↑
    #           2×4 = 8 个头
    #
    # 【最终效果】
    #
    #   原来 2 个 KV 头:     [K0, K1]
    #   现在 8 个 KV 头:     [K0, K0, K0, K0, K1, K1, K1, K1]
    #                        ↑───共享───↑  ↑───共享───↑
    #                        Q头0,1,2,3用   Q头4,5,6,7用
    #
    # 【为什么这样做？】
    #   Q @ K.T 要求 Q 和 K 的头数相同
    #   GQA 中 Q=8头, K=2头，必须把 K 复制 4 次才能计算
    #
    # ════════════════════════════════════════════════════════════════════
    # 扩展维度 [B,L,KV,1,D] -> [B,L,KV,n_rep,D] -> [B,L,KV*n_rep,D]
    return (
        x[:, :, :, None, :].expand(bs, slen, num_key_value_heads, n_rep, head_dim).reshape(bs, slen, num_key_value_heads * n_rep, head_dim)
    )


class Attention(nn.Module):
    """
    多头注意力层 (支持 GQA 和 Flash Attention)
    
    【注意力机制核心公式】
    Attention(Q, K, V) = softmax(Q @ K^T / √d_k) @ V
    
    其中:
    - Q (Query): "我在找什么" - 当前位置的查询向量
    - K (Key): "我有什么" - 所有位置的键向量
    - V (Value): "我的内容" - 所有位置的值向量
    - d_k: Key 的维度，用于缩放防止梯度消失
    
    【多头注意力】
    将 Q,K,V 分成多个"头"，每个头独立计算注意力:
    - 不同的头关注不同类型的模式
    - 增加模型的表达能力
    - 计算可以并行化
    
    【GQA vs MHA】
    - MHA: 每个 Q 头都有对应的 K,V 头 (8Q, 8K, 8V)
    - GQA: 多个 Q 头共享 K,V 头 (8Q, 2K, 2V)
    - GQA 优势: 减少 KV Cache 内存，推理更快
    
    【Flash Attention】
    - 标准注意力需要 O(n²) 内存存储注意力矩阵
    - Flash Attention 分块计算，只需 O(n) 内存
    - 同时由于更好的内存访问模式，速度更快
    """
    def __init__(self, args: MiniMindConfig):
        super().__init__()
        # 确定 KV 头数 (GQA 配置)
        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads
        assert args.num_attention_heads % self.num_key_value_heads == 0
        
        self.n_local_heads = args.num_attention_heads        # Q 头数
        self.n_local_kv_heads = self.num_key_value_heads     # KV 头数
        self.n_rep = self.n_local_heads // self.n_local_kv_heads  # GQA 重复因子
        self.head_dim = args.hidden_size // args.num_attention_heads  # 每头维度
        
        # 投影层: 将 hidden_size 映射到各个头
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)
        
        self.attn_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.dropout = args.dropout
        # 检查是否支持 Flash Attention (需要 PyTorch >= 2.0)
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn

    def forward(self,
                x: torch.Tensor,
                position_embeddings: Tuple[torch.Tensor, torch.Tensor],  # (cos, sin)
                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                use_cache=False,
                attention_mask: Optional[torch.Tensor] = None):
        """
        前向传播
        
        【参数】
        - x: 输入张量 [batch, seq_len, hidden_size]
        - position_embeddings: (cos, sin) RoPE 位置编码
        - past_key_value: KV Cache，用于增量解码
        - use_cache: 是否返回 KV Cache
        - attention_mask: 注意力掩码
        
        【返回】
        - output: [batch, seq_len, hidden_size]
        - past_kv: 更新后的 KV Cache
        """
        bsz, seq_len, _ = x.shape
        
        # 步骤1: 线性投影得到 Q, K, V
        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        # 重塑为多头格式: [B, L, heads, head_dim]
        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)

        # 步骤2: 应用 RoPE 位置编码
        cos, sin = position_embeddings
        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])

        # 步骤3: KV Cache 处理 (用于推理时的增量解码)
        # 将新的 K,V 拼接到之前的 cache 上
        if past_key_value is not None:
            xk = torch.cat([past_key_value[0], xk], dim=1)
            xv = torch.cat([past_key_value[1], xv], dim=1)
        past_kv = (xk, xv) if use_cache else None

        # 步骤4: GQA 处理 - 复制 KV 以匹配 Q 头数
        # 转置为 [B, heads, L, head_dim] 以便矩阵乘法
        xq, xk, xv = (
            xq.transpose(1, 2),
            repeat_kv(xk, self.n_rep).transpose(1, 2),
            repeat_kv(xv, self.n_rep).transpose(1, 2)
        )

        # 步骤5: 计算注意力
        # ════════════════════════════════════════════════════════════════════
        # 【Flash Attention 条件判断详解】
        # ════════════════════════════════════════════════════════════════════
        #
        # self.flash: 是否启用 Flash Attention (配置决定)
        # seq_len > 1: 序列长度大于1 (单token推理时Flash没优势)
        # attention_mask is None or torch.all(attention_mask == 1):
        #   → 没有 mask 或全是1 (Flash Attention 不支持复杂mask)
        #
        # 【只有三个条件都满足才用 Flash Attention】
        #
        # ════════════════════════════════════════════════════════════════════
        if self.flash and seq_len > 1 and (attention_mask is None or torch.all(attention_mask == 1)):
            # ════════════════════════════════════════════════════════════════
            # 【scaled_dot_product_attention 详解】
            # ════════════════════════════════════════════════════════════════
            #
            # 这是 PyTorch 2.0+ 提供的高效注意力实现
            #
            # 【数学上等价于】
            #
            #   scores = (Q @ K.T) / sqrt(d_k)    # [batch, heads, seq, seq]
            #   if is_causal:
            #       scores = scores + causal_mask  # 上三角设为 -inf
            #   weights = softmax(scores)          # 归一化
            #   weights = dropout(weights)         # 训练时随机丢弃
            #   output = weights @ V               # 加权求和
            #
            # 【但实现上完全不同！】
            #
            # 【标准实现的问题】
            #   scores 矩阵: [batch, heads, seq, seq]
            #   seq=32768 时: 32768 × 32768 × 4字节 = 4GB 显存！
            #   而且要整个算完才能做 softmax
            #
            # 【Flash Attention 怎么做？】
            #
            # ┌─────────────────────────────────────────────────────────────┐
            # │ 1. 分块计算 (Tiling)                                        │
            # │    把 Q, K, V 分成小块，每次只算一小块的注意力               │
            # │                                                             │
            # │ 2. 在线 Softmax                                             │
            # │    不需要完整 scores 矩阵，边算边更新 softmax 结果          │
            # │                                                             │
            # │ 3. 不存储中间结果                                           │
            # │    scores 矩阵从不完整存在于显存中                          │
            # └─────────────────────────────────────────────────────────────┘
            #
            # 【参数解释】
            #   xq, xk, xv: Q, K, V 张量
            #   dropout_p:  训练时的 dropout 概率
            #   is_causal:  True = 因果注意力 (只看前面的token)
            #
            # 【性能提升】
            #   - 内存: O(n²) → O(n)，从 4GB 变成几十MB
            #   - 速度: 快 2-4 倍 (更好的内存访问模式)
            #
            # ════════════════════════════════════════════════════════════════
            # Flash Attention: 更快更省内存
            output = F.scaled_dot_product_attention(xq, xk, xv, dropout_p=self.dropout if self.training else 0.0, is_causal=True)
        else:
            # 标准注意力: Q @ K^T / sqrt(d_k)
            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
            
            # 因果掩码: 上三角设为 -inf，防止看到未来 token
            scores = scores + torch.triu(
                torch.full((seq_len, seq_len), float("-inf"), device=scores.device),
                diagonal=1
            ).unsqueeze(0).unsqueeze(0)

            # 可选的额外注意力掩码 (如 padding mask)
            if attention_mask is not None:
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9
                scores = scores + extended_attention_mask

            # Softmax 归一化得到注意力权重
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            scores = self.attn_dropout(scores)
            # 注意力加权求和: weights @ V
            output = scores @ xv

        # ════════════════════════════════════════════════════════════════════
        # 【步骤6: transpose + reshape 详解】合并多头
        # ════════════════════════════════════════════════════════════════════
        #
        # 【问题】注意力计算后，输出是多头格式，需要合并回原维度
        #
        # ────────────────────────────────────────────────────────────────────
        # 【当前 output 的形状】
        # ────────────────────────────────────────────────────────────────────
        #
        #   output: [batch, heads, seq_len, head_dim]
        #         = [2,     8,     10,      64]
        #
        # ────────────────────────────────────────────────────────────────────
        # 【第1步】output.transpose(1, 2)
        # ────────────────────────────────────────────────────────────────────
        #
        # transpose(1, 2) = 交换维度1和维度2
        #
        #   [batch, heads, seq_len, head_dim] → [batch, seq_len, heads, head_dim]
        #   [2,     8,     10,      64]       → [2,     10,      8,     64]
        #
        # 【为什么要交换？】
        # - 我们想把 heads 和 head_dim 合并
        # - 但 reshape 是按最后几个维度合并的
        # - 所以先把 heads 移到 head_dim 前面
        #
        # ────────────────────────────────────────────────────────────────────
        # 【第2步】.reshape(bsz, seq_len, -1)
        # ────────────────────────────────────────────────────────────────────
        #
        # reshape 把最后两个维度合并:
        #
        #   [batch, seq_len, heads, head_dim] → [batch, seq_len, heads×head_dim]
        #   [2,     10,      8,     64]       → [2,     10,      512]
        #                                              ↑
        #                                         8×64 = 512 = hidden_size
        #
        # 【-1 是什么意思？】
        # -1 表示"自动计算这个维度"
        # PyTorch 会根据总元素数自动算出 -1 应该是 512
        #
        # ────────────────────────────────────────────────────────────────────
        # 【完整流程图】
        # ────────────────────────────────────────────────────────────────────
        #
        #   注意力输出:       [batch, heads, seq, head_dim]
        #                     [2,     8,     10,  64]
        #                           ↓ transpose(1,2)
        #   交换后:           [batch, seq, heads, head_dim]
        #                     [2,     10,  8,     64]
        #                           ↓ reshape(..., -1)
        #   合并多头:         [batch, seq, hidden_size]
        #                     [2,     10,  512]
        #                           ↓ o_proj
        #   线性投影:         [batch, seq, hidden_size]
        #                     [2,     10,  512]
        #
        # 【o_proj 的作用】
        # 把 8 个头的输出"混合"成最终结果
        # W_o: [512, 512]，让不同头的信息相互交流
        #
        # ════════════════════════════════════════════════════════════════════
        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)
        output = self.resid_dropout(self.o_proj(output))
        return output, past_kv


class FeedForward(nn.Module):
    """
    前馈神经网络 (SwiGLU 变体)
    
    【传统 FFN】
    FFN(x) = ReLU(x @ W1 + b1) @ W2 + b2
    
    【SwiGLU FFN】(LLaMA 使用的变体)
    FFN(x) = (SiLU(x @ W_gate) ⊙ (x @ W_up)) @ W_down
    
    其中:
    - SiLU(x) = x * sigmoid(x)，也叫 Swish 激活函数
    - ⊙ 表示逐元素乘法 (Hadamard product)
    - W_gate 提供门控信号，W_up 提供内容
    
    【为什么使用 SwiGLU】
    1. SiLU 比 ReLU 更平滑，梯度更稳定
    2. 门控机制让网络更有选择性
    3. 实践中表现更好 (LLaMA、GPT-4 都在用)
    
    【维度变化】
    hidden_size -> intermediate_size -> hidden_size
    例如: 512 -> 1376 -> 512 (约 2.67 倍扩展)
    """
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        # 计算中间层维度，默认约 2.67 倍 hidden_size
        if config.intermediate_size is None:
            intermediate_size = int(config.hidden_size * 8 / 3)
            # 对齐到 64 的倍数，提高硬件计算效率
            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)
        
        # 三个投影层
        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)  # 门控
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)  # 下投影
        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)    # 上投影
        self.dropout = nn.Dropout(config.dropout)
        self.act_fn = ACT2FN[config.hidden_act]  # SiLU 激活函数

    def forward(self, x):
        # SwiGLU: down_proj(act(gate_proj(x)) * up_proj(x))
        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)))


class MoEGate(nn.Module):
    """
    MoE 门控网络 (Mixture of Experts Gate)
    
    【MoE 原理】
    MoE 包含多个"专家"(Expert)，每个专家是一个独立的 FFN。
    门控网络根据输入决定激活哪些专家。
    
    【工作流程】
    1. 门控网络计算每个专家的分数: scores = softmax(x @ W)
    2. 选择 top-k 个分数最高的专家
    3. 每个选中的专家处理输入，输出加权求和
    
    【优势】
    1. 稀疏激活: 每次只激活部分专家，计算量可控
    2. 大容量: 可以有很多专家，模型容量大
    3. 专业化: 不同专家学习不同类型的知识
    
    【辅助损失 (Auxiliary Loss)】
    为防止"专家崩塌"(某些专家过度使用，其他闲置):
    aux_loss = α * Σ(frequency_i * routing_prob_i)
    这鼓励负载均衡，让所有专家都被使用
    """
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.top_k = config.num_experts_per_tok      # 每个 token 激活的专家数
        self.n_routed_experts = config.n_routed_experts  # 总专家数

        self.scoring_func = config.scoring_func      # 评分函数 (softmax)
        self.alpha = config.aux_loss_alpha           # 辅助损失权重
        self.seq_aux = config.seq_aux                # 是否使用序列级辅助损失

        self.norm_topk_prob = config.norm_topk_prob  # 是否归一化 top-k 概率
        self.gating_dim = config.hidden_size
        # 门控权重矩阵: [n_experts, hidden_size]
        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim)))
        self.reset_parameters()

    def reset_parameters(self) -> None:
        """Kaiming 初始化"""
        init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, hidden_states):
        # ════════════════════════════════════════════════════════════════════
        # 【MoEGate forward 详解】这里在干嘛？怎么跟专家扯上关系？
        # ════════════════════════════════════════════════════════════════════
        #
        # 【这段代码的目的】
        # 给每个 token 计算: "应该去找哪几个专家？每个专家权重多少？"
        #
        # ────────────────────────────────────────────────────────────────────
        # 【第1步】获取输入形状
        # ────────────────────────────────────────────────────────────────────
        bsz, seq_len, h = hidden_states.shape
        #
        # 例子: hidden_states = [batch=2, seq=10, hidden=512]
        #       bsz=2, seq_len=10, h=512
        #
        # ────────────────────────────────────────────────────────────────────
        # 【第2步】view(-1, h) 展平 batch 和 seq 维度
        # ────────────────────────────────────────────────────────────────────
        hidden_states = hidden_states.view(-1, h)
        #
        # [2, 10, 512] → [20, 512]
        #
        # 为什么要展平？因为每个 token 独立选择专家！
        # 20 个 token，每个都要独立决定去找哪个专家
        #
        # ────────────────────────────────────────────────────────────────────
        # 【第3步】F.linear 计算每个专家的分数
        # ────────────────────────────────────────────────────────────────────
        logits = F.linear(hidden_states, self.weight, None)
        #
        # self.weight: [n_experts, hidden_size] = [4, 512]  (Router 的权重矩阵)
        #
        # F.linear(x, W) = x @ W.T
        # [20, 512] @ [512, 4] = [20, 4]
        #
        # 【输出含义】
        # logits[i, j] = 第 i 个 token 对第 j 个专家的"打分"
        #
        # 例如:
        #   logits[0] = [1.2, 0.5, 0.8, 0.3]  → token 0 对 4 个专家的分数
        #   logits[1] = [0.3, 1.5, 0.2, 0.9]  → token 1 对 4 个专家的分数
        #
        # 【这就是 Router！】
        # Router 通过 W_router 学习: "什么样的 token 应该去找什么专家"
        #
        # ════════════════════════════════════════════════════════════════════
        if self.scoring_func == 'softmax':
            scores = logits.softmax(dim=-1)
        else:
            raise NotImplementedError(f'insupportable scoring function for MoE gating: {self.scoring_func}')

        topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)

        if self.top_k > 1 and self.norm_topk_prob:
            denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20
            topk_weight = topk_weight / denominator

        # ════════════════════════════════════════════════════════════════════
        # 【aux_loss 辅助损失详解】这里在干嘛？不是应该选专家干活吗？
        # ════════════════════════════════════════════════════════════════════
        #
        # 【问题】为什么需要辅助损失？
        #
        # 如果没有辅助损失，Router 会"偷懒":
        #   - 总是把 token 发给同一个专家 (这个专家被训练得最好)
        #   - 其他专家闲置，浪费参数
        #   - 模型退化成单专家！
        #
        # 【辅助损失的目的】
        # 惩罚"负载不均衡"，逼迫 Router 均匀分配 token 给各专家
        #
        # ────────────────────────────────────────────────────────────────────
        # 【核心公式】
        # ────────────────────────────────────────────────────────────────────
        #
        # aux_loss = α × Σ(Pi × fi)
        #
        # 其中:
        #   - Pi = 专家 i 的平均路由概率 (Router 给的分数)
        #   - fi = 专家 i 实际处理的 token 比例
        #   - α = aux_loss_alpha，控制惩罚强度
        #
        # 【直觉】
        # 如果某专家既热门 (Pi 高) 又处理多 (fi 高)，乘积就大 → 惩罚
        # 这逼迫 Router 把 token 分散到不同专家
        #
        # 【例子】
        #   4 个专家，10 个 token，top_k=2
        #   理想: 每个专家处理 10×2/4 = 5 个 token
        #   不均衡: 专家0处理15个，专家1处理5个，专家2,3处理0个
        #          → aux_loss 变大 → 惩罚这种分配
        #
        # ────────────────────────────────────────────────────────────────────
        # 【代码解释】
        # ────────────────────────────────────────────────────────────────────
        if self.training and self.alpha > 0.0:
            scores_for_aux = scores
            aux_topk = self.top_k
            topk_idx_for_aux_loss = topk_idx.view(bsz, -1)
            if self.seq_aux:
                # 【序列级辅助损失】每个序列单独计算负载均衡
                scores_for_seq_aux = scores_for_aux.view(bsz, seq_len, -1)
                ce = torch.zeros(bsz, self.n_routed_experts, device=hidden_states.device)
                # scatter_add_: 统计每个专家被选中的次数
                ce.scatter_add_(1, topk_idx_for_aux_loss,
                                torch.ones(bsz, seq_len * aux_topk, device=hidden_states.device)).div_(
                    seq_len * aux_topk / self.n_routed_experts)
                # ce = 每个专家的实际负载比例
                # scores_for_seq_aux.mean(dim=1) = 每个专家的平均路由概率
                aux_loss = (ce * scores_for_seq_aux.mean(dim=1)).sum(dim=1).mean() * self.alpha
            else:
                # 【全局辅助损失】所有 token 一起计算
                mask_ce = F.one_hot(topk_idx_for_aux_loss.view(-1), num_classes=self.n_routed_experts)
                ce = mask_ce.float().mean(0)  # 每个专家被选中的比例
                Pi = scores_for_aux.mean(0)   # 每个专家的平均路由概率
                fi = ce * self.n_routed_experts  # 归一化后的负载
                aux_loss = (Pi * fi).sum() * self.alpha
        #
        # 【注意】这里只是计算 aux_loss，不是选专家！
        # 选专家在上面的 topk 已经完成了
        # aux_loss 会加到总损失里，帮助训练 Router 更均衡
        #
        # ════════════════════════════════════════════════════════════════════
        else:
            aux_loss = scores.new_zeros(1).squeeze()
        return topk_idx, topk_weight, aux_loss


class MOEFeedForward(nn.Module):
    """
    MoE 前馈层 (Mixture of Experts Feed-Forward)
    
    【结构】
    - n_routed_experts 个路由专家 (稀疏激活，每次只用 top-k 个)
    - n_shared_experts 个共享专家 (总是激活，所有 token 都经过)
    - MoEGate 门控网络 (决定每个 token 用哪些专家)
    
    【工作流程】
    输入 x -> 门控选择 top-k 专家 -> 各专家处理 -> 加权求和 -> 加上共享专家输出
    
    【例子】
    - 4 个路由专家, top-k=2: 每个 token 激活 2 个专家
    - 1 个共享专家: 所有 token 都会经过
    - 总计算量约等于 3 个 FFN (2 + 1)
    - 但模型容量是 5 个 FFN (4 + 1)
    """
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        # 创建路由专家列表
        self.experts = nn.ModuleList([
            FeedForward(config)
            for _ in range(config.n_routed_experts)
        ])
        # 门控网络
        self.gate = MoEGate(config)
        # 共享专家 (可选)
        if config.n_shared_experts > 0:
            self.shared_experts = nn.ModuleList([
                FeedForward(config)
                for _ in range(config.n_shared_experts)
            ])

    def forward(self, x):
        identity = x
        orig_shape = x.shape
        bsz, seq_len, _ = x.shape
        # 使用门控机制选择专家
        topk_idx, topk_weight, aux_loss = self.gate(x)
        x = x.view(-1, x.shape[-1])
        flat_topk_idx = topk_idx.view(-1)
        # ════════════════════════════════════════════════════════════════════
        # 【训练时的 MoE 计算】共享专家什么时候训练？
        # ════════════════════════════════════════════════════════════════════
        #
        # 【训练流程】
        #
        # 1. 每个 token 复制 top_k 份 (因为要送给 top_k 个专家)
        # 2. 每个专家处理分配给它的 token
        # 3. 加权求和得到路由专家的输出
        # 4. 最后加上共享专家的输出
        #
        # ────────────────────────────────────────────────────────────────────
        if self.training:
            # repeat_interleave: 每个 token 复制 top_k 份
            # [20, 512] → [40, 512] (top_k=2)
            x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0)
            y = torch.empty_like(x, dtype=x.dtype)
            
            # 遍历每个专家，处理分配给它的 token
            for i, expert in enumerate(self.experts):
                # flat_topk_idx == i: 找出所有被分配给专家 i 的 token
                expert_out = expert(x[flat_topk_idx == i])
                if expert_out.shape[0] > 0: 
                    y[flat_topk_idx == i] = expert_out.to(y.dtype)
                else: 
                    # 即使没有 token，也要让梯度流过 (防止专家参数不更新)
                    y[flat_topk_idx == i] = expert_out.to(y.dtype) + 0 * sum(p.sum() for p in expert.parameters())
            
            # 加权求和: 每个 token 的 top_k 个专家输出加权合并
            y = (y.view(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)).sum(dim=1)
            y = y.view(*orig_shape)
        else:
            # 推理时用优化版本 (见下面 moe_infer 的解释)
            y = self.moe_infer(x, flat_topk_idx, topk_weight.view(-1, 1)).view(*orig_shape)
        
        # ────────────────────────────────────────────────────────────────────
        # 【共享专家】在 MoE 之后运算，而且训练时也训练！
        # ────────────────────────────────────────────────────────────────────
        #
        # 【疑问】为什么训练的时候不训练共享专家？
        # 【回答】共享专家也在训练！看这段代码:
        #
        if self.config.n_shared_experts > 0:
            for expert in self.shared_experts:
                y = y + expert(identity)
        #
        # 【解释】
        # - identity = 原始输入 x (在 forward 开头保存的)
        # - expert(identity) 会计算共享专家的输出
        # - 这个计算有梯度流过，所以共享专家的参数会更新！
        #
        # 【训练时】
        # - 路由专家: 只有被选中的 top_k 个会更新 (稀疏)
        # - 共享专家: 每次都更新，因为每个 token 都经过 (稠密)
        #
        # 【为什么共享专家放在最后？】
        # 这是 DeepSeek-MoE 的设计:
        #   output = 路由专家加权输出 + 共享专家输出
        #
        # 共享专家捕获"通用知识"，路由专家捕获"专业知识"
        #
        # ════════════════════════════════════════════════════════════════════
        self.aux_loss = aux_loss
        return y

    # ════════════════════════════════════════════════════════════════════
    # 【moe_infer 推理优化详解】每个专家处理不同部分？
    # ════════════════════════════════════════════════════════════════════
    #
    # 【问题】为什么不像训练那样遍历所有 token？
    #
    # 训练时: 遍历每个专家，用布尔索引找 token → 效率低
    # 推理时: 先排序，让同一专家的 token 连续 → 批量处理更快
    #
    # ────────────────────────────────────────────────────────────────────
    # 【核心思想】把 token 按专家分组，批量处理
    # ────────────────────────────────────────────────────────────────────
    #
    # 假设 10 个 token，top_k=2，4 个专家:
    #
    # 原始分配 (乱序):
    #   token 0 → 专家 [1, 3]
    #   token 1 → 专家 [0, 2]
    #   token 2 → 专家 [1, 0]
    #   ...
    #
    # 排序后 (按专家分组):
    #   专家 0: [token 1, token 2, token 5, ...]
    #   专家 1: [token 0, token 2, token 7, ...]
    #   专家 2: [token 1, token 4, ...]
    #   专家 3: [token 0, token 6, ...]
    #
    # 这样每个专家可以一次性处理所有属于它的 token！
    #
    # ────────────────────────────────────────────────────────────────────
    @torch.no_grad()
    def moe_infer(self, x, flat_expert_indices, flat_expert_weights):
        expert_cache = torch.zeros_like(x)  # 累积每个 token 的输出
        
        # argsort: 按专家 ID 排序，返回排序后的索引
        idxs = flat_expert_indices.argsort()
        
        # bincount: 统计每个专家有多少 token
        # cumsum: 累加得到每个专家的结束位置
        tokens_per_expert = flat_expert_indices.bincount().cpu().numpy().cumsum(0)
        
        # 还原原始 token 索引 (因为 top_k 复制了 token)
        token_idxs = idxs // self.config.num_experts_per_tok
        
        # ────────────────────────────────────────────────────────────────
        # 【具体例子】
        # ────────────────────────────────────────────────────────────────
        # tokens_per_expert = [6, 15, 20, 26]
        # 含义:
        #   - 专家 0: 处理 6 个 (索引 0~5)
        #   - 专家 1: 处理 9 个 (索引 6~14，因为 15-6=9)
        #   - 专家 2: 处理 5 个 (索引 15~19)
        #   - 专家 3: 处理 6 个 (索引 20~25)
        #
        # token_idxs = [3, 7, 19, 21, 24, 25, 4, 5, 6, 10, 11, 12, ...]
        # 含义:
        #   - token_idxs[:6] = [3,7,19,21,24,25] 是专家0处理的token
        #   - token_idxs[6:15] = [4,5,6,10,11,...] 是专家1处理的token
        #   - 依此类推...
        #
        # 【注意】同一个 token 可能出现在多个专家的列表中！
        # 因为 top_k=2，每个 token 被 2 个专家处理
        # ────────────────────────────────────────────────────────────────
        # 当tokens_per_expert = [6, 15, 20, 26]，tokens_per_expert.shape[0]即为专家数量（此时为4）
        # 且token_idxs = [3, 7, 19, 21, 24, 25,  4,  5,  6, 10, 11, 12...] 时
        # 意味token_idxs[:6] -> [3, 7, 19, 21, 24, 25]这6个位置属于专家0处理的token（每个token有可能被多个专家处理，这取决于num_experts_per_tok）
        # 接下来9个位置token_idxs[6:15] -> [4,  5,  6, 10, 11, 12...]属于专家1处理的token...依此类推
        for i, end_idx in enumerate(tokens_per_expert):
            start_idx = 0 if i == 0 else tokens_per_expert[i - 1]
            if start_idx == end_idx:
                continue  # 这个专家没有分配到任何 token
            expert = self.experts[i]
            exp_token_idx = token_idxs[start_idx:end_idx]  # 这个专家处理的 token 索引
            expert_tokens = x[exp_token_idx]               # 取出这些 token
            expert_out = expert(expert_tokens).to(expert_cache.dtype)  # 批量计算！
            expert_out.mul_(flat_expert_weights[idxs[start_idx:end_idx]])  # 乘权重
            # scatter_add_: 把结果累加回对应位置 (因为一个 token 可能被多个专家处理)
            expert_cache.scatter_add_(0, exp_token_idx.view(-1, 1).repeat(1, x.shape[-1]), expert_out)
        #
        # 【最终效果】
        # expert_cache[i] = Σ(weight_j × expert_j(token_i))
        # 即每个 token 的输出是其 top_k 个专家输出的加权和
        #
        # ════════════════════════════════════════════════════════════════════
        return expert_cache


class MiniMindBlock(nn.Module):
    """
    Transformer 基本块 (Pre-LN 结构)
    
    【结构图】
    输入 x
        │
        ├──────────────────────┐
        ▼                      │ (残差连接)
    RMSNorm                    │
        │                      │
    Self-Attention             │
        │                      │
        ├──────────────────────┘
        ▼
    相加 (x + attention_output)
        │
        ├──────────────────────┐
        ▼                      │ (残差连接)
    RMSNorm                    │
        │                      │
    FFN/MoE                    │
        │                      │
        ├──────────────────────┘
        ▼
    相加 (x + ffn_output)
        │
        ▼
    输出
    
    【Pre-LN vs Post-LN】
    - Post-LN (原始 Transformer): Norm 在残差连接之后
    - Pre-LN (现代做法): Norm 在残差连接之前，训练更稳定
    
    【残差连接的作用】
    1. 缓解梯度消失: 梯度可以直接流过
    2. 信息高速公路: 浅层特征可以直接传到深层
    3. 使深层网络可训练
    """
    def __init__(self, layer_id: int, config: MiniMindConfig):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.hidden_size = config.hidden_size
        self.head_dim = config.hidden_size // config.num_attention_heads
        
        # 自注意力层
        self.self_attn = Attention(config)
        self.layer_id = layer_id
        
        # 归一化层 (Pre-LN: 在 attention/ffn 之前进行归一化)
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        
        # FFN 或 MoE
        self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)

    def forward(self, hidden_states, position_embeddings, past_key_value=None, use_cache=False, attention_mask=None):
        # 第一个残差块: Self-Attention
        residual = hidden_states
        hidden_states, present_key_value = self.self_attn(
            self.input_layernorm(hidden_states), position_embeddings,
            past_key_value, use_cache, attention_mask
        )
        hidden_states = hidden_states + residual  # 残差连接
        
        # 第二个残差块: FFN/MoE
        hidden_states = hidden_states + self.mlp(self.post_attention_layernorm(hidden_states))
        
        return hidden_states, present_key_value


class MiniMindModel(nn.Module):
    """
    MiniMind 基础模型 (不含 LM Head)
    
    【整体结构】
    Token IDs → Embedding → [Transformer Block × N] → Final Norm → Hidden States
    
    【组件说明】
    1. embed_tokens: 词嵌入层，将 token ID 映射为向量
       - 输入: [batch, seq_len] 的整数 ID
       - 输出: [batch, seq_len, hidden_size] 的向量
    
    2. layers: N 个 Transformer Block 堆叠
       - 每层包含: Attention + FFN/MoE
       - 深度决定模型的表达能力
    
    3. norm: 最终的 RMSNorm 归一化
       - 在输出前进行归一化，稳定输出分布
    
    4. freqs_cos/freqs_sin: 预计算的 RoPE 位置编码
       - 注册为 buffer，不参与训练但会保存
    """
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.vocab_size, self.num_hidden_layers = config.vocab_size, config.num_hidden_layers
        
        # 词嵌入: 将 token ID 映射为 hidden_size 维向量
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.dropout = nn.Dropout(config.dropout)
        
        # 堆叠 N 个 Transformer Block
        self.layers = nn.ModuleList([MiniMindBlock(l, config) for l in range(self.num_hidden_layers)])
        
        # 最终归一化层
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        # 预计算 RoPE 频率并注册为 buffer
        freqs_cos, freqs_sin = precompute_freqs_cis(dim=config.hidden_size // config.num_attention_heads,
                                                    end=config.max_position_embeddings, rope_base=config.rope_theta,
                                                    rope_scaling=config.rope_scaling)
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                **kwargs):
        batch_size, seq_length = input_ids.shape
        if hasattr(past_key_values, 'layers'): past_key_values = None
        past_key_values = past_key_values or [None] * len(self.layers)
        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0

        hidden_states = self.dropout(self.embed_tokens(input_ids))

        position_embeddings = (
            self.freqs_cos[start_pos:start_pos + seq_length],
            self.freqs_sin[start_pos:start_pos + seq_length]
        )

        # ════════════════════════════════════════════════════════════════════
        # 【Transformer 层循环详解】这段代码太高级了？逐行解释！
        # ════════════════════════════════════════════════════════════════════
        #
        # 【整体目的】
        # 让输入依次通过 N 个 Transformer Block，逐层提取特征
        #
        # ────────────────────────────────────────────────────────────────────
        # 【代码结构】
        # ────────────────────────────────────────────────────────────────────
        #
        # presents = []  ← 用于存储每层的 KV Cache
        #
        # for layer_idx, (layer, past_key_value) in enumerate(zip(...)):
        #     │
        #     ├─ layer_idx: 当前是第几层 (0, 1, 2, ...)
        #     ├─ layer: 当前的 Transformer Block
        #     └─ past_key_value: 这一层之前的 KV Cache
        #
        # ────────────────────────────────────────────────────────────────────
        # 【zip(self.layers, past_key_values) 做了什么？】
        # ────────────────────────────────────────────────────────────────────
        #
        # self.layers = [Block_0, Block_1, Block_2, ..., Block_N-1]
        # past_key_values = [cache_0, cache_1, cache_2, ..., cache_N-1]
        #
        # zip 把它们配对:
        #   [(Block_0, cache_0), (Block_1, cache_1), ...]
        #
        # enumerate 加上索引:
        #   [(0, (Block_0, cache_0)), (1, (Block_1, cache_1)), ...]
        #
        # ────────────────────────────────────────────────────────────────────
        # 【每次循环做了什么？】
        # ────────────────────────────────────────────────────────────────────
        #
        # hidden_states, present = layer(hidden_states, ...)
        #
        # 1. 把当前 hidden_states 送入这一层
        # 2. 层内部做: Attention + FFN (或 MoE)
        # 3. 返回:
        #    - hidden_states: 更新后的隐藏状态 (送入下一层)
        #    - present: 这一层的新 KV Cache (用于推理)
        #
        # ────────────────────────────────────────────────────────────────────
        # 【数据流动】
        # ────────────────────────────────────────────────────────────────────
        #
        # 输入: "明天 天气 是 晴天"
        #
        #   Embedding 后: [batch, 4, 512]
        #        │
        #        ▼
        #   Layer 0: Attention + FFN → [batch, 4, 512]
        #        │
        #        ▼
        #   Layer 1: Attention + FFN → [batch, 4, 512]
        #        │
        #        ▼
        #   ...
        #        │
        #        ▼
        #   Layer N-1: Attention + FFN → [batch, 4, 512]
        #        │
        #        ▼
        #   Final Norm → [batch, 4, 512]
        #
        # 【注意】维度始终是 [batch, seq, 512]，但内容越来越"高级"
        # - 浅层: 捕获语法、词汇信息
        # - 深层: 捕获语义、推理信息
        #
        # ════════════════════════════════════════════════════════════════════
        #
        # ════════════════════════════════════════════════════════════════════
        # 【presents = [] 不是清空 cache 吗？为什么不会丢失？】
        # ════════════════════════════════════════════════════════════════════
        #
        # 【你的疑问】
        # "每次 forward 都 presents = []，那之前的 cache 不就没了？"
        #
        # 【关键】presents 和 past_key_values 是两个不同的东西！
        #
        # ────────────────────────────────────────────────────────────────────
        # 【变量关系】
        # ────────────────────────────────────────────────────────────────────
        #
        #   past_key_values: 输入参数，是上一次调用传进来的旧 cache
        #   presents:        输出结果，是本次调用产生的新 cache
        #
        # ────────────────────────────────────────────────────────────────────
        # 【推理时的完整流程】
        # ────────────────────────────────────────────────────────────────────
        #
        # 【第1次调用】输入 "今天"
        #
        #   forward(input_ids="今天", past_key_values=None)
        #           │
        #           ├─ presents = []  ← 初始化空列表
        #           │
        #           ├─ Layer 0: past=None → 计算 K,V → present=[K0,V0]
        #           │           presents.append([K0,V0])
        #           │
        #           ├─ Layer 1: past=None → 计算 K,V → present=[K1,V1]
        #           │           presents.append([K1,V1])
        #           │
        #           └─ return ..., presents=[[K0,V0], [K1,V1]]
        #                              ↓
        #                    调用者保存这个 presents
        #
        # 【第2次调用】输入 "天气" (只有新 token)
        #
        #   forward(input_ids="天气", past_key_values=[[K0,V0], [K1,V1]])
        #           │                        ↑
        #           │               上次返回的 presents 传回来了！
        #           │
        #           ├─ presents = []  ← 又初始化空列表
        #           │
        #           ├─ Layer 0: past=[K0,V0] → 计算新 K',V'
        #           │           → concat: [K0,K'] → present=[K0+K', V0+V']
        #           │           presents.append(拼接后的新cache)
        #           │
        #           └─ return ..., presents=[[K0+K', V0+V'], ...]
        #
        # ────────────────────────────────────────────────────────────────────
        # 【图示】
        # ────────────────────────────────────────────────────────────────────
        #
        #   调用1                     调用2                     调用3
        #     │                         │                         │
        #     ▼                         ▼                         ▼
        #   forward()               forward()                forward()
        #     │                         │                         │
        #     │ past=None               │ past=presents1          │ past=presents2
        #     │                         │                         │
        #     ▼                         ▼                         ▼
        #   presents1 ──────────→   presents2 ──────────→    presents3
        #   [token0的KV]           [token0+1的KV]          [token0+1+2的KV]
        #
        # 【关键理解】
        # - presents = [] 只是创建一个新容器来收集本次结果
        # - 旧的 cache 通过 past_key_values 参数传进来
        # - 每层把 past 和新计算的 KV 拼接后放入 presents
        # - 调用者 (generate 函数) 负责把 presents 传给下一次调用
        #
        # ════════════════════════════════════════════════════════════════════
        presents = []
        for layer_idx, (layer, past_key_value) in enumerate(zip(self.layers, past_key_values)):
            hidden_states, present = layer(
                hidden_states,
                position_embeddings,
                past_key_value=past_key_value,  # ← 旧 cache 从这里传入
                use_cache=use_cache,
                attention_mask=attention_mask
            )
            presents.append(present)  # ← 新 cache (包含旧的) 收集到这里

        hidden_states = self.norm(hidden_states)

        aux_loss = sum([l.mlp.aux_loss for l in self.layers if isinstance(l.mlp, MOEFeedForward)], hidden_states.new_zeros(1).squeeze())
        return hidden_states, presents, aux_loss


# ════════════════════════════════════════════════════════════════════════════
# 【MiniMindForCausalLM vs MiniMindModel】它们的关系是什么？
# ════════════════════════════════════════════════════════════════════════════
#
# 【简单回答】
# - MiniMindModel: "大脑"，理解文本
# - MiniMindForCausalLM: "大脑" + "嘴巴"，理解文本 + 生成文本
#
# ────────────────────────────────────────────────────────────────────────────
# 【结构对比】
# ────────────────────────────────────────────────────────────────────────────
#
# ┌─────────────────────────────────────────────────────────────────────────┐
# │ MiniMindModel (基础模型)                                                │
# │                                                                         │
# │   Token IDs → Embedding → [Transformer ×N] → Norm → Hidden States      │
# │                                                       ↑                 │
# │                                              输出: [batch, seq, 512]    │
# │                                              语义向量，但不是文字       │
# └─────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────┐
# │ MiniMindForCausalLM (因果语言模型)                                      │
# │                                                                         │
# │   Token IDs → MiniMindModel → Hidden States → LM Head → Logits         │
# │                    ↑                             ↑                      │
# │               包含基础模型                  [batch, seq, vocab_size]    │
# │                                             每个位置预测下一个词       │
# └─────────────────────────────────────────────────────────────────────────┘
#
# ────────────────────────────────────────────────────────────────────────────
# 【这是 Reasoning 模型吗？哪里体现？】
# ────────────────────────────────────────────────────────────────────────────
#
# 【答案】这不是专门的 Reasoning 模型！
#
# 这是一个标准的因果语言模型 (Causal LM):
#   - 只做"预测下一个词"
#   - 没有专门的推理模块
#   - 没有 Chain-of-Thought 强制机制
#
# 【什么是真正的 Reasoning 模型？】
#
# 1. OpenAI o1 / DeepSeek-R1:
#    - 有专门的"思考"阶段
#    - 模型被训练成先输出 <think>推理过程</think>
#    - 然后再输出答案
#
# 2. 本项目的 Reasoning 能力来自:
#    - 用 DeepSeek-R1 蒸馏的数据训练
#    - 模型学会了 R1 的"思考风格"
#    - 但架构本身没有特殊设计
#
# ────────────────────────────────────────────────────────────────────────────
# 【当前项目中 Reasoning 是怎么训练的？】
# ────────────────────────────────────────────────────────────────────────────
#
# 【方法】知识蒸馏 (Knowledge Distillation)
#
# 训练文件: trainer/train_distillation.py
#
# 【流程】
# 1. 用大模型 (DeepSeek-R1) 生成带推理过程的回答
#    问: "123 + 456 = ?"
#    R1 回答: "<think>首先，123+456...个位3+6=9...十位2+5=7...</think> 579"
#
# 2. 用这些 (问题, R1回答) 对训练 MiniMind
#    MiniMind 学会模仿 R1 的思考方式
#
# 3. 训练后，MiniMind 也会输出类似风格:
#    MiniMind: "让我想想...123+456，先算个位...答案是579"
#
# 【本质】
# - 架构没变，还是普通 Transformer
# - 只是训练数据包含了"思考过程"
# - 模型学会了"先思考再回答"的模式
#
# 【区别于真正的 Reasoning 模型】
# - o1/R1: 架构上有专门设计，强制执行推理
# - MiniMind: 只是模仿了推理风格，没有架构保证
#
# ════════════════════════════════════════════════════════════════════════════

class MiniMindForCausalLM(PreTrainedModel, GenerationMixin):
    """
    MiniMind 因果语言模型 (完整模型)
    
    【结构】
    Token IDs → MiniMindModel → Hidden States → LM Head → Logits
    
    【因果语言模型 (Causal LM)】
    自回归模型，从左到右预测下一个 token:
    P(x₁, x₂, ..., xₙ) = P(x₁) × P(x₂|x₁) × ... × P(xₙ|x₁...xₙ₋₁)
    
    【LM Head 的作用】
    将隐藏状态映射回词表:
    - 输入: [batch, seq_len, hidden_size]
    - 输出: [batch, seq_len, vocab_size]
    - 每个位置输出词表大小的 logits，表示下一个 token 的概率分布
    
    【权重共享 (Weight Tying)】
    LM Head 和 Embedding 共享权重:
    - 减少参数量 (vocab_size × hidden_size)
    - 使输入输出表示一致
    - 在小模型上效果尤其好
    
    【继承关系】
    - PreTrainedModel: HuggingFace 预训练模型基类，提供保存/加载等功能
    - GenerationMixin: 提供 generate() 方法用于文本生成
    """
    config_class = MiniMindConfig

    def __init__(self, config: MiniMindConfig = None):
        self.config = config or MiniMindConfig()
        super().__init__(self.config)
        
        # 基础模型
        self.model = MiniMindModel(self.config)
        
        # 语言模型头: hidden_size → vocab_size
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)
        
        # 权重共享: embedding 和 lm_head 使用相同的权重矩阵
        self.model.embed_tokens.weight = self.lm_head.weight

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                logits_to_keep: Union[int, torch.Tensor] = 0,
                **args):
        """
        前向传播
        
        【参数】
        - input_ids: 输入 token ID [batch, seq_len]
        - attention_mask: 注意力掩码 (1=有效, 0=padding)
        - past_key_values: KV Cache，用于增量解码
        - use_cache: 是否返回 KV Cache
        - logits_to_keep: 只保留最后 N 个位置的 logits (节省内存)
        
        【返回】
        CausalLMOutputWithPast 对象，包含:
        - logits: 每个位置的 token 概率分布 [batch, seq_len, vocab_size]
        - past_key_values: KV Cache
        - hidden_states: 隐藏状态
        - aux_loss: MoE 辅助损失
        
        【训练 vs 推理】
        训练时: logits_to_keep=0，返回所有位置的 logits
        推理时: logits_to_keep=1，只需要最后一个位置的 logits
        """
        # 通过基础模型得到隐藏状态
        hidden_states, past_key_values, aux_loss = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            **args
        )
        
        # 只保留需要的位置 (用于高效推理)
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        
        # LM Head: hidden_states → logits
        logits = self.lm_head(hidden_states[:, slice_indices, :])
        
        # 构建输出对象
        output = CausalLMOutputWithPast(logits=logits, past_key_values=past_key_values, hidden_states=hidden_states)
        output.aux_loss = aux_loss
        return output
