"""
================================================================================
                    MiniMind Web Demo (Gradio ç•Œé¢)
================================================================================

ã€ä»€ä¹ˆæ˜¯è¿™ä¸ªè„šæœ¬ã€‘
è¿™æ˜¯ä¸€ä¸ªåŸºäº Gradio çš„ Web æ¼”ç¤ºç•Œé¢:
- æä¾›ç›´è§‚çš„èŠå¤©ç•Œé¢
- æ”¯æŒå‚æ•°è°ƒèŠ‚
- å®æ—¶æµå¼è¾“å‡º

ã€Gradio ç®€ä»‹ã€‘
Gradio æ˜¯ä¸€ä¸ªå¿«é€Ÿåˆ›å»º ML æ¼”ç¤ºç•Œé¢çš„åº“:
- å‡ è¡Œä»£ç åˆ›å»º Web UI
- è‡ªåŠ¨å¤„ç†å‰åç«¯é€šä¿¡
- æ”¯æŒå„ç§è¾“å…¥è¾“å‡ºç»„ä»¶

ã€åŠŸèƒ½ç‰¹ç‚¹ã€‘
1. èŠå¤©ç•Œé¢: ç±»ä¼¼ ChatGPT çš„å¯¹è¯æ¡†
2. å‚æ•°è°ƒèŠ‚: æ¸©åº¦ã€top_pã€max_tokens
3. æµå¼è¾“å‡º: å®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹
4. æ¸…é™¤å†å²: ä¸€é”®æ¸…é™¤å¯¹è¯

ã€ä½¿ç”¨æ–¹æ³•ã€‘
å¯åŠ¨æœåŠ¡:
    python web_demo.py --port 7860 --model_weight full_sft

ç„¶ååœ¨æµè§ˆå™¨è®¿é—®: http://localhost:7860

ã€ç•Œé¢å¸ƒå±€ã€‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 MiniMind Chat           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ç”¨æˆ·: ä½ å¥½                      â”‚    â”‚
â”‚  â”‚ åŠ©æ‰‹: ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„ï¼Ÿ  â”‚    â”‚
â”‚  â”‚ ...                              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  è¾“å…¥æ¡†: [________________] [å‘é€]      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  å‚æ•°: Temperature [0.7] Max Tokens [512]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

import os
import sys

# å°†çˆ¶ç›®å½•æ·»åŠ åˆ°è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import argparse
import torch
import gradio as gr
from transformers import AutoTokenizer

from model.model_minimind import MiniMindForCausalLM, MiniMindConfig
from model.model_lora import apply_lora, load_lora


# ==================== å…¨å±€å˜é‡ ====================
model = None
tokenizer = None
args = None


def generate_response(message: str, history: list, temperature: float = 0.7, 
                      top_p: float = 0.9, max_tokens: int = 512):
    """
    ç”Ÿæˆå¯¹è¯å›å¤ (æµå¼)
    
    ã€æµç¨‹ã€‘
    1. å°†å¯¹è¯å†å²å’Œå½“å‰æ¶ˆæ¯ç»„è£…æˆ messages åˆ—è¡¨
    2. ä½¿ç”¨ chat_template æ ¼å¼åŒ–
    3. é€ token ç”Ÿæˆå¹¶ yield
    
    ã€å‚æ•°ã€‘
    - message: ç”¨æˆ·å½“å‰è¾“å…¥
    - history: å¯¹è¯å†å² [(user_msg, bot_msg), ...]
    - temperature: é‡‡æ ·æ¸©åº¦
    - top_p: nucleus é‡‡æ ·å‚æ•°
    - max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
    
    ã€Yieldsã€‘
    - é€æ­¥ç”Ÿæˆçš„å›å¤æ–‡æœ¬
    """
    # ==================== 1. ç»„è£…å¯¹è¯å†å² ====================
    messages = []
    
    # æ·»åŠ ç³»ç»Ÿæç¤º (å¯é€‰)
    # messages.append({"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"})
    
    # æ·»åŠ å†å²å¯¹è¯
    for user_msg, bot_msg in history:
        messages.append({"role": "user", "content": user_msg})
        if bot_msg:  # bot_msg å¯èƒ½ä¸º None (æ­£åœ¨ç”Ÿæˆæ—¶)
            messages.append({"role": "assistant", "content": bot_msg})
    
    # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
    messages.append({"role": "user", "content": message})
    
    # ==================== 2. ä½¿ç”¨ chat_template æ ¼å¼åŒ– ====================
    # add_generation_prompt=True ä¼šæ·»åŠ  <|im_start|>assistant\n
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # ==================== 3. Token åŒ– ====================
    input_ids = tokenizer(
        prompt, 
        return_tensors="pt", 
        add_special_tokens=False
    ).input_ids.to(args.device)
    
    # ==================== 4. æµå¼ç”Ÿæˆ ====================
    generated_text = ""
    past_key_values = None
    current_input_ids = input_ids
    
    for _ in range(max_tokens):
        with torch.no_grad():
            # å‰å‘ä¼ æ’­
            outputs = model(
                current_input_ids,
                past_key_values=past_key_values,
                use_cache=True  # ä½¿ç”¨ KV ç¼“å­˜åŠ é€Ÿ
            )
            
            # è·å–æœ€åä¸€ä¸ªä½ç½®çš„ logits
            next_token_logits = outputs.logits[:, -1, :]
            
            # é‡‡æ ·ç­–ç•¥
            if temperature > 0:
                # åº”ç”¨æ¸©åº¦
                next_token_logits = next_token_logits / temperature
                
                # Top-p (nucleus) é‡‡æ ·
                # åªä¿ç•™ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° top_p çš„ token
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                
                # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ top_p çš„ä½ç½®
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                
                # å°†è¿™äº›ä½ç½®çš„ logits è®¾ä¸ºè´Ÿæ— ç©·
                indices_to_remove = sorted_indices_to_remove.scatter(
                    1, sorted_indices, sorted_indices_to_remove
                )
                next_token_logits[indices_to_remove] = float('-inf')
                
                # ä»ä¿®æ”¹åçš„åˆ†å¸ƒä¸­é‡‡æ ·
                probs = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
            else:
                # è´ªå©ªè§£ç  (temperature=0)
                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸ token
            if next_token.item() == tokenizer.eos_token_id:
                break
            
            # è§£ç å½“å‰ token
            token_text = tokenizer.decode(next_token[0], skip_special_tokens=True)
            generated_text += token_text
            
            # Yield å½“å‰ç”Ÿæˆçš„æ–‡æœ¬ (æµå¼è¾“å‡º)
            yield generated_text
            
            # æ›´æ–°çŠ¶æ€
            current_input_ids = next_token
            past_key_values = outputs.past_key_values
    
    # è¿”å›æœ€ç»ˆç»“æœ
    yield generated_text


def clear_history():
    """æ¸…é™¤å¯¹è¯å†å²"""
    return [], ""


def init_model_from_args(args_input):
    """
    æ ¹æ®å‘½ä»¤è¡Œå‚æ•°åˆå§‹åŒ–æ¨¡å‹
    
    ã€åŠ è½½æµç¨‹ã€‘
    1. åˆ›å»ºæ¨¡å‹é…ç½®
    2. åŠ è½½ tokenizer
    3. åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½æƒé‡
    4. (å¯é€‰) åº”ç”¨ LoRA
    5. è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    """
    global model, tokenizer, args
    args = args_input
    
    # 1. åˆ›å»ºæ¨¡å‹é…ç½®
    lm_config = MiniMindConfig(
        hidden_size=args.hidden_size,
        num_hidden_layers=args.num_hidden_layers,
        use_moe=bool(args.use_moe)
    )
    
    # 2. åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained('../model', trust_remote_code=True)
    
    # 3. åˆå§‹åŒ–æ¨¡å‹
    model = MiniMindForCausalLM(lm_config)
    
    # 4. åŠ è½½æƒé‡
    moe_suffix = '_moe' if lm_config.use_moe else ''
    weight_path = f'../out/{args.model_weight}_{lm_config.hidden_size}{moe_suffix}.pth'
    state_dict = torch.load(weight_path, map_location=args.device)
    model.load_state_dict(state_dict, strict=False)
    
    # 5. (å¯é€‰) åŠ è½½ LoRA
    if args.lora_weight:
        apply_lora(model)
        lora_path = f'../out/lora/{args.lora_weight}_{lm_config.hidden_size}.pth'
        load_lora(model, lora_path)
        print(f"LoRA æƒé‡å·²åŠ è½½: {lora_path}")
    
    # 6. ç§»åˆ°è®¾å¤‡å¹¶è®¾ä¸ºè¯„ä¼°æ¨¡å¼
    model = model.to(args.device)
    model.eval()
    
    print(f"æ¨¡å‹åŠ è½½å®Œæˆ: {weight_path}")


def create_demo():
    """
    åˆ›å»º Gradio æ¼”ç¤ºç•Œé¢
    
    ã€ç•Œé¢ç»„ä»¶ã€‘
    - Chatbot: å¯¹è¯æ˜¾ç¤ºåŒºåŸŸ
    - Textbox: ç”¨æˆ·è¾“å…¥æ¡†
    - Button: å‘é€å’Œæ¸…é™¤æŒ‰é’®
    - Slider: å‚æ•°è°ƒèŠ‚æ»‘å—
    """
    
    # åˆ›å»ºç•Œé¢
    with gr.Blocks(title="MiniMind Chat", theme=gr.themes.Soft()) as demo:
        # æ ‡é¢˜
        gr.Markdown("""
        # ğŸ§  MiniMind Chat
        è½»é‡çº§ä¸­æ–‡è¯­è¨€æ¨¡å‹å¯¹è¯æ¼”ç¤º
        """)
        
        # å¯¹è¯åŒºåŸŸ
        chatbot = gr.Chatbot(
            height=500,
            bubble_full_width=False,
            avatar_images=(None, "https://em-content.zobj.net/source/apple/354/robot_1f916.png")
        )
        
        # è¾“å…¥åŒºåŸŸ
        with gr.Row():
            msg = gr.Textbox(
                placeholder="è¾“å…¥ä½ çš„é—®é¢˜...",
                show_label=False,
                container=False,
                scale=8
            )
            submit_btn = gr.Button("å‘é€", variant="primary", scale=1)
            clear_btn = gr.Button("æ¸…é™¤", scale=1)
        
        # å‚æ•°è°ƒèŠ‚åŒºåŸŸ
        with gr.Accordion("âš™ï¸ é«˜çº§å‚æ•°", open=False):
            with gr.Row():
                temperature = gr.Slider(
                    minimum=0.0, 
                    maximum=2.0, 
                    value=0.7, 
                    step=0.1, 
                    label="Temperature (æ¸©åº¦)",
                    info="è¶Šé«˜è¶Šéšæœºï¼Œè¶Šä½è¶Šç¡®å®š"
                )
                top_p = gr.Slider(
                    minimum=0.0, 
                    maximum=1.0, 
                    value=0.9, 
                    step=0.1, 
                    label="Top-P",
                    info="Nucleus é‡‡æ ·å‚æ•°"
                )
                max_tokens = gr.Slider(
                    minimum=64, 
                    maximum=2048, 
                    value=512, 
                    step=64, 
                    label="Max Tokens",
                    info="æœ€å¤§ç”Ÿæˆé•¿åº¦"
                )
        
        # ä½¿ç”¨ç¤ºä¾‹
        gr.Examples(
            examples=[
                "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
                "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
                "å¸®æˆ‘å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
                "1+1ç­‰äºå¤šå°‘ï¼Ÿè¯·è¯¦ç»†è§£é‡Š"
            ],
            inputs=msg
        )
        
        # ==================== äº‹ä»¶ç»‘å®š ====================
        
        def user_submit(message, history):
            """ç”¨æˆ·æäº¤æ¶ˆæ¯"""
            if not message.strip():
                return "", history
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å² (bot å›å¤å…ˆè®¾ä¸º None)
            history = history + [[message, None]]
            return "", history
        
        def bot_response(history, temperature, top_p, max_tokens):
            """ç”Ÿæˆæœºå™¨äººå›å¤"""
            if not history:
                return history
            
            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            user_message = history[-1][0]
            
            # æµå¼ç”Ÿæˆå›å¤
            for response in generate_response(
                user_message, 
                history[:-1],  # å†å²ä¸åŒ…æ‹¬å½“å‰è¿™æ¡
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens
            ):
                history[-1][1] = response
                yield history
        
        # æäº¤æŒ‰é’®äº‹ä»¶
        submit_btn.click(
            user_submit,
            [msg, chatbot],
            [msg, chatbot],
            queue=False
        ).then(
            bot_response,
            [chatbot, temperature, top_p, max_tokens],
            chatbot
        )
        
        # å›è½¦æäº¤
        msg.submit(
            user_submit,
            [msg, chatbot],
            [msg, chatbot],
            queue=False
        ).then(
            bot_response,
            [chatbot, temperature, top_p, max_tokens],
            chatbot
        )
        
        # æ¸…é™¤æŒ‰é’®äº‹ä»¶
        clear_btn.click(
            lambda: ([], ""),
            None,
            [chatbot, msg]
        )
    
    return demo


if __name__ == "__main__":
    # ==================== å‚æ•°è§£æ ====================
    parser = argparse.ArgumentParser(description="MiniMind Web Demo")
    
    # æœåŠ¡å™¨é…ç½®
    parser.add_argument("--host", type=str, default="0.0.0.0", help="æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--port", type=int, default=7860, help="æœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--share", action="store_true", help="æ˜¯å¦åˆ›å»ºå…¬å…±é“¾æ¥")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--device", type=str, default="cuda:0" if torch.cuda.is_available() else "cpu", help="æ¨ç†è®¾å¤‡")
    parser.add_argument("--hidden_size", type=int, default=512, help="éšè—å±‚ç»´åº¦")
    parser.add_argument("--num_hidden_layers", type=int, default=8, help="éšè—å±‚æ•°é‡")
    parser.add_argument("--use_moe", type=int, default=0, choices=[0, 1], help="æ˜¯å¦ä½¿ç”¨MoE")
    
    # æƒé‡é…ç½®
    parser.add_argument("--model_weight", type=str, default="full_sft", help="åŸºç¡€æƒé‡åç§°")
    parser.add_argument("--lora_weight", type=str, default=None, help="LoRAæƒé‡åç§°ï¼ˆå¯é€‰ï¼‰")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    init_model_from_args(args)
    
    # åˆ›å»ºå¹¶å¯åŠ¨ demo
    print(f"å¯åŠ¨ Web Demo: http://{args.host}:{args.port}")
    demo = create_demo()
    demo.queue()
    demo.launch(
        server_name=args.host,
        server_port=args.port,
        share=args.share
    )
