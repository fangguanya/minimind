"""
UnrealEngine RAGå¢å¼ºæœåŠ¡
========================

ç”±äºå°æ¨¡å‹éš¾ä»¥ç²¾ç¡®è®°å¿†å¤§é‡ä»£ç ç»†èŠ‚ï¼Œæœ¬æ¨¡å—æä¾›RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰åŠŸèƒ½ï¼š
1. æ„å»ºUEä»£ç å‘é‡ç´¢å¼•
2. æ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢ç›¸å…³ä»£ç 
3. å°†æ£€ç´¢ç»“æœæ³¨å…¥åˆ°Promptä¸­ï¼Œå¢å¼ºLLMå›ç­”èƒ½åŠ›

ä¾èµ–å®‰è£…ï¼š
pip install sentence-transformers faiss-cpu

ä½¿ç”¨æ–¹æ³•ï¼š
1. æ„å»ºç´¢å¼•: python ue_rag_server.py --build_index --ue_source_path "D:/UnrealEngine/Engine/Source"
2. å¯åŠ¨æœåŠ¡: python ue_rag_server.py --serve
"""

import os
import sys
import json
import argparse
import pickle
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import re

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))


@dataclass
class CodeChunk:
    """ä»£ç å—"""
    file_path: str
    content: str
    chunk_type: str  # 'class', 'function', 'header', 'general'
    class_name: Optional[str] = None
    function_name: Optional[str] = None


class UECodeIndexer:
    """UEä»£ç ç´¢å¼•å™¨"""
    
    def __init__(self, embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.embedding_model_name = embedding_model
        self.embedder = None
        self.index = None
        self.chunks: List[CodeChunk] = []
        
    def _init_embedder(self):
        """å»¶è¿Ÿåˆå§‹åŒ–embeddingæ¨¡å‹"""
        if self.embedder is None:
            try:
                from sentence_transformers import SentenceTransformer
                print(f"åŠ è½½Embeddingæ¨¡å‹: {self.embedding_model_name}")
                self.embedder = SentenceTransformer(self.embedding_model_name)
            except ImportError:
                print("è¯·å…ˆå®‰è£…: pip install sentence-transformers")
                raise
    
    def extract_code_chunks(self, ue_source_path: str, max_files: int = None) -> List[CodeChunk]:
        """ä»UEæºç æå–ä»£ç å—"""
        chunks = []
        source_path = Path(ue_source_path)
        
        skip_dirs = {'ThirdParty', 'Intermediate', 'Binaries', '.git'}
        
        header_files = []
        for root, dirs, files in os.walk(source_path):
            dirs[:] = [d for d in dirs if d not in skip_dirs]
            for f in files:
                if f.endswith('.h'):
                    header_files.append(Path(root) / f)
        
        if max_files:
            header_files = header_files[:max_files]
        
        print(f"å¤„ç† {len(header_files)} ä¸ªå¤´æ–‡ä»¶...")
        
        from tqdm import tqdm
        for file_path in tqdm(header_files, desc="æå–ä»£ç å—"):
            try:
                chunks.extend(self._extract_from_file(file_path, source_path))
            except Exception as e:
                continue
        
        print(f"å…±æå– {len(chunks)} ä¸ªä»£ç å—")
        return chunks
    
    def _extract_from_file(self, file_path: Path, base_path: Path) -> List[CodeChunk]:
        """ä»å•ä¸ªæ–‡ä»¶æå–ä»£ç å—"""
        chunks = []
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
        except Exception:
            return chunks
        
        try:
            rel_path = str(file_path.relative_to(base_path))
        except ValueError:
            rel_path = file_path.name
        
        # æå–UCLASSå®šä¹‰
        uclass_pattern = r'(UCLASS\([^)]*\)\s*class\s+(?:\w+_API\s+)?(\w+)[^{]*\{[^}]*(?:\{[^}]*\}[^}]*)*\})'
        for match in re.finditer(uclass_pattern, content, re.DOTALL):
            class_code = match.group(0)[:2000]  # é™åˆ¶é•¿åº¦
            class_name = match.group(2)
            
            chunks.append(CodeChunk(
                file_path=rel_path,
                content=class_code,
                chunk_type='class',
                class_name=class_name
            ))
        
        # æå–UFUNCTIONå®šä¹‰
        ufunc_pattern = r'(UFUNCTION\([^)]*\)[^;{]+(?:\{[^}]*\}|;))'
        for match in re.finditer(ufunc_pattern, content):
            func_code = match.group(0)
            # æå–å‡½æ•°å
            name_match = re.search(r'(\w+)\s*\(', func_code)
            func_name = name_match.group(1) if name_match else None
            
            chunks.append(CodeChunk(
                file_path=rel_path,
                content=func_code,
                chunk_type='function',
                function_name=func_name
            ))
        
        # å¦‚æœæ–‡ä»¶è¾ƒå°ï¼Œæ•´ä½“ä½œä¸ºä¸€ä¸ªchunk
        if len(content) < 3000 and not chunks:
            chunks.append(CodeChunk(
                file_path=rel_path,
                content=content[:2000],
                chunk_type='header'
            ))
        
        return chunks
    
    def build_index(self, chunks: List[CodeChunk]):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        self._init_embedder()
        self.chunks = chunks
        
        try:
            import faiss
            import numpy as np
        except ImportError:
            print("è¯·å…ˆå®‰è£…: pip install faiss-cpu")
            raise
        
        # ç”Ÿæˆæ–‡æœ¬ç”¨äºembedding
        texts = []
        for chunk in chunks:
            # ç»„åˆæ–‡ä»¶è·¯å¾„ã€ç±»å/å‡½æ•°åå’Œä»£ç å†…å®¹
            text = f"File: {chunk.file_path}\n"
            if chunk.class_name:
                text += f"Class: {chunk.class_name}\n"
            if chunk.function_name:
                text += f"Function: {chunk.function_name}\n"
            text += chunk.content[:500]  # é™åˆ¶embeddingçš„æ–‡æœ¬é•¿åº¦
            texts.append(text)
        
        print("ç”ŸæˆEmbeddings...")
        embeddings = self.embedder.encode(texts, show_progress_bar=True)
        embeddings = np.array(embeddings).astype('float32')
        
        # æ„å»ºFAISSç´¢å¼•
        print("æ„å»ºFAISSç´¢å¼•...")
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
        
        # å½’ä¸€åŒ–ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings)
        
        print(f"ç´¢å¼•æ„å»ºå®Œæˆï¼å…± {self.index.ntotal} ä¸ªå‘é‡")
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[CodeChunk, float]]:
        """æœç´¢ç›¸å…³ä»£ç """
        self._init_embedder()
        
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_index()")
        
        import numpy as np
        import faiss
        
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.embedder.encode([query])
        query_embedding = np.array(query_embedding).astype('float32')
        faiss.normalize_L2(query_embedding)
        
        # æœç´¢
        scores, indices = self.index.search(query_embedding, top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.chunks):
                results.append((self.chunks[idx], float(score)))
        
        return results
    
    def save(self, path: str):
        """ä¿å­˜ç´¢å¼•"""
        import faiss
        
        path = Path(path)
        path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜FAISSç´¢å¼•
        faiss.write_index(self.index, str(path / "index.faiss"))
        
        # ä¿å­˜chunks
        with open(path / "chunks.pkl", 'wb') as f:
            pickle.dump(self.chunks, f)
        
        print(f"ç´¢å¼•å·²ä¿å­˜è‡³: {path}")
    
    def load(self, path: str):
        """åŠ è½½ç´¢å¼•"""
        import faiss
        
        path = Path(path)
        
        # åŠ è½½FAISSç´¢å¼•
        self.index = faiss.read_index(str(path / "index.faiss"))
        
        # åŠ è½½chunks
        with open(path / "chunks.pkl", 'rb') as f:
            self.chunks = pickle.load(f)
        
        print(f"ç´¢å¼•å·²åŠ è½½ï¼å…± {len(self.chunks)} ä¸ªä»£ç å—")


class UERAGChatBot:
    """RAGå¢å¼ºçš„UEé—®ç­”æœºå™¨äºº"""
    
    def __init__(self, indexer: UECodeIndexer, model_path: str = None, 
                 hidden_size: int = 512, num_hidden_layers: int = 8):
        self.indexer = indexer
        self.model = None
        self.tokenizer = None
        self.model_path = model_path
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers
        self.device = 'cuda' if self._check_cuda() else 'cpu'
        
    def _check_cuda(self):
        """æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨"""
        try:
            import torch
            return torch.cuda.is_available()
        except:
            return False
        
    def _init_model(self):
        """åˆå§‹åŒ–LLMæ¨¡å‹"""
        if self.model is not None:
            return True
        
        import torch
        from transformers import AutoTokenizer
        
        # æ·»åŠ é¡¹ç›®è·¯å¾„
        model_dir = PROJECT_ROOT / "model"
        sys.path.insert(0, str(PROJECT_ROOT))
        
        try:
            from model.model_minimind import MiniMindConfig, MiniMindForCausalLM
        except ImportError:
            print("[é”™è¯¯] æ— æ³•å¯¼å…¥MiniMindæ¨¡å‹ï¼Œè¯·ç¡®ä¿åœ¨minimindé¡¹ç›®ç›®å½•ä¸‹è¿è¡Œ")
            return False
        
        # åŠ è½½tokenizer
        tokenizer_path = PROJECT_ROOT / "model"
        self.tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_path))
        
        # æŸ¥æ‰¾æ¨¡å‹æƒé‡
        weight_path = None
        if self.model_path and os.path.exists(self.model_path):
            weight_path = self.model_path
        else:
            # è‡ªåŠ¨æŸ¥æ‰¾ue_sftæƒé‡
            default_weight = PROJECT_ROOT / "out" / f"ue_sft_{self.hidden_size}.pth"
            if default_weight.exists():
                weight_path = str(default_weight)
            else:
                # å°è¯•full_sftæƒé‡
                fallback_weight = PROJECT_ROOT / "out" / f"full_sft_{self.hidden_size}.pth"
                if fallback_weight.exists():
                    weight_path = str(fallback_weight)
        
        if not weight_path:
            print("[è­¦å‘Š] æœªæ‰¾åˆ°æ¨¡å‹æƒé‡ï¼Œä»…ä½¿ç”¨RAGæ£€ç´¢åŠŸèƒ½")
            print(f"  å°è¯•æŸ¥æ‰¾: out/ue_sft_{self.hidden_size}.pth")
            return False
        
        print(f"åŠ è½½MiniMindæ¨¡å‹: {weight_path}")
        
        # åˆ›å»ºæ¨¡å‹
        config = MiniMindConfig(
            hidden_size=self.hidden_size,
            num_hidden_layers=self.num_hidden_layers
        )
        self.model = MiniMindForCausalLM(config)
        
        # åŠ è½½æƒé‡
        state_dict = torch.load(weight_path, map_location=self.device)
        self.model.load_state_dict(state_dict, strict=True)
        self.model = self.model.eval().to(self.device)
        
        print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼è®¾å¤‡: {self.device}")
        return True
    
    def build_prompt_with_context(self, query: str, top_k: int = 3) -> Tuple[str, List[CodeChunk]]:
        """æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„Prompt"""
        # æ£€ç´¢ç›¸å…³ä»£ç 
        results = self.indexer.search(query, top_k=top_k)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        retrieved_chunks = []
        
        for chunk, score in results:
            context = f"ã€ç›¸å…³ä»£ç  - {chunk.file_path}ã€‘\n"
            if chunk.class_name:
                context += f"ç±»å: {chunk.class_name}\n"
            if chunk.function_name:
                context += f"å‡½æ•°: {chunk.function_name}\n"
            context += f"```cpp\n{chunk.content[:800]}\n```\n"
            
            context_parts.append(context)
            retrieved_chunks.append(chunk)
        
        context_str = "\n".join(context_parts)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªUnrealEngineä»£ç åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒä»£ç å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€å‚è€ƒä»£ç ã€‘
{context_str}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
1. å¦‚æœå‚è€ƒä»£ç èƒ½å›ç­”é—®é¢˜ï¼Œè¯·å¼•ç”¨ç›¸å…³çš„ç±»åã€å‡½æ•°åå’Œæ–‡ä»¶è·¯å¾„
2. æä¾›æ¸…æ™°çš„è§£é‡Šå’Œä»£ç ç¤ºä¾‹
3. å¦‚æœå‚è€ƒä»£ç ä¸è¶³ä»¥å›ç­”ï¼Œè¯·è¯´æ˜å¹¶ç»™å‡ºä¸€èˆ¬æ€§å»ºè®®

ã€å›ç­”ã€‘"""
        
        return prompt, retrieved_chunks
    
    def chat(self, query: str, top_k: int = 3) -> Dict:
        """é—®ç­”æ¥å£"""
        prompt, chunks = self.build_prompt_with_context(query, top_k)
        
        response = {
            "query": query,
            "retrieved_chunks": [
                {
                    "file_path": c.file_path,
                    "class_name": c.class_name,
                    "function_name": c.function_name,
                    "content_preview": c.content[:200] + "..."
                }
                for c in chunks
            ],
            "prompt": prompt,
            "answer": None
        }
        
        # å°è¯•åˆå§‹åŒ–æ¨¡å‹
        model_loaded = self._init_model()
        
        # å¦‚æœæœ‰æ¨¡å‹ï¼Œç”Ÿæˆå›ç­”
        if model_loaded and self.model is not None:
            import torch
            
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    max_new_tokens=512,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            answer = self.tokenizer.decode(outputs[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
            response["answer"] = answer
        else:
            # æ²¡æœ‰æ¨¡å‹æ—¶ï¼ŒåŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç®€å•å›ç­”
            answer_parts = ["æ ¹æ®æ£€ç´¢åˆ°çš„UEä»£ç ä¿¡æ¯ï¼š\n"]
            for i, chunk in enumerate(chunks, 1):
                answer_parts.append(f"\n**{i}. {chunk.file_path}**")
                if chunk.class_name:
                    answer_parts.append(f"\n   ç±»å: `{chunk.class_name}`")
                if chunk.function_name:
                    answer_parts.append(f"\n   å‡½æ•°: `{chunk.function_name}`")
                answer_parts.append(f"\n   ä»£ç é¢„è§ˆ:\n```cpp\n{chunk.content[:300]}...\n```")
            
            response["answer"] = "".join(answer_parts)
        
        return response


def build_index(args):
    """æ„å»ºç´¢å¼•"""
    indexer = UECodeIndexer()
    
    # æå–ä»£ç å—
    chunks = indexer.extract_code_chunks(
        args.ue_source_path,
        max_files=args.max_files
    )
    
    # æ„å»ºç´¢å¼•
    indexer.build_index(chunks)
    
    # ä¿å­˜
    indexer.save(args.index_path)


def serve_rag(args):
    """å¯åŠ¨RAGæœåŠ¡"""
    indexer = UECodeIndexer()
    indexer.load(args.index_path)
    
    chatbot = UERAGChatBot(
        indexer, 
        model_path=args.model_path,
        hidden_size=args.hidden_size,
        num_hidden_layers=args.num_hidden_layers
    )
    
    print("\n" + "="*60)
    print("ğŸš€ UE RAGé—®ç­”æœåŠ¡å·²å¯åŠ¨")
    print("="*60)
    print(f"æ¨¡å‹é…ç½®: hidden_size={args.hidden_size}, layers={args.num_hidden_layers}")
    print("è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    print("="*60 + "\n")
    
    while True:
        try:
            query = input("ğŸ’¬ é—®é¢˜: ").strip()
            if query.lower() in ['quit', 'exit', 'q']:
                break
            if not query:
                continue
            
            print("\nğŸ” æ£€ç´¢ç›¸å…³ä»£ç ...")
            result = chatbot.chat(query, top_k=args.top_k)
            
            print("\nğŸ“š æ£€ç´¢åˆ°çš„ç›¸å…³ä»£ç :")
            print("-" * 40)
            for i, chunk in enumerate(result["retrieved_chunks"], 1):
                print(f"  [{i}] {chunk['file_path']}")
                if chunk['class_name']:
                    print(f"      ç±»å: {chunk['class_name']}")
                if chunk['function_name']:
                    print(f"      å‡½æ•°: {chunk['function_name']}")
            print("-" * 40)
            
            print(f"\nğŸ¤– å›ç­”:")
            print(result['answer'])
            print("\n" + "="*60 + "\n")
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"[é”™è¯¯] {e}")
            continue
    
    print("\nğŸ‘‹ å†è§ï¼")


def main():
    parser = argparse.ArgumentParser(description="UEä»£ç RAGæœåŠ¡")
    
    subparsers = parser.add_subparsers(dest='command')
    
    # æ„å»ºç´¢å¼•
    build_parser = subparsers.add_parser('build', help='æ„å»ºä»£ç ç´¢å¼•')
    build_parser.add_argument('--ue_source_path', type=str, required=True,
                              help="UEæºä»£ç è·¯å¾„")
    build_parser.add_argument('--index_path', type=str, default='./ue_index',
                              help="ç´¢å¼•ä¿å­˜è·¯å¾„")
    build_parser.add_argument('--max_files', type=int, default=None,
                              help="æœ€å¤§å¤„ç†æ–‡ä»¶æ•°")
    
    # å¯åŠ¨æœåŠ¡
    serve_parser = subparsers.add_parser('serve', help='å¯åŠ¨RAGæœåŠ¡')
    serve_parser.add_argument('--index_path', type=str, default='./ue_index',
                              help="ç´¢å¼•è·¯å¾„")
    serve_parser.add_argument('--model_path', type=str, default=None,
                              help="LLMæ¨¡å‹æƒé‡è·¯å¾„ï¼ˆ.pthæ–‡ä»¶ï¼Œå¯é€‰ï¼‰")
    serve_parser.add_argument('--hidden_size', type=int, default=512,
                              help="æ¨¡å‹éšè—å±‚ç»´åº¦ (512=Small, 768=Base)")
    serve_parser.add_argument('--num_hidden_layers', type=int, default=8,
                              help="æ¨¡å‹å±‚æ•° (8=Small, 16=Base)")
    serve_parser.add_argument('--top_k', type=int, default=5,
                              help="æ£€ç´¢ç»“æœæ•°é‡")
    
    args = parser.parse_args()
    
    if args.command == 'build':
        build_index(args)
    elif args.command == 'serve':
        serve_rag(args)
    else:
        parser.print_help()


if __name__ == '__main__':
    main()
